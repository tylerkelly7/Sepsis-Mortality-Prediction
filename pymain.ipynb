{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ed17d5-3441-42fa-9772-e9c6d22ad61f",
   "metadata": {},
   "source": [
    "##### Improving Prediction Accuracy of Sepsis Mortality using Machine Learning and Natural Language Processing\n",
    "## Tyler Kelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e5ce0-29f8-4607-b46c-364598c412a7",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd196b-2a8c-476f-8122-5da74fd724e6",
   "metadata": {},
   "source": [
    "## Set Up and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4bbcd-afdf-4f3d-b47b-1f9216e5c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets transformers pandas shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c39c3",
   "metadata": {},
   "source": [
    "## Part 0 Preprocessing (Pull Code from Author's ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5889d9",
   "metadata": {},
   "source": [
    "The following code is adapted from the github repository 'https://github.com/yuyinglu2000/Sepsis-Mortality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5e92c",
   "metadata": {},
   "source": [
    "### Data Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907322",
   "metadata": {},
   "source": [
    "Begin by creating bigquery search to get the 38 unique features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cb3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw = pd.read_csv('Data/data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw.shape\n",
    "# Expect to get a dataframe 808188x38 (38 old columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356e8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "# regroup the race\n",
    "race_mapping = {\n",
    "    'WHITE': 'White',\n",
    "    'HISPANIC OR LATINO': 'Hispanic or Latin',\n",
    "    'BLACK/AFRICAN AMERICAN': 'Black or African American',\n",
    "    'BLACK/CARIBBEAN ISLAND': 'Black or African American',\n",
    "    'HISPANIC/LATINO - DOMINICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - CENTRAL AMERICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - GUATEMALAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - PUERTO RICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - SALVADORAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - HONDURAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - MEXICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - CUBAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - COLUMBIAN': 'Hispanic or Latin',\n",
    "    'BLACK/CAPE VERDEAN': 'Black or African American',\n",
    "    'BLACK/AFRICAN': 'Black or African American',\n",
    "    'SOUTH AMERICAN': 'Hispanic or Latin',\n",
    "    'WHITE - BRAZILIAN': 'Hispanic or Latin',\n",
    "    'WHITE - OTHER EUROPEAN': 'White',\n",
    "    'WHITE - RUSSIAN': 'White',\n",
    "    'WHITE - EASTERN EUROPEAN': 'White',\n",
    "    'ASIAN': 'Others race',\n",
    "    'ASIAN - SOUTH EAST ASIAN': 'Others race',\n",
    "    'ASIAN - CHINESE': 'Others race',\n",
    "    'ASIAN - ASIAN INDIAN': 'Others race',\n",
    "    'ASIAN - KOREAN': 'Others race',\n",
    "    'AMERICAN INDIAN/ALASKA NATIVE': 'Others race',\n",
    "    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'Others race',\n",
    "    'MULTIPLE RACE/ETHNICITY': 'Others race',\n",
    "    'PORTUGUESE': 'Others race',\n",
    "    'UNKNOWN': 'Others race',\n",
    "    'OTHER': 'Others race',\n",
    "    'UNABLE TO OBTAIN': 'Others race',\n",
    "    'PATIENT DECLINED TO ANSWER': 'Others race'\n",
    "}\n",
    "\n",
    "#df_raw['race'] = df_raw['race'].map(race_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396b5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# df = ... (your DataFrame)\n",
    "\n",
    "# Define a mapping for antibiotics to their respective groups\n",
    "antibiotic_mapping = {\n",
    "    'Gentamicin Sulfate': 'Aminoglycoside',\n",
    "    'Tobramycin Sulfate': 'Aminoglycoside',\n",
    "    'Streptomycin Sulfate': 'Aminoglycoside',\n",
    "    'Neomycin Sulfate': 'Aminoglycoside',\n",
    "    'Neomycin/Polymyxin B Sulfate': 'Aminoglycoside',\n",
    "    'Meropenem': 'Carbapenem',\n",
    "    'Meropenem Graded Challenge': 'Carbapenem',\n",
    "    'Vancomycin': 'Glycopeptide',\n",
    "    'Vancomycin Oral Liquid': 'Glycopeptide',\n",
    "    'Vancomycin Antibiotic Lock': 'Glycopeptide',\n",
    "    'Vancomycin Enema': 'Glycopeptide',\n",
    "    'Vancomycin Intrathecal': 'Glycopeptide',\n",
    "    'Vancomycin Ora': 'Glycopeptide',\n",
    "    'Linezolid': 'Oxazolidinone',\n",
    "    'Linezolid Suspension': 'Oxazolidinone',\n",
    "    'Penicillin G Benzathine': 'Penicillin',\n",
    "    'Penicillin G Potassium': 'Penicillin',\n",
    "    'Penicillin V Potassium': 'Penicillin',\n",
    "    'Sulfameth/Trimethoprim': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim DS': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim SS': 'Sulfonamide',\n",
    "    'Sulfamethoxazole-Trimethoprim': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim Suspension': 'Sulfonamide',\n",
    "    'Tetracycline': 'Tetracycline',\n",
    "    'Tetracycline HCl': 'Tetracycline'\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Applying the mapping to the 'antibiotic' column\n",
    "#df_raw['antibiotic'] = df_raw['antibiotic'].map(antibiotic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56745eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "#df_raw['antibiotic'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9cc603",
   "metadata": {},
   "source": [
    "### Get Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "df_encoded = pd.get_dummies(df_raw, columns=df_raw.select_dtypes(include=['object']).columns)\n",
    "df_dropped = df_encoded.dropna()\n",
    "df_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check empty values *for tetracycline* ###\n",
    "#AC\n",
    "empty_values = df_dropped['antibiotic_Tetracycline'].isnull().any()\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38a1c0",
   "metadata": {},
   "source": [
    "After applying get_dummy_variables there is now 53 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ebb55",
   "metadata": {},
   "source": [
    "### Drop Duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC with minor edits\n",
    "\n",
    "duplicated_rows_mask = df_dropped['subject_id'].duplicated(keep=False)\n",
    "\n",
    "# Extract the duplicated rows\n",
    "duplicated_rows = df_dropped[duplicated_rows_mask]\n",
    "new_data  = df_dropped.drop_duplicates()\n",
    "duplicated_rows_mask = new_data['subject_id'].duplicated(keep=False)\n",
    "\n",
    "# Extract the duplicated rows\n",
    "duplicated_rows = new_data[duplicated_rows_mask]\n",
    "# Separate out columns based on data types\n",
    "int_float_cols = new_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "uint8_cols = new_data.select_dtypes(include=['uint8']).columns\n",
    "\n",
    "# Sort dataframe\n",
    "# For int and float columns: sort in descending order so that larger values come first\n",
    "df_raw = new_data.sort_values(by=list(int_float_cols), ascending=False)\n",
    "\n",
    "# For uint8 columns: sort in descending order so that 1 comes before 0\n",
    "df_raw = df_raw.sort_values(by=list(uint8_cols), ascending=False)\n",
    "\n",
    "# Drop duplicates based on subject_id, keeping the first (which are the desired rows after sorting)\n",
    "df_reduced = df_raw.drop_duplicates(subset='subject_id', keep='first')\n",
    "\n",
    "# Reset index if needed\n",
    "df_reduced = df_reduced.reset_index(drop=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82be4d9",
   "metadata": {},
   "source": [
    "After reducing the dataframe we get the 6401 patients reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec042c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab3776-07b1-477e-bf34-aec08063d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subject = df_reduced['subject_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subject.to_csv(\"data_subject_id_ready_to_query.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5df87b",
   "metadata": {},
   "source": [
    "## Part 1 Upload data_ready_to_merge.csv to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905776e4",
   "metadata": {},
   "source": [
    "Upload the subject_id dataset to bigquery and merge the dataset to radiology and discharge notes separately. Save the downloaded file to downloads (or find a way to save it directly to my BIOST 2021 Thesis / Main ) as\n",
    "'data_radiology_notes.csv' and 'data_discharge_notes.csv' then joing below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edac5ba",
   "metadata": {},
   "source": [
    "Place SQL code chunks below (if time write them into the script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6943900",
   "metadata": {},
   "source": [
    "# READ THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987127df",
   "metadata": {},
   "source": [
    "data_full_notes_old.csv uses an old outdated dataset but I haven't figured out the correct sql to get the correct dataset at 6401.\n",
    "\n",
    "For now, I will use `df_old` when using the outdated dataset and `df` when I get the new corrected one.\n",
    "\n",
    "To get the radiology and discharge notes, I uploaded the data_after_cleaning table to big query and used the below code sql query to get a table with\n",
    "data_after_cleaning joined to the notes columns found in the mimic-iv discharge and radiology tables.\n",
    "\n",
    "Above, using the correct subject_id list to get the 6401 patients, I use the same method to obtain the rad_notes and discharge_notes tables using bigquery, and join in an identical manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de455f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Old/data_full_notes_old.csv') # df_old\n",
    "# change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737320aa-c0be-465b-b531-2b7c35bfd481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303994, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25600a63-6c3b-41a4-9c75-42c7e346c246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303994 entries, 0 to 303993\n",
      "Data columns (total 58 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   int64_field_0                          303994 non-null  int64  \n",
      " 1   subject_id                             303994 non-null  int64  \n",
      " 2   hospital_expire_flag                   303994 non-null  int64  \n",
      " 3   max_age                                303994 non-null  int64  \n",
      " 4   los_icu                                303994 non-null  float64\n",
      " 5   first_hosp_stay                        303994 non-null  bool   \n",
      " 6   suspected_infection                    303994 non-null  int64  \n",
      " 7   sofa_score                             303994 non-null  int64  \n",
      " 8   sepsis3                                303994 non-null  bool   \n",
      " 9   avg_urineoutput                        303994 non-null  float64\n",
      " 10  glucose_min                            303994 non-null  int64  \n",
      " 11  glucose_max                            303994 non-null  int64  \n",
      " 12  glucose_average                        303994 non-null  float64\n",
      " 13  sodium_max                             303994 non-null  int64  \n",
      " 14  sodium_min                             303994 non-null  int64  \n",
      " 15  sodium_average                         303994 non-null  float64\n",
      " 16  diabetes_without_cc                    303994 non-null  int64  \n",
      " 17  diabetes_with_cc                       303994 non-null  int64  \n",
      " 18  severe_liver_disease                   303994 non-null  int64  \n",
      " 19  aids                                   303994 non-null  int64  \n",
      " 20  renal_disease                          303994 non-null  int64  \n",
      " 21  heart_rate_min                         303994 non-null  int64  \n",
      " 22  heart_rate_max                         303994 non-null  int64  \n",
      " 23  heart_rate_mean                        303994 non-null  float64\n",
      " 24  sbp_min                                303994 non-null  float64\n",
      " 25  sbp_max                                303994 non-null  float64\n",
      " 26  sbp_mean                               303994 non-null  float64\n",
      " 27  dbp_min                                303994 non-null  float64\n",
      " 28  dbp_max                                303994 non-null  float64\n",
      " 29  dbp_mean                               303994 non-null  float64\n",
      " 30  resp_rate_min                          303994 non-null  float64\n",
      " 31  resp_rate_max                          303994 non-null  float64\n",
      " 32  resp_rate_mean                         303994 non-null  float64\n",
      " 33  spo2_min                               303994 non-null  int64  \n",
      " 34  spo2_max                               303994 non-null  int64  \n",
      " 35  spo2_mean                              303994 non-null  float64\n",
      " 36  coma                                   303994 non-null  int64  \n",
      " 37  albumin                                303994 non-null  float64\n",
      " 38  race_Black or African American         303994 non-null  int64  \n",
      " 39  race_Hispanic or Latin                 303994 non-null  int64  \n",
      " 40  race_Others race                       303994 non-null  int64  \n",
      " 41  race_White                             303994 non-null  int64  \n",
      " 42  antibiotic_Vancomycin                  303994 non-null  int64  \n",
      " 43  antibiotic_Vancomycin Antibiotic Lock  303994 non-null  int64  \n",
      " 44  antibiotic_Vancomycin Enema            303994 non-null  int64  \n",
      " 45  antibiotic_Vancomycin Intrathecal      303994 non-null  int64  \n",
      " 46  antibiotic_Vancomycin Oral Liquid      303994 non-null  int64  \n",
      " 47  gender_F                               303994 non-null  int64  \n",
      " 48  gender_M                               303994 non-null  int64  \n",
      " 49  note_id                                303994 non-null  object \n",
      " 50  subject_id_1                           303994 non-null  int64  \n",
      " 51  hadm_id                                237446 non-null  float64\n",
      " 52  note_type                              303994 non-null  object \n",
      " 53  note_seq                               303994 non-null  int64  \n",
      " 54  charttime                              303994 non-null  object \n",
      " 55  storetime                              303994 non-null  object \n",
      " 56  text                                   303994 non-null  object \n",
      " 57  note_type_1                            303994 non-null  object \n",
      "dtypes: bool(2), float64(17), int64(33), object(6)\n",
      "memory usage: 130.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76a458-f382-4e39-93c8-fabdf0b024b3",
   "metadata": {},
   "source": [
    "## Part 2 Truncate Notes for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013c98a8-9450-4fb4-9cd0-e3ae8d1adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83765b2e-1ba0-4f72-94e6-0060b027568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Define cleaning function - Clean individual note text ===\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'_+', '', text)    # Remove underlines\n",
    "    text = re.sub(r'[^\\w\\s.,:;!?()\\-\\n]', '', text)  # Remove junk, keep clinical symbols\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1012ffaf-a180-45ac-b82f-8eda85aeca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Function to process one group. Process a group into (subject_id, note_type_1, combined_notes) ===\n",
    "def process_group(record):\n",
    "    subject_id = record['subject_id']\n",
    "    note_type_1 = record['note_type_1'] #this column identifies addendums and base notes to just be 'radiology' notes\n",
    "    texts = record['text']\n",
    "    cleaned_notes = [clean_text(text) for text in texts]\n",
    "    combined_notes = \" \".join(cleaned_notes)\n",
    "    return {\n",
    "        'subject_id': subject_id,\n",
    "        'note_type_1': note_type_1,\n",
    "        'combined_notes': combined_notes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7893d23a-1fe0-48c2-b711-19aac68c1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Group the notes. Load and group your data ===\n",
    "# Replace this with your actual loading logic / dataframe\n",
    "# Data loaded above as `df`\n",
    "grouped_df = (\n",
    "    df.groupby(['subject_id', 'note_type_1'])['text']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "records = grouped_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "605c2269-543d-4012-89fa-e4e3f910979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Parallel processing with joblib ===\n",
    "\n",
    "#this num_cores variable is used later for other parallel processing jobs\n",
    "num_cores = multiprocessing.cpu_count() - 1\n",
    "\n",
    "processed = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_group)(record) for record in records\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8e925-2a6a-4d6c-bea6-4c3915c06594",
   "metadata": {},
   "source": [
    "The following step creates a long dataframe for notes per patient by type (some patients have radiology, discharge, both, or none for notes), it is later converted to wide to potentially compare NLP to see if discharge notes significantly provide insight into mortality rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a510f691-b265-4c3c-8151-619dcdd69e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Create DataFrame and save to csv ===\n",
    "nlp_long_df = pd.DataFrame(processed).sort_values(by=['subject_id', 'note_type_1'])\n",
    "\n",
    "nlp_long_df.to_csv(\"Data/Old/data_trunc_notes_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96bbd31a-3e65-40d6-9174-fa9dc34d7d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10402, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_long_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8db47d90-108d-4afa-a844-81be9b9fd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 6: Pivot to wide format for multiple columns ===\n",
    "# Convert note_type_1 to columns like 'Radiology_notes', etc.\n",
    "nlp_wide_df = nlp_long_df.pivot(\n",
    "    index='subject_id',\n",
    "    columns='note_type_1',\n",
    "    values='combined_notes'\n",
    ").reset_index()\n",
    "\n",
    "nlp_wide_df.columns.name = None # Remove category label\n",
    "\n",
    "# Rename columns to make clear\n",
    "nlp_wide_df = nlp_wide_df.rename(columns={\n",
    "    'radiology': 'Radiology_notes',\n",
    "    'discharge': 'Discharge_summary_notes'\n",
    "})\n",
    "\n",
    "nlp_wide_df = nlp_wide_df.fillna(\"\") #fills NA columns with empty strings\n",
    "\n",
    "# Save\n",
    "nlp_wide_df.to_csv(\"Data/Old/data_trunc_notes_wide_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2be1ee-2fbb-400f-a13e-9d3bb14dff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 7: Combine Radiology and Discharge notes per subject_id ===\n",
    "nlp_combined_df = nlp_wide_df.copy()\n",
    "\n",
    "# Concatenate the two columns into one\n",
    "nlp_combined_df['combined_notes'] = (\n",
    "    nlp_combined_df['Radiology_notes'].str.strip() + \" \" +\n",
    "    nlp_combined_df['Discharge_summary_notes'].str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Combined DataFrame with just subject_id + combined text\n",
    "nlp_combined_notes_df = nlp_combined_df[['subject_id', 'combined_notes']]\n",
    "\n",
    "# Save\n",
    "nlp_combined_notes_df.to_csv(\"Data/Old/data_trunc_notes_combined_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67738c-a9c8-4bad-825d-956d067ca1d6",
   "metadata": {},
   "source": [
    "The code chunk below creates `nlp_ready_df`, a dataframe that includes a row for each subject_id and appends 3 new columns with all text truncated based on radiology notes, discharge notes, and combined radiology and discharged notes. I lose information related to the note_id, note_id_type, and more importantly charttime, but here it allows word2vec to work properly. This part of my thesis focuses more on using embeddings from the clinical text to aid in predicting mortality rather than finding the best way to do it (i.e. finding the best time of day where it is more likely to happen than not, or finding the best drug at predicting it, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72cdbd3d-3399-4449-80c6-648bd73160c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 8: Join nlp_wide_df and nlp_combined_notes_df to data_after_cleaning ===\n",
    "\n",
    "# === Step i: Load the original cleaned dataset, df_reduced ===\n",
    "# ie this is df_reduced\n",
    "\n",
    "# For now, this is `df_clean` the cleaned dataset.\n",
    "\n",
    "df_clean = pd.read_csv('Data/Old/data_after_cleaning.csv')\n",
    "\n",
    "# === Step ii: Merge df_reduced with nlp_wide_df ===\n",
    "# This adds the radiology and discharge notes as 2 new columns to df_reduced\n",
    "# use data_clean until df_reduced is finalized\n",
    "\n",
    "nlp_ready_df = df_clean.merge(\n",
    "    nlp_wide_df,\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# === Step iii: Merge with combined notes ===\n",
    "# This adds one new column of all notes combined together as a single note (per patient) to the nlp_ready_df above\n",
    "nlp_ready_df = nlp_ready_df.merge(\n",
    "    nlp_combined_notes_df,\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Save\n",
    "nlp_ready_df.to_csv(\"Data/Old/data_nlp_ready_old.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "094ddd80-f078-4f19-bafd-bb544b64d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 9: Check shape of dataframes ===\n",
    "nlp_wide_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3f044b1-4c59-4cb6-b74c-32d9de161bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_combined_notes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e18979ae-ac9d-4c3e-aeb8-13bd72afdfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d43814b0-307d-42b1-b565-1316c6bb1821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 51)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ready_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43007975-bac1-499e-bcd9-33695e1f7dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hospital_expire_flag', 'max_age', 'los_icu',\n",
       "       'first_hosp_stay', 'suspected_infection', 'sofa_score', 'sepsis3',\n",
       "       'avg_urineoutput', 'glucose_min', 'glucose_max', 'glucose_average',\n",
       "       'sodium_max', 'sodium_min', 'sodium_average', 'diabetes_without_cc',\n",
       "       'diabetes_with_cc', 'severe_liver_disease', 'aids', 'renal_disease',\n",
       "       'heart_rate_min', 'heart_rate_max', 'heart_rate_mean', 'sbp_min',\n",
       "       'sbp_max', 'sbp_mean', 'dbp_min', 'dbp_max', 'dbp_mean',\n",
       "       'resp_rate_min', 'resp_rate_max', 'resp_rate_mean', 'spo2_min',\n",
       "       'spo2_max', 'spo2_mean', 'coma', 'albumin',\n",
       "       'race_Black or African American', 'race_Hispanic or Latin',\n",
       "       'race_Others race', 'race_White', 'antibiotic_Vancomycin',\n",
       "       'antibiotic_Vancomycin Antibiotic Lock', 'antibiotic_Vancomycin Enema',\n",
       "       'antibiotic_Vancomycin Intrathecal',\n",
       "       'antibiotic_Vancomycin Oral Liquid', 'gender_F', 'gender_M',\n",
       "       'Discharge_summary_notes', 'Radiology_notes', 'combined_notes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ready_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccb06cff-23d0-4d94-ae50-e6dae3d3c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5208 entries, 0 to 5207\n",
      "Data columns (total 51 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   subject_id                             5208 non-null   int64  \n",
      " 1   hospital_expire_flag                   5208 non-null   int64  \n",
      " 2   max_age                                5208 non-null   int64  \n",
      " 3   los_icu                                5208 non-null   float64\n",
      " 4   first_hosp_stay                        5208 non-null   bool   \n",
      " 5   suspected_infection                    5208 non-null   int64  \n",
      " 6   sofa_score                             5208 non-null   int64  \n",
      " 7   sepsis3                                5208 non-null   bool   \n",
      " 8   avg_urineoutput                        5208 non-null   float64\n",
      " 9   glucose_min                            5208 non-null   int64  \n",
      " 10  glucose_max                            5208 non-null   int64  \n",
      " 11  glucose_average                        5208 non-null   float64\n",
      " 12  sodium_max                             5208 non-null   int64  \n",
      " 13  sodium_min                             5208 non-null   int64  \n",
      " 14  sodium_average                         5208 non-null   float64\n",
      " 15  diabetes_without_cc                    5208 non-null   int64  \n",
      " 16  diabetes_with_cc                       5208 non-null   int64  \n",
      " 17  severe_liver_disease                   5208 non-null   int64  \n",
      " 18  aids                                   5208 non-null   int64  \n",
      " 19  renal_disease                          5208 non-null   int64  \n",
      " 20  heart_rate_min                         5208 non-null   int64  \n",
      " 21  heart_rate_max                         5208 non-null   int64  \n",
      " 22  heart_rate_mean                        5208 non-null   float64\n",
      " 23  sbp_min                                5208 non-null   float64\n",
      " 24  sbp_max                                5208 non-null   float64\n",
      " 25  sbp_mean                               5208 non-null   float64\n",
      " 26  dbp_min                                5208 non-null   float64\n",
      " 27  dbp_max                                5208 non-null   float64\n",
      " 28  dbp_mean                               5208 non-null   float64\n",
      " 29  resp_rate_min                          5208 non-null   float64\n",
      " 30  resp_rate_max                          5208 non-null   float64\n",
      " 31  resp_rate_mean                         5208 non-null   float64\n",
      " 32  spo2_min                               5208 non-null   int64  \n",
      " 33  spo2_max                               5208 non-null   int64  \n",
      " 34  spo2_mean                              5208 non-null   float64\n",
      " 35  coma                                   5208 non-null   int64  \n",
      " 36  albumin                                5208 non-null   float64\n",
      " 37  race_Black or African American         5208 non-null   int64  \n",
      " 38  race_Hispanic or Latin                 5208 non-null   int64  \n",
      " 39  race_Others race                       5208 non-null   int64  \n",
      " 40  race_White                             5208 non-null   int64  \n",
      " 41  antibiotic_Vancomycin                  5208 non-null   int64  \n",
      " 42  antibiotic_Vancomycin Antibiotic Lock  5208 non-null   int64  \n",
      " 43  antibiotic_Vancomycin Enema            5208 non-null   int64  \n",
      " 44  antibiotic_Vancomycin Intrathecal      5208 non-null   int64  \n",
      " 45  antibiotic_Vancomycin Oral Liquid      5208 non-null   int64  \n",
      " 46  gender_F                               5208 non-null   int64  \n",
      " 47  gender_M                               5208 non-null   int64  \n",
      " 48  Discharge_summary_notes                5208 non-null   object \n",
      " 49  Radiology_notes                        5208 non-null   object \n",
      " 50  combined_notes                         5208 non-null   object \n",
      "dtypes: bool(2), float64(16), int64(30), object(3)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "nlp_ready_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcaabe-ba2d-498d-9f79-1f2cad58c687",
   "metadata": {},
   "source": [
    "## Part 3 Create Note File for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79f1a367-d882-4b2f-929b-4d2f7b35b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Write the 'Radiology_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_Radiology_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"Radiology_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4c2c47b-ef7d-41f1-81d7-439470764fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the 'Discharge_summary_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_Discharge_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"Discharge_summary_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c3a5d89-c689-4fbd-9499-9e172a4aa1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the 'combined_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_combined_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"combined_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d0c99",
   "metadata": {},
   "source": [
    "## Part 4 Prepare Word2Vec - Proceed to main.rmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3d2f9",
   "metadata": {},
   "source": [
    "## Part 5 Model Training - Proceed to Sepsis_Model_Training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c224e",
   "metadata": {},
   "source": [
    "After completing / running / saving models in model training, upload them into the workspace in the following code chunks if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bc3e0-7f90-41dd-856d-6673a057c292",
   "metadata": {},
   "source": [
    "## Part 6 Create Dataset for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb41763-6061-46e3-97de-b3c759433ff4",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "\n",
    "Consider filtering data prior to setting up these BERT datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63d4ca06-89f1-4103-b8f3-14b5b3a6b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def write_single_note_clean(row, output_dir):\n",
    "    try:\n",
    "        subject_id = str(row[\"subject_id\"]).strip()\n",
    "        note_id = str(row[\"note_id\"]).strip()\n",
    "        note_text = row.get(\"text\", \"\")\n",
    "\n",
    "        # Ensure note_text is a clean string\n",
    "        if not isinstance(note_text, str):\n",
    "            note_text = \"\" if pd.isna(note_text) else str(note_text)\n",
    "        note_text = note_text.strip()\n",
    "\n",
    "        # Skip empty notes\n",
    "        if not note_text:\n",
    "            return\n",
    "\n",
    "        # Make subject directory\n",
    "        subject_dir = os.path.join(output_dir, subject_id)\n",
    "        os.makedirs(subject_dir, exist_ok=True)\n",
    "\n",
    "        # Save note\n",
    "        file_path = os.path.join(subject_dir, f\"{subject_id}_{note_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(note_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing note {row.get('note_id', 'unknown')}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Change output directories / metadata for discharge notes\n",
    "def write_mimic_notes_parallel_for_bert(df, \n",
    "                                        output_dir=\"BERT/BERT_old/notes_old\", \n",
    "                                        metadata_csv=\"BERT/BERT_old/metadata_notes_old.csv\", \n",
    "                                        n_jobs=num_cores):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    rows = df.to_dict(\"records\")\n",
    "\n",
    "    # Write notes in parallel\n",
    "    Parallel(n_jobs=n_jobs, prefer=\"threads\")(\n",
    "        delayed(write_single_note_clean)(row, output_dir) for row in rows\n",
    "    )\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_cols = ['subject_id', 'note_id', 'note_type_1', 'charttime']\n",
    "    metadata_df = df.dropna(subset=['subject_id', 'note_id'])\n",
    "    metadata_df = metadata_df[metadata_cols]\n",
    "    metadata_df.to_csv(metadata_csv, index=False)\n",
    "\n",
    "    print(f\"✅ Notes saved to: {output_dir}\")\n",
    "    print(f\"✅ Metadata saved to: {metadata_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efd40843-3906-409e-bb50-7cb99ef68e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def create_bert_dataset_from_notes(metadata_csv, notes_root_dir):\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    def load_text(row):\n",
    "        subject_id = str(row['subject_id'])\n",
    "        note_id = str(row['note_id'])\n",
    "        file_path = os.path.join(notes_root_dir, subject_id, f\"{subject_id}_{note_id}.txt\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    texts = Parallel(n_jobs=num_cores)(delayed(load_text)(row) for _, row in df.iterrows())\n",
    "    df['text'] = texts\n",
    "    df = df[df['text'].str.strip() != \"\"]  # Remove blanks\n",
    "\n",
    "    dataset = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "    print(f\"✅ Dataset created with {len(dataset)} entries\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87d04438-74a9-44af-866b-c0fa3fd27d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Notes saved to: BERT/BERT_old/notes_old_test\n",
      "✅ Metadata saved to: BERT/BERT_old/metadata_notes_old_test.csv\n",
      "Time for 100 notes: 0.14 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test mimic_notes_parallel_for_bert\n",
    "import time\n",
    "\n",
    "sample_df = df.sample(100)\n",
    "start = time.time()\n",
    "write_mimic_notes_parallel_for_bert(df = sample_df, output_dir=\"BERT/BERT_old/notes_old_test\", metadata_csv=\"BERT/BERT_old/metadata_notes_old_test.csv\", n_jobs=1)\n",
    "print(f\"Time for 100 notes: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b65f05c2-7fc3-4091-9c9b-a22589400596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Notes saved to: BERT/BERT_old/notes_old\n",
      "✅ Metadata saved to: BERT/BERT_old/metadata_notes_old.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract and save notes - use df from df_full_notes.csv\n",
    "write_mimic_notes_parallel_for_bert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db34057c-af04-4099-a0f8-5018bccf382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset created with 0 entries\n",
      "Time for 100 notes: 2.11 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test Rebuild dataset\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "create_bert_dataset_from_notes(\"BERT/BERT_old/metadata_notes_old_test.csv\", \"BERT/BERT_old/notes_test\")\n",
    "print(f\"Time for 100 notes: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5df0976f-2051-4677-aa03-9a3a056b1079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset created with 303994 entries\n"
     ]
    }
   ],
   "source": [
    "# Rebuild Dataset\n",
    "bert_dataset = create_bert_dataset_from_notes(\"BERT/BERT_old/metadata_notes_old.csv\", \"BERT/BERT_old/notes_old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00290804-dac3-4ad3-948e-a02401f26f5a",
   "metadata": {},
   "source": [
    "## Part 7 Prepare BERT ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be9f81-e36a-4c3e-bed6-ebf564173807",
   "metadata": {},
   "source": [
    "### Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1c4e4fe-cb91-4103-b708-6da9381eceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ------------------------------\n",
    "# Tokenize dataset with caching\n",
    "# ------------------------------\n",
    "def tokenize_bert_dataset_with_cache(hf_dataset, model_name, cache_dir=\"tokenizer_cache\"):\n",
    "    \"\"\"\n",
    "    Tokenize a HuggingFace Dataset and cache the result.\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Hash dataset + model name to create cache filename\n",
    "    hash_input = \"\".join([str(item) for item in hf_dataset]) + model_name\n",
    "    dataset_hash = hashlib.md5(hash_input.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_dir, f\"tokenized_{dataset_hash}.pkl\")\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached tokenized dataset from {cache_path}\")\n",
    "        tokenized_dataset = joblib.load(cache_path)\n",
    "    else:\n",
    "        print(f\"Tokenizing dataset for {model_name}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenized_dataset = hf_dataset.map(\n",
    "            lambda x: tokenizer(x['text'], truncation=True, padding='max_length', max_length=512),\n",
    "            batched=False\n",
    "        )\n",
    "        joblib.dump(tokenized_dataset, cache_path)\n",
    "        print(f\"Saved tokenized dataset cache: {cache_path}\")\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ea862-afe0-465c-81ad-683ad9dfde4a",
   "metadata": {},
   "source": [
    "### Extract Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fe1a2-6faf-4874-902b-d50766b3cf13",
   "metadata": {},
   "source": [
    "The following code chunk creates a function that is passed into a function factory defined below that accepts a tokenized_dataset and extracts the embeddings. Effectively, a tokenized_dataset is created using tokenize_bert_dataset() but the embeddings still need to be extracted for expanding the feature space and being prepped for model training in run_full_workflow(), to be defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7d3f78c-be61-46a5-9b0e-32f8c3d90de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Patient-level embedding extraction with model-specific caching\n",
    "# ------------------------------\n",
    "def extract_patient_embeddings_from_df(df_embeddings, merge_note_types=True,\n",
    "                                       model_name=None, cache_dir=\"embedding_cache\"):\n",
    "    \"\"\"\n",
    "    Aggregate embeddings per patient, cache per model.\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    hash_input = df_embeddings.to_csv(index=False) + (model_name or \"\")\n",
    "    dataset_hash = hashlib.md5(hash_input.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_dir, f\"patient_embeddings_{dataset_hash}.pkl\")\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached patient embeddings from {cache_path}\")\n",
    "        patient_embeddings_df = joblib.load(cache_path)\n",
    "    else:\n",
    "        patient_embeddings_df = df_embeddings.groupby(['subject_id', 'note_type_1']).mean().reset_index()\n",
    "        joblib.dump(patient_embeddings_df, cache_path)\n",
    "        print(f\"Saved patient embeddings cache: {cache_path}\")\n",
    "\n",
    "    pivoted_dict = {}\n",
    "    for note_type in patient_embeddings_df['note_type_1'].unique():\n",
    "        temp_df = patient_embeddings_df[patient_embeddings_df['note_type_1'] == note_type].copy()\n",
    "        temp_df = temp_df.drop(columns=['note_type_1'])\n",
    "        pivoted_dict[note_type] = temp_df\n",
    "\n",
    "    if merge_note_types:\n",
    "        merged_df = pivoted_dict[list(pivoted_dict.keys())[0]].copy()\n",
    "        merged_df = merged_df.rename(columns={col: f\"{col}_{list(pivoted_dict.keys())[0]}\" \n",
    "                                              for col in merged_df.columns if col != 'subject_id'})\n",
    "        for note_type, df_note in list(pivoted_dict.items())[1:]:\n",
    "            df_renamed = df_note.rename(columns={col: f\"{col}_{note_type}\" \n",
    "                                                 for col in df_note.columns if col != 'subject_id'})\n",
    "            merged_df = pd.merge(merged_df, df_renamed, on='subject_id', how='left')\n",
    "        return merged_df\n",
    "\n",
    "    return pivoted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15182020-7276-4c80-b6e6-f7ef7739e6f3",
   "metadata": {},
   "source": [
    "The below code chunk defines two functions to save embeddings and run a full workflow in order to tokenize, extract embeddings, create pivots, perform radiology only embedding extraction, and finally save the datasets across different BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46069039-2630-4d49-b1e3-cdb972491be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "# Ensure extract_patient_embeddings() is already imported\n",
    "\n",
    "# ------------------------------\n",
    "# Function to save only new embeddings\n",
    "# ------------------------------\n",
    "def save_embeddings_only(merged_df, radiology_df, model_name, base_output_dir=\"BERT/BERT_old\"):\n",
    "    safe_model_name = model_name.replace('/', '_')\n",
    "    model_dir = os.path.join(base_output_dir, safe_model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save merged all-note-types DataFrame\n",
    "    merged_path = os.path.join(model_dir, \"merged_all_note_types.csv\")\n",
    "    merged_df.to_csv(merged_path, index=False)\n",
    "    print(f\"✅ Merged all-note-types DataFrame saved to: {merged_path}\")\n",
    "\n",
    "    # Save radiology-only DataFrame\n",
    "    radiology_path = os.path.join(model_dir, \"merged_radiology_only.csv\")\n",
    "    radiology_df.to_csv(radiology_path, index=False)\n",
    "    print(f\"✅ Radiology-only DataFrame saved to: {radiology_path}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Validation helper\n",
    "# ------------------------------\n",
    "def validate_bert_dataset(dataset, required_columns=None):\n",
    "    if required_columns is None:\n",
    "        required_columns = ['subject_id', 'note_type_1', 'text']\n",
    "\n",
    "    # HuggingFace Dataset\n",
    "    if isinstance(dataset, Dataset):\n",
    "        dataset_columns = dataset.column_names\n",
    "    # pandas DataFrame\n",
    "    elif isinstance(dataset, pd.DataFrame):\n",
    "        dataset_columns = dataset.columns.tolist()\n",
    "    else:\n",
    "        raise TypeError(\"Dataset must be a pandas DataFrame or HuggingFace Dataset.\")\n",
    "\n",
    "    missing = [col for col in required_columns if col not in dataset_columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"❌ Dataset is missing required columns: {missing}\")\n",
    "\n",
    "    print(f\"✅ Dataset validated. Columns present: {dataset_columns}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647a388-327f-43ac-91bd-30df2808aaaa",
   "metadata": {},
   "source": [
    "### Full Workflow Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5514c9fb-322c-4602-bfdb-b5097bfa2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "# ------------------------------\n",
    "# Full workflow with progress ETA\n",
    "# ------------------------------\n",
    "def run_full_workflow(raw_notes_df, df_clean, bert_models,\n",
    "                      batch_size=16, cache_dir=\"BERT/BERT_cache\"):\n",
    "    \"\"\"\n",
    "    End-to-end workflow with:\n",
    "      - tokenization\n",
    "      - GPU batching + caching of embeddings\n",
    "      - patient-level aggregation (via extract_patient_embeddings_from_df)\n",
    "      - merge with cleaned clinical data\n",
    "      - radiology-only extraction\n",
    "      - saving outputs for multiple BERT models\n",
    "      \n",
    "    Displays progress checkpoints with ETA estimates.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if isinstance(raw_notes_df, pd.DataFrame):\n",
    "        hf_dataset = Dataset.from_pandas(raw_notes_df.reset_index(drop=True))\n",
    "        print(f\"✅ Converted raw_notes DataFrame to HuggingFace Dataset with {len(hf_dataset)} entries\")\n",
    "    else:\n",
    "        hf_dataset = raw_notes_df\n",
    "        print(f\"✅ Using provided HuggingFace Dataset with {len(hf_dataset)} entries\")\n",
    "\n",
    "    validate_bert_dataset(hf_dataset)\n",
    "\n",
    "    for model_name in bert_models:\n",
    "        print(f\"\\n=== Processing {model_name} ===\")\n",
    "        start_model_time = time.time()\n",
    "\n",
    "        # ------------------------------\n",
    "        # 1️⃣ Tokenize with caching\n",
    "        # ------------------------------\n",
    "        start_time = time.time()\n",
    "        tokenized_dataset = tokenize_bert_dataset_with_cache(\n",
    "            hf_dataset, model_name,\n",
    "            cache_dir=os.path.join(cache_dir, \"tokenizer_cache\")\n",
    "        )\n",
    "        print(f\"Tokenization completed in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # 2️⃣ GPU batching + embedding extraction with caching\n",
    "        # ------------------------------\n",
    "        embeddings_hash_input = \"\".join([str(item) for item in tokenized_dataset]) + model_name\n",
    "        embeddings_cache_path = os.path.join(cache_dir, f\"embeddings_{hashlib.md5(embeddings_hash_input.encode()).hexdigest()}.pkl\")\n",
    "\n",
    "        if os.path.exists(embeddings_cache_path):\n",
    "            print(f\"Loading cached embeddings from {embeddings_cache_path}\")\n",
    "            embeddings_df = joblib.load(embeddings_cache_path)\n",
    "        else:\n",
    "            print(f\"Extracting embeddings for {model_name}...\")\n",
    "            model = AutoModel.from_pretrained(model_name)\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            # ------------------------------\n",
    "            # Determine VRAM-safe batch size\n",
    "            # ------------------------------\n",
    "            if device.type == \"cuda\":\n",
    "                total_vram = torch.cuda.get_device_properties(device).total_memory\n",
    "                reserved_vram = torch.cuda.memory_reserved(device)\n",
    "                free_vram = total_vram - reserved_vram\n",
    "                # Rough heuristic: 1.5MB per token per sample, plus buffer for long sequences\n",
    "                max_tokens = max(len(item['input_ids']) for item in tokenized_dataset)\n",
    "                estimated_bytes_per_sample = 1.5e6 * max_tokens / 512  # scale relative to typical 512 tokens\n",
    "                safe_batch_size = max(1, int(free_vram / (estimated_bytes_per_sample * 1.2)))  # 20% buffer\n",
    "                batch_size = min(batch_size, safe_batch_size)\n",
    "                print(f\"⚡ Adjusted batch size for GPU memory: {batch_size}\")\n",
    "\n",
    "            dataloader = DataLoader(tokenized_dataset, batch_size=batch_size,\n",
    "                                    collate_fn=lambda batch: {\n",
    "                                        'input_ids': torch.stack([torch.tensor(b['input_ids']) for b in batch]).to(device),\n",
    "                                        'attention_mask': torch.stack([torch.tensor(b['attention_mask']) for b in batch]).to(device),\n",
    "                                        'subject_id': [b['subject_id'] for b in batch],\n",
    "                                        'note_type_1': [b['note_type_1'] for b in batch]\n",
    "                                    })\n",
    "\n",
    "            embeddings_list = []\n",
    "            subject_ids = []\n",
    "            note_types = []\n",
    "\n",
    "            total_batches = len(dataloader)\n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(tqdm(dataloader, desc=\"Embedding batches\", unit=\"batch\")):\n",
    "                    batch_start = time.time()\n",
    "                    try:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "                            emb = outputs.pooler_output.cpu().numpy()\n",
    "                    except RuntimeError as e:\n",
    "                        if 'out of memory' in str(e):\n",
    "                            print(f\"⚠️ CUDA OOM at batch {i+1}, reducing batch size and retrying...\")\n",
    "                            torch.cuda.empty_cache()\n",
    "                            # Simple fallback: process batch one by one\n",
    "                            emb_list = []\n",
    "                            for j in range(len(batch['input_ids'])):\n",
    "                                single_input = batch['input_ids'][j].unsqueeze(0)\n",
    "                                single_mask = batch['attention_mask'][j].unsqueeze(0)\n",
    "                                with torch.cuda.amp.autocast():\n",
    "                                    single_output = model(input_ids=single_input, attention_mask=single_mask)\n",
    "                                    emb_list.append(single_output.pooler_output.cpu().numpy())\n",
    "                            emb = np.vstack(emb_list)\n",
    "                        else:\n",
    "                            raise e\n",
    "                    embeddings_list.append(emb)\n",
    "                    subject_ids.extend(batch['subject_id'])\n",
    "                    note_types.extend(batch['note_type_1'])\n",
    "            \n",
    "                    elapsed = time.time() - batch_start\n",
    "                    remaining = elapsed * (total_batches - i - 1)\n",
    "                    if i % 50 == 0 or i == total_batches - 1:\n",
    "                        print(f\"Batch {i+1}/{total_batches} complete, ETA ~{remaining/60:.1f} min\")\n",
    "\n",
    "            # Preallocate embeddings and ID arrays to avoid repeated appends/vstack\n",
    "            total_rows = sum(emb.shape[0] for emb in embeddings_list)\n",
    "            embedding_dim = embeddings_list[0].shape[1]\n",
    "            all_embeddings = np.empty((total_rows, embedding_dim), dtype=np.float32)\n",
    "            all_subject_ids = [None] * total_rows\n",
    "            all_note_types = [None] * total_rows\n",
    "            \n",
    "            start = 0\n",
    "            for emb, ids, notes in zip(embeddings_list, subject_ids, note_types):\n",
    "                n = emb.shape[0]\n",
    "                all_embeddings[start:start+n, :] = emb\n",
    "                all_subject_ids[start:start+n] = ids if isinstance(ids, list) else [ids]\n",
    "                all_note_types[start:start+n] = notes if isinstance(notes, list) else [notes]\n",
    "                start += n\n",
    "            \n",
    "            embeddings_df = pd.DataFrame(all_embeddings, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\n",
    "            embeddings_df['subject_id'] = all_subject_ids\n",
    "            embeddings_df['note_type_1'] = all_note_types\n",
    "\n",
    "            joblib.dump(embeddings_df, embeddings_cache_path)\n",
    "            print(f\"Saved embeddings cache: {embeddings_cache_path}\")\n",
    "            del model\n",
    "            try:\n",
    "                del outputs\n",
    "            except NameError:\n",
    "                pass\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # ------------------------------\n",
    "        # 3️⃣ Patient-level aggregation with caching\n",
    "        # ------------------------------\n",
    "        agg_cache_path = os.path.join(cache_dir, f\"patient_agg_{model_name.replace('/', '_')}.pkl\")\n",
    "        if os.path.exists(agg_cache_path):\n",
    "            print(f\"Loading cached patient-level embeddings from {agg_cache_path}\")\n",
    "            merged_df = joblib.load(agg_cache_path)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            merged_df = extract_patient_embeddings_from_df(\n",
    "                embeddings_df, merge_note_types=True,\n",
    "                model_name=model_name,\n",
    "                cache_dir=cache_dir\n",
    "            )\n",
    "            joblib.dump(merged_df, agg_cache_path)\n",
    "            print(f\"Patient aggregation completed in {time.time() - start_time:.1f}s\")\n",
    "            print(f\"Saved patient-level cache: {agg_cache_path}\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # 4️⃣ Merge with cleaned clinical data\n",
    "        # ------------------------------\n",
    "        merged_with_cleaned = pd.merge(df_clean, merged_df, on='subject_id', how='left')\n",
    "\n",
    "        # ------------------------------\n",
    "        # 5️⃣ Radiology-only DataFrame\n",
    "        # ------------------------------\n",
    "        radiology_cols = [col for col in merged_with_cleaned.columns if '_radiology' in col]\n",
    "        radiology_df = pd.concat([merged_with_cleaned['subject_id'], merged_with_cleaned[radiology_cols]], axis=1)\n",
    "\n",
    "        # ------------------------------\n",
    "        # 6️⃣ Save outputs\n",
    "        # ------------------------------\n",
    "        output_dir = os.path.join(cache_dir, model_name.replace('/', '_'))\n",
    "        save_embeddings_only(merged_with_cleaned, radiology_df, model_name, base_output_dir=output_dir)\n",
    "\n",
    "        print(f\"✅ Completed workflow for {model_name} in {(time.time() - start_model_time)/60:.1f} min\")\n",
    "        print(f\"   - All note types shape: {merged_with_cleaned.shape}\")\n",
    "        print(f\"   - Radiology-only shape: {radiology_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c62923-88ac-4fce-bdfc-6572f16350fe",
   "metadata": {},
   "source": [
    "### Embedding Extraction and Assembling BERT Datasets for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5ea70-4b79-4583-b2b4-d8f5f23519c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Perform Embedding Extraction and BERT Dataset Assembly for Each Transformer\n",
    "# ------------------------------\n",
    "bert_models = [\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'dmis-lab/biobert-base-cased-v1.2',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "]\n",
    "\n",
    "# raw_notes_df: can be either a pandas DataFrame or a HuggingFace Dataset\n",
    "# df_clean: your clinical/demographic dataset\n",
    "run_full_workflow(\n",
    "    raw_notes_df=bert_dataset,   # your notes dataset\n",
    "    df_clean=df_clean,           # your clinical dataset\n",
    "    bert_models=bert_models,     # list of BERT models\n",
    "    batch_size=16,               # optional, VRAM-adaptive will adjust automatically\n",
    "    cache_dir=\"BERT/BERT_cache\"  # optional, where tokenization/embedding caches go\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c4fb5-7b57-4b56-afbe-8049fc436500",
   "metadata": {},
   "source": [
    "Proceed to `sepsis_model_training.ipynb` for model training, testing, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
