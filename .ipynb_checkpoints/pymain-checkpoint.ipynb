{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ed17d5-3441-42fa-9772-e9c6d22ad61f",
   "metadata": {},
   "source": [
    "##### Improving Prediction Accuracy of Sepsis Mortality using Machine Learning and Natural Language Processing\n",
    "## Tyler Kelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e5ce0-29f8-4607-b46c-364598c412a7",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd196b-2a8c-476f-8122-5da74fd724e6",
   "metadata": {},
   "source": [
    "## Set Up and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4bbcd-afdf-4f3d-b47b-1f9216e5c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets transformers pandas shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c39c3",
   "metadata": {},
   "source": [
    "## Part 0 Preprocessing (Pull Code from Author's ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5889d9",
   "metadata": {},
   "source": [
    "The following code is adapted from the github repository 'https://github.com/yuyinglu2000/Sepsis-Mortality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5e92c",
   "metadata": {},
   "source": [
    "### Data Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907322",
   "metadata": {},
   "source": [
    "Begin by creating bigquery search to get the 38 unique features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('Data/data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape\n",
    "# Expect to get a dataframe 808188x38 (38 old columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "# regroup the race\n",
    "race_mapping = {\n",
    "    'WHITE': 'White',\n",
    "    'HISPANIC OR LATINO': 'Hispanic or Latin',\n",
    "    'BLACK/AFRICAN AMERICAN': 'Black or African American',\n",
    "    'BLACK/CARIBBEAN ISLAND': 'Black or African American',\n",
    "    'HISPANIC/LATINO - DOMINICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - CENTRAL AMERICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - GUATEMALAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - PUERTO RICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - SALVADORAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - HONDURAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - MEXICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - CUBAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - COLUMBIAN': 'Hispanic or Latin',\n",
    "    'BLACK/CAPE VERDEAN': 'Black or African American',\n",
    "    'BLACK/AFRICAN': 'Black or African American',\n",
    "    'SOUTH AMERICAN': 'Hispanic or Latin',\n",
    "    'WHITE - BRAZILIAN': 'Hispanic or Latin',\n",
    "    'WHITE - OTHER EUROPEAN': 'White',\n",
    "    'WHITE - RUSSIAN': 'White',\n",
    "    'WHITE - EASTERN EUROPEAN': 'White',\n",
    "    'ASIAN': 'Others race',\n",
    "    'ASIAN - SOUTH EAST ASIAN': 'Others race',\n",
    "    'ASIAN - CHINESE': 'Others race',\n",
    "    'ASIAN - ASIAN INDIAN': 'Others race',\n",
    "    'ASIAN - KOREAN': 'Others race',\n",
    "    'AMERICAN INDIAN/ALASKA NATIVE': 'Others race',\n",
    "    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'Others race',\n",
    "    'MULTIPLE RACE/ETHNICITY': 'Others race',\n",
    "    'PORTUGUESE': 'Others race',\n",
    "    'UNKNOWN': 'Others race',\n",
    "    'OTHER': 'Others race',\n",
    "    'UNABLE TO OBTAIN': 'Others race',\n",
    "    'PATIENT DECLINED TO ANSWER': 'Others race'\n",
    "}\n",
    "\n",
    "df_raw['race'] = df_raw['race'].map(race_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# df = ... (your DataFrame)\n",
    "\n",
    "# Define a mapping for antibiotics to their respective groups\n",
    "antibiotic_mapping = {\n",
    "    'Gentamicin Sulfate': 'Aminoglycoside',\n",
    "    'Tobramycin Sulfate': 'Aminoglycoside',\n",
    "    'Streptomycin Sulfate': 'Aminoglycoside',\n",
    "    'Neomycin Sulfate': 'Aminoglycoside',\n",
    "    'Neomycin/Polymyxin B Sulfate': 'Aminoglycoside',\n",
    "    'Meropenem': 'Carbapenem',\n",
    "    'Meropenem Graded Challenge': 'Carbapenem',\n",
    "    'Vancomycin': 'Glycopeptide',\n",
    "    'Vancomycin Oral Liquid': 'Glycopeptide',\n",
    "    'Vancomycin Antibiotic Lock': 'Glycopeptide',\n",
    "    'Vancomycin Enema': 'Glycopeptide',\n",
    "    'Vancomycin Intrathecal': 'Glycopeptide',\n",
    "    'Vancomycin Ora': 'Glycopeptide',\n",
    "    'Linezolid': 'Oxazolidinone',\n",
    "    'Linezolid Suspension': 'Oxazolidinone',\n",
    "    'Penicillin G Benzathine': 'Penicillin',\n",
    "    'Penicillin G Potassium': 'Penicillin',\n",
    "    'Penicillin V Potassium': 'Penicillin',\n",
    "    'Sulfameth/Trimethoprim': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim DS': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim SS': 'Sulfonamide',\n",
    "    'Sulfamethoxazole-Trimethoprim': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim Suspension': 'Sulfonamide',\n",
    "    'Tetracycline': 'Tetracycline',\n",
    "    'Tetracycline HCl': 'Tetracycline'\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Applying the mapping to the 'antibiotic' column\n",
    "df_raw['antibiotic'] = df_raw['antibiotic'].map(antibiotic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56745eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "df_raw['antibiotic'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9cc603",
   "metadata": {},
   "source": [
    "### Get Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "df_encoded = pd.get_dummies(df_raw, columns=df_raw.select_dtypes(include=['object']).columns)\n",
    "df_dropped = df_encoded.dropna()\n",
    "df_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check empty values *for tetracycline* ###\n",
    "#AC\n",
    "empty_values = df_dropped['antibiotic_Tetracycline'].isnull().any()\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38a1c0",
   "metadata": {},
   "source": [
    "After applying get_dummy_variables there is now 53 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ebb55",
   "metadata": {},
   "source": [
    "### Drop Duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC with minor edits\n",
    "\n",
    "duplicated_rows_mask = df_dropped['subject_id'].duplicated(keep=False)\n",
    "\n",
    "# Extract the duplicated rows\n",
    "duplicated_rows = df_dropped[duplicated_rows_mask]\n",
    "new_data  = df_dropped.drop_duplicates()\n",
    "duplicated_rows_mask = new_data['subject_id'].duplicated(keep=False)\n",
    "\n",
    "# Extract the duplicated rows\n",
    "duplicated_rows = new_data[duplicated_rows_mask]\n",
    "# Separate out columns based on data types\n",
    "int_float_cols = new_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "uint8_cols = new_data.select_dtypes(include=['uint8']).columns\n",
    "\n",
    "# Sort dataframe\n",
    "# For int and float columns: sort in descending order so that larger values come first\n",
    "df_raw = new_data.sort_values(by=list(int_float_cols), ascending=False)\n",
    "\n",
    "# For uint8 columns: sort in descending order so that 1 comes before 0\n",
    "df_raw = df_raw.sort_values(by=list(uint8_cols), ascending=False)\n",
    "\n",
    "# Drop duplicates based on subject_id, keeping the first (which are the desired rows after sorting)\n",
    "df_reduced = df_raw.drop_duplicates(subset='subject_id', keep='first')\n",
    "\n",
    "# Reset index if needed\n",
    "df_reduced = df_reduced.reset_index(drop=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82be4d9",
   "metadata": {},
   "source": [
    "After reducing the dataframe we get the 6401 patients reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec042c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab3776-07b1-477e-bf34-aec08063d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subject = df_reduced['subject_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subject.to_csv(\"data_subject_id_ready_to_query.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5df87b",
   "metadata": {},
   "source": [
    "## Part 1 Upload data_ready_to_merge.csv to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905776e4",
   "metadata": {},
   "source": [
    "Upload the subject_id dataset to bigquery and merge the dataset to radiology and discharge notes separately. Save the downloaded file to downloads (or find a way to save it directly to my BIOST 2021 Thesis / Main ) as\n",
    "'data_radiology_notes.csv' and 'data_discharge_notes.csv' then joing below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edac5ba",
   "metadata": {},
   "source": [
    "Place SQL code chunks below (if time write them into the script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6943900",
   "metadata": {},
   "source": [
    "# READ THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987127df",
   "metadata": {},
   "source": [
    "data_full_notes_old.csv uses an old outdated dataset but I haven't figured out the correct sql to get the correct dataset at 6401.\n",
    "\n",
    "For now, I will use `df_old` when using the outdated dataset and `df` when I get the new corrected one.\n",
    "\n",
    "To get the radiology and discharge notes, I uploaded the data_after_cleaning table to big query and used the below code sql query to get a table with\n",
    "data_after_cleaning joined to the notes columns found in the mimic-iv discharge and radiology tables.\n",
    "\n",
    "Above, using the correct subject_id list to get the 6401 patients, I use the same method to obtain the rad_notes and discharge_notes tables using bigquery, and join in an identical manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de455f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Old/data_full_notes_old.csv') # df_old\n",
    "# change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737320aa-c0be-465b-b531-2b7c35bfd481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303994, 58)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25600a63-6c3b-41a4-9c75-42c7e346c246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303994 entries, 0 to 303993\n",
      "Data columns (total 58 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   int64_field_0                          303994 non-null  int64  \n",
      " 1   subject_id                             303994 non-null  int64  \n",
      " 2   hospital_expire_flag                   303994 non-null  int64  \n",
      " 3   max_age                                303994 non-null  int64  \n",
      " 4   los_icu                                303994 non-null  float64\n",
      " 5   first_hosp_stay                        303994 non-null  bool   \n",
      " 6   suspected_infection                    303994 non-null  int64  \n",
      " 7   sofa_score                             303994 non-null  int64  \n",
      " 8   sepsis3                                303994 non-null  bool   \n",
      " 9   avg_urineoutput                        303994 non-null  float64\n",
      " 10  glucose_min                            303994 non-null  int64  \n",
      " 11  glucose_max                            303994 non-null  int64  \n",
      " 12  glucose_average                        303994 non-null  float64\n",
      " 13  sodium_max                             303994 non-null  int64  \n",
      " 14  sodium_min                             303994 non-null  int64  \n",
      " 15  sodium_average                         303994 non-null  float64\n",
      " 16  diabetes_without_cc                    303994 non-null  int64  \n",
      " 17  diabetes_with_cc                       303994 non-null  int64  \n",
      " 18  severe_liver_disease                   303994 non-null  int64  \n",
      " 19  aids                                   303994 non-null  int64  \n",
      " 20  renal_disease                          303994 non-null  int64  \n",
      " 21  heart_rate_min                         303994 non-null  int64  \n",
      " 22  heart_rate_max                         303994 non-null  int64  \n",
      " 23  heart_rate_mean                        303994 non-null  float64\n",
      " 24  sbp_min                                303994 non-null  float64\n",
      " 25  sbp_max                                303994 non-null  float64\n",
      " 26  sbp_mean                               303994 non-null  float64\n",
      " 27  dbp_min                                303994 non-null  float64\n",
      " 28  dbp_max                                303994 non-null  float64\n",
      " 29  dbp_mean                               303994 non-null  float64\n",
      " 30  resp_rate_min                          303994 non-null  float64\n",
      " 31  resp_rate_max                          303994 non-null  float64\n",
      " 32  resp_rate_mean                         303994 non-null  float64\n",
      " 33  spo2_min                               303994 non-null  int64  \n",
      " 34  spo2_max                               303994 non-null  int64  \n",
      " 35  spo2_mean                              303994 non-null  float64\n",
      " 36  coma                                   303994 non-null  int64  \n",
      " 37  albumin                                303994 non-null  float64\n",
      " 38  race_Black or African American         303994 non-null  int64  \n",
      " 39  race_Hispanic or Latin                 303994 non-null  int64  \n",
      " 40  race_Others race                       303994 non-null  int64  \n",
      " 41  race_White                             303994 non-null  int64  \n",
      " 42  antibiotic_Vancomycin                  303994 non-null  int64  \n",
      " 43  antibiotic_Vancomycin Antibiotic Lock  303994 non-null  int64  \n",
      " 44  antibiotic_Vancomycin Enema            303994 non-null  int64  \n",
      " 45  antibiotic_Vancomycin Intrathecal      303994 non-null  int64  \n",
      " 46  antibiotic_Vancomycin Oral Liquid      303994 non-null  int64  \n",
      " 47  gender_F                               303994 non-null  int64  \n",
      " 48  gender_M                               303994 non-null  int64  \n",
      " 49  note_id                                303994 non-null  object \n",
      " 50  subject_id_1                           303994 non-null  int64  \n",
      " 51  hadm_id                                237446 non-null  float64\n",
      " 52  note_type                              303994 non-null  object \n",
      " 53  note_seq                               303994 non-null  int64  \n",
      " 54  charttime                              303994 non-null  object \n",
      " 55  storetime                              303994 non-null  object \n",
      " 56  text                                   303994 non-null  object \n",
      " 57  note_type_1                            303994 non-null  object \n",
      "dtypes: bool(2), float64(17), int64(33), object(6)\n",
      "memory usage: 130.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76a458-f382-4e39-93c8-fabdf0b024b3",
   "metadata": {},
   "source": [
    "## Part 2 Truncate Notes for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013c98a8-9450-4fb4-9cd0-e3ae8d1adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83765b2e-1ba0-4f72-94e6-0060b027568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Define cleaning function - Clean individual note text ===\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'_+', '', text)    # Remove underlines\n",
    "    text = re.sub(r'[^\\w\\s.,:;!?()\\-\\n]', '', text)  # Remove junk, keep clinical symbols\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1012ffaf-a180-45ac-b82f-8eda85aeca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Function to process one group. Process a group into (subject_id, note_type, combined_notes) ===\n",
    "def process_group(record):\n",
    "    subject_id = record['subject_id']\n",
    "    note_type_1 = record['note_type_1'] #this column identifies addendums and base notes to just be 'radiology' notes\n",
    "    texts = record['text']\n",
    "    cleaned_notes = [clean_text(text) for text in texts]\n",
    "    combined_notes = \" \".join(cleaned_notes)\n",
    "    return {\n",
    "        'subject_id': subject_id,\n",
    "        'note_type_1': note_type_1,\n",
    "        'combined_notes': combined_notes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7893d23a-1fe0-48c2-b711-19aac68c1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Group the notes. Load and group your data ===\n",
    "# Replace this with your actual loading logic / dataframe\n",
    "# Data loaded above as `df`\n",
    "grouped_df = (\n",
    "    df.groupby(['subject_id', 'note_type_1'])['text']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "records = grouped_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605c2269-543d-4012-89fa-e4e3f910979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Parallel processing with joblib ===\n",
    "\n",
    "#this num_cores variable is used later for other parallel processing jobs\n",
    "num_cores = multiprocessing.cpu_count() - 1\n",
    "\n",
    "processed = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_group)(record) for record in records\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8e925-2a6a-4d6c-bea6-4c3915c06594",
   "metadata": {},
   "source": [
    "The following step creates a long dataframe for notes per patient by type (some patients have radiology, discharge, both, or none for notes), it is later converted to wide to potentially compare NLP to see if discharge notes significantly provide insight into mortality rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a510f691-b265-4c3c-8151-619dcdd69e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Create DataFrame and save to csv ===\n",
    "nlp_long_df = pd.DataFrame(processed).sort_values(by=['subject_id', 'note_type_1'])\n",
    "\n",
    "nlp_long_df.to_csv(\"Data/Old/data_trunc_notes_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96bbd31a-3e65-40d6-9174-fa9dc34d7d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10402, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_long_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db47d90-108d-4afa-a844-81be9b9fd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 6: Pivot to wide format for multiple columns ===\n",
    "# Convert note_type to columns like 'Radiology_notes', etc.\n",
    "nlp_wide_df = nlp_long_df.pivot(\n",
    "    index='subject_id',\n",
    "    columns='note_type_1',\n",
    "    values='combined_notes'\n",
    ").reset_index()\n",
    "\n",
    "nlp_wide_df.columns.name = None # Remove category label\n",
    "\n",
    "# Rename columns to make clear\n",
    "nlp_wide_df = nlp_wide_df.rename(columns={\n",
    "    'radiology': 'Radiology_notes',\n",
    "    'discharge': 'Discharge_summary_notes'\n",
    "})\n",
    "\n",
    "nlp_wide_df = nlp_wide_df.fillna(\"\") #fills NA columns with empty strings\n",
    "\n",
    "# Save\n",
    "nlp_wide_df.to_csv(\"Data/Old/data_trunc_notes_wide_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2be1ee-2fbb-400f-a13e-9d3bb14dff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 7: Combine Radiology and Discharge notes per subject_id ===\n",
    "nlp_combined_df = nlp_wide_df.copy()\n",
    "\n",
    "# Concatenate the two columns into one\n",
    "nlp_combined_df['combined_notes'] = (\n",
    "    nlp_combined_df['Radiology_notes'].str.strip() + \" \" +\n",
    "    nlp_combined_df['Discharge_summary_notes'].str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Combined DataFrame with just subject_id + combined text\n",
    "nlp_combined_notes_df = nlp_combined_df[['subject_id', 'combined_notes']]\n",
    "\n",
    "# Save\n",
    "nlp_combined_notes_df.to_csv(\"Data/Old/data_trunc_notes_combined_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67738c-a9c8-4bad-825d-956d067ca1d6",
   "metadata": {},
   "source": [
    "The code chunk below creates `nlp_ready_df`, a dataframe that includes a row for each subject_id and appends 3 new columns with all text truncated based on radiology notes, discharge notes, and combined radiology and discharged notes. I lose information related to the note_id, note_id_type, and more importantly charttime, but here it allows word2vec to work properly. This part of my thesis focuses more on using embeddings from the clinical text to aid in predicting mortality rather than finding the best way to do it (i.e. finding the best time of day where it is more likely to happen than not, or finding the best drug at predicting it, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72cdbd3d-3399-4449-80c6-648bd73160c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 8: Join nlp_wide_df and nlp_combined_notes_df to data_after_cleaning ===\n",
    "\n",
    "# === Step i: Load the original cleaned dataset, df_reduced ===\n",
    "# ie this is df_reduced\n",
    "\n",
    "# For now, this is `df_clean` the cleaned dataset.\n",
    "\n",
    "df_clean = pd.read_csv('Data/Old/data_after_cleaning.csv')\n",
    "\n",
    "# === Step ii: Merge df_reduced with nlp_wide_df ===\n",
    "# This adds the radiology and discharge notes as 2 new columns to df_reduced\n",
    "# use data_clean until df_reduced is finalized\n",
    "\n",
    "nlp_ready_df = df_clean.merge(\n",
    "    nlp_wide_df,\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# === Step iii: Merge with combined notes ===\n",
    "# This adds one new column of all notes combined together as a single note (per patient) to the nlp_ready_df above\n",
    "nlp_ready_df = nlp_ready_df.merge(\n",
    "    nlp_combined_notes_df,\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Save\n",
    "nlp_ready_df.to_csv(\"Data/Old/data_nlp_ready_old.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "094ddd80-f078-4f19-bafd-bb544b64d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 9: Check shape of dataframes ===\n",
    "nlp_wide_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3f044b1-4c59-4cb6-b74c-32d9de161bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_combined_notes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e18979ae-ac9d-4c3e-aeb8-13bd72afdfa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reduced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_reduced\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reduced' is not defined"
     ]
    }
   ],
   "source": [
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d43814b0-307d-42b1-b565-1316c6bb1821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 51)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ready_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43007975-bac1-499e-bcd9-33695e1f7dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hospital_expire_flag', 'max_age', 'los_icu',\n",
       "       'first_hosp_stay', 'suspected_infection', 'sofa_score', 'sepsis3',\n",
       "       'avg_urineoutput', 'glucose_min', 'glucose_max', 'glucose_average',\n",
       "       'sodium_max', 'sodium_min', 'sodium_average', 'diabetes_without_cc',\n",
       "       'diabetes_with_cc', 'severe_liver_disease', 'aids', 'renal_disease',\n",
       "       'heart_rate_min', 'heart_rate_max', 'heart_rate_mean', 'sbp_min',\n",
       "       'sbp_max', 'sbp_mean', 'dbp_min', 'dbp_max', 'dbp_mean',\n",
       "       'resp_rate_min', 'resp_rate_max', 'resp_rate_mean', 'spo2_min',\n",
       "       'spo2_max', 'spo2_mean', 'coma', 'albumin',\n",
       "       'race_Black or African American', 'race_Hispanic or Latin',\n",
       "       'race_Others race', 'race_White', 'antibiotic_Vancomycin',\n",
       "       'antibiotic_Vancomycin Antibiotic Lock', 'antibiotic_Vancomycin Enema',\n",
       "       'antibiotic_Vancomycin Intrathecal',\n",
       "       'antibiotic_Vancomycin Oral Liquid', 'gender_F', 'gender_M',\n",
       "       'Discharge_summary_notes', 'Radiology_notes', 'combined_notes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ready_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccb06cff-23d0-4d94-ae50-e6dae3d3c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5208 entries, 0 to 5207\n",
      "Data columns (total 51 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   subject_id                             5208 non-null   int64  \n",
      " 1   hospital_expire_flag                   5208 non-null   int64  \n",
      " 2   max_age                                5208 non-null   int64  \n",
      " 3   los_icu                                5208 non-null   float64\n",
      " 4   first_hosp_stay                        5208 non-null   bool   \n",
      " 5   suspected_infection                    5208 non-null   int64  \n",
      " 6   sofa_score                             5208 non-null   int64  \n",
      " 7   sepsis3                                5208 non-null   bool   \n",
      " 8   avg_urineoutput                        5208 non-null   float64\n",
      " 9   glucose_min                            5208 non-null   int64  \n",
      " 10  glucose_max                            5208 non-null   int64  \n",
      " 11  glucose_average                        5208 non-null   float64\n",
      " 12  sodium_max                             5208 non-null   int64  \n",
      " 13  sodium_min                             5208 non-null   int64  \n",
      " 14  sodium_average                         5208 non-null   float64\n",
      " 15  diabetes_without_cc                    5208 non-null   int64  \n",
      " 16  diabetes_with_cc                       5208 non-null   int64  \n",
      " 17  severe_liver_disease                   5208 non-null   int64  \n",
      " 18  aids                                   5208 non-null   int64  \n",
      " 19  renal_disease                          5208 non-null   int64  \n",
      " 20  heart_rate_min                         5208 non-null   int64  \n",
      " 21  heart_rate_max                         5208 non-null   int64  \n",
      " 22  heart_rate_mean                        5208 non-null   float64\n",
      " 23  sbp_min                                5208 non-null   float64\n",
      " 24  sbp_max                                5208 non-null   float64\n",
      " 25  sbp_mean                               5208 non-null   float64\n",
      " 26  dbp_min                                5208 non-null   float64\n",
      " 27  dbp_max                                5208 non-null   float64\n",
      " 28  dbp_mean                               5208 non-null   float64\n",
      " 29  resp_rate_min                          5208 non-null   float64\n",
      " 30  resp_rate_max                          5208 non-null   float64\n",
      " 31  resp_rate_mean                         5208 non-null   float64\n",
      " 32  spo2_min                               5208 non-null   int64  \n",
      " 33  spo2_max                               5208 non-null   int64  \n",
      " 34  spo2_mean                              5208 non-null   float64\n",
      " 35  coma                                   5208 non-null   int64  \n",
      " 36  albumin                                5208 non-null   float64\n",
      " 37  race_Black or African American         5208 non-null   int64  \n",
      " 38  race_Hispanic or Latin                 5208 non-null   int64  \n",
      " 39  race_Others race                       5208 non-null   int64  \n",
      " 40  race_White                             5208 non-null   int64  \n",
      " 41  antibiotic_Vancomycin                  5208 non-null   int64  \n",
      " 42  antibiotic_Vancomycin Antibiotic Lock  5208 non-null   int64  \n",
      " 43  antibiotic_Vancomycin Enema            5208 non-null   int64  \n",
      " 44  antibiotic_Vancomycin Intrathecal      5208 non-null   int64  \n",
      " 45  antibiotic_Vancomycin Oral Liquid      5208 non-null   int64  \n",
      " 46  gender_F                               5208 non-null   int64  \n",
      " 47  gender_M                               5208 non-null   int64  \n",
      " 48  Discharge_summary_notes                5208 non-null   object \n",
      " 49  Radiology_notes                        5208 non-null   object \n",
      " 50  combined_notes                         5208 non-null   object \n",
      "dtypes: bool(2), float64(16), int64(30), object(3)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "nlp_ready_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcaabe-ba2d-498d-9f79-1f2cad58c687",
   "metadata": {},
   "source": [
    "## Part 3 Create Note File for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79f1a367-d882-4b2f-929b-4d2f7b35b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Write the 'Radiology_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_Radiology_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"Radiology_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4c2c47b-ef7d-41f1-81d7-439470764fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the 'Discharge_summary_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_Discharge_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"Discharge_summary_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c3a5d89-c689-4fbd-9499-9e172a4aa1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the 'combined_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_combined_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"combined_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d0c99",
   "metadata": {},
   "source": [
    "## Part 4 Prepare Word2Vec - Proceed to main.rmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3d2f9",
   "metadata": {},
   "source": [
    "## Part 5 Model Training - Proceed to Sepsis_Model_Training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c224e",
   "metadata": {},
   "source": [
    "After completing / running / saving models in model training, upload them into the workspace in the following code chunks if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bc3e0-7f90-41dd-856d-6673a057c292",
   "metadata": {},
   "source": [
    "## Part 6 Create Dataset for Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073ba80-5b33-4f36-8a69-d8d5c662f38c",
   "metadata": {},
   "source": [
    "Metadata csv files for radiology and discharge are created and saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63d4ca06-89f1-4103-b8f3-14b5b3a6b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def write_single_note_clean(row, output_dir):\n",
    "    try:\n",
    "        subject_id = str(row[\"subject_id\"]).strip()\n",
    "        note_id = str(row[\"note_id\"]).strip()\n",
    "        note_text = row.get(\"text\", \"\")\n",
    "\n",
    "        # Ensure note_text is a clean string\n",
    "        if not isinstance(note_text, str):\n",
    "            note_text = \"\" if pd.isna(note_text) else str(note_text)\n",
    "        note_text = note_text.strip()\n",
    "\n",
    "        # Skip empty notes\n",
    "        if not note_text:\n",
    "            return\n",
    "\n",
    "        # Make subject directory\n",
    "        subject_dir = os.path.join(output_dir, subject_id)\n",
    "        os.makedirs(subject_dir, exist_ok=True)\n",
    "\n",
    "        # Save note\n",
    "        file_path = os.path.join(subject_dir, f\"{subject_id}_{note_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(note_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing note {row.get('note_id', 'unknown')}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Change output directories / metadata for discharge notes\n",
    "def write_mimic_notes_parallel_for_bert(df, \n",
    "                                        output_dir=\"BERT/BERT_old/rad_notes\", \n",
    "                                        metadata_csv=\"BERT/BERT_old/metadata_rad_notes_old.csv\", \n",
    "                                        n_jobs=num_cores):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    rows = df.to_dict(\"records\")\n",
    "\n",
    "    # Write notes in parallel\n",
    "    Parallel(n_jobs=n_jobs, prefer=\"threads\")(\n",
    "        delayed(write_single_note_clean)(row, output_dir) for row in rows\n",
    "    )\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_cols = ['subject_id', 'note_id', 'note_type_1', 'charttime']\n",
    "    metadata_df = df.dropna(subset=['subject_id', 'note_id'])\n",
    "    metadata_df = metadata_df[metadata_cols]\n",
    "    metadata_df.to_csv(metadata_csv, index=False)\n",
    "\n",
    "    print(f\"âœ… Notes saved to: {output_dir}\")\n",
    "    print(f\"âœ… Metadata saved to: {metadata_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efd40843-3906-409e-bb50-7cb99ef68e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def create_bert_dataset_from_notes(metadata_csv, notes_root_dir):\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    def load_text(row):\n",
    "        subject_id = str(row['subject_id'])\n",
    "        note_id = str(row['note_id'])\n",
    "        file_path = os.path.join(notes_root_dir, subject_id, f\"{subject_id}_{note_id}.txt\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    texts = Parallel(n_jobs=num_cores)(delayed(load_text)(row) for _, row in df.iterrows())\n",
    "    df['text'] = texts\n",
    "    df = df[df['text'].str.strip() != \"\"]  # Remove blanks\n",
    "\n",
    "    dataset = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "    print(f\"âœ… Dataset created with {len(dataset)} entries\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c4e4fe-cb91-4103-b708-6da9381eceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_bert_dataset(dataset, model_name='emilyalsentzer/Bio_ClinicalBERT', text_column='text'):\n",
    "    \"\"\"\n",
    "    Tokenizes a HuggingFace dataset for a specified BERT model.\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset containing at least `text_column`.\n",
    "        model_name: BERT model name to use for tokenization.\n",
    "        text_column: Column name containing the text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        tokenized: HuggingFace dataset with tokenized fields.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example[text_column],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ”„ Tokenizing dataset for {model_name}...\")\n",
    "    tokenized = dataset.map(tokenize_function, batched=True)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b4352c-4640-4f83-b4fb-1dd66b98f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import hashlib\n",
    "\n",
    "def extract_patient_embeddings(tokenized_dataset,\n",
    "                               model_name='emilyalsentzer/Bio_ClinicalBERT',\n",
    "                               batch_size=16,\n",
    "                               use_pooler=True,\n",
    "                               device=None,\n",
    "                               cache_dir='embedding_cache',\n",
    "                               seed=42,\n",
    "                               merge_note_types=False):\n",
    "    \"\"\"\n",
    "    Extract patient-level embeddings from a tokenized BERT dataset with caching,\n",
    "    optionally merging all note types into a single wide DataFrame ready for ML.\n",
    "\n",
    "    Args:\n",
    "        tokenized_dataset: HuggingFace Dataset with tokenized clinical notes.\n",
    "            Must include 'subject_id' and 'note_type'.\n",
    "        model_name: Pretrained BERT model name.\n",
    "        batch_size: Batch size for inference.\n",
    "        use_pooler: If True, use pooler_output; else use mean of last_hidden_state.\n",
    "        device: 'cuda' or 'cpu'. If None, auto-choose.\n",
    "        cache_dir: Directory to store cached embeddings.\n",
    "        seed: Random seed for reproducibility.\n",
    "        merge_note_types: If True, return a single DataFrame with all note types pivoted\n",
    "                          as separate columns per patient.\n",
    "\n",
    "    Returns:\n",
    "        pivoted_dict OR merged_df:\n",
    "            - If merge_note_types=False: dict of note_type -> patient-level DataFrame\n",
    "            - If merge_note_types=True: single wide DataFrame with all note types as columns\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Generate hash-based cache filename\n",
    "    hash_input = ''\n",
    "    for item in tokenized_dataset:\n",
    "        hash_input += str(item['subject_id'])\n",
    "        hash_input += str(item['note_type'])\n",
    "        hash_input += ''.join(map(str, item['input_ids']))\n",
    "    dataset_hash = hashlib.md5(hash_input.encode('utf-8')).hexdigest()\n",
    "    safe_model_name = model_name.replace('/', '_')\n",
    "    cache_path = os.path.join(cache_dir,\n",
    "                              f\"{safe_model_name}_{dataset_hash}_pooler{use_pooler}.pkl\")\n",
    "\n",
    "    # Load cache if exists\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached embeddings from {cache_path}...\")\n",
    "        patient_embeddings_df = joblib.load(cache_path)\n",
    "    else:\n",
    "        device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load BERT model\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Collate function\n",
    "        def collate_fn(batch):\n",
    "            collated = {\n",
    "                'input_ids': torch.stack([torch.tensor(b['input_ids']) for b in batch]).to(device),\n",
    "                'attention_mask': torch.stack([torch.tensor(b['attention_mask']) for b in batch]).to(device),\n",
    "                'subject_id': [b['subject_id'] for b in batch],\n",
    "                'note_type': [b['note_type'] for b in batch]\n",
    "            }\n",
    "            return collated\n",
    "\n",
    "        dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "        embeddings_list = []\n",
    "        subject_ids = []\n",
    "        note_types = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "                emb = outputs.pooler_output if use_pooler else outputs.last_hidden_state.mean(dim=1)\n",
    "                embeddings_list.append(emb.cpu().numpy())\n",
    "                subject_ids.extend(batch['subject_id'])\n",
    "                note_types.extend(batch['note_type'])\n",
    "\n",
    "        all_embeddings = np.vstack(embeddings_list)\n",
    "        df = pd.DataFrame(all_embeddings)\n",
    "        df['subject_id'] = subject_ids\n",
    "        df['note_type'] = note_types\n",
    "\n",
    "        # Aggregate per patient per note_type\n",
    "        patient_embeddings_df = df.groupby(['subject_id', 'note_type']).mean().reset_index()\n",
    "        joblib.dump(patient_embeddings_df, cache_path)\n",
    "        print(f\"Saved embeddings to cache: {cache_path}\")\n",
    "\n",
    "    # Pivot each note_type into fully ready-to-merge DataFrames\n",
    "    pivoted_dict = {}\n",
    "    for note_type in patient_embeddings_df['note_type'].unique():\n",
    "        temp_df = patient_embeddings_df[patient_embeddings_df['note_type'] == note_type].copy()\n",
    "        temp_df = temp_df.drop(columns=['note_type'])\n",
    "        pivoted_dict[note_type] = temp_df\n",
    "\n",
    "    if merge_note_types:\n",
    "        # Merge all note types into single wide DataFrame\n",
    "        merged_df = pivoted_dict[list(pivoted_dict.keys())[0]].copy()\n",
    "        merged_df = merged_df.rename(columns={col: f\"{col}_{list(pivoted_dict.keys())[0]}\" for col in merged_df.columns if col != 'subject_id'})\n",
    "\n",
    "        for note_type, df_note in list(pivoted_dict.items())[1:]:\n",
    "            df_renamed = df_note.rename(columns={col: f\"{col}_{note_type}\" for col in df_note.columns if col != 'subject_id'})\n",
    "            merged_df = pd.merge(merged_df, df_renamed, on='subject_id', how='left')\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    return pivoted_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46069039-2630-4d49-b1e3-cdb972491be7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_notes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 82\u001b[0m\n\u001b[0;32m     74\u001b[0m bert_models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdmis-lab/biobert-base-cased-v1.2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     78\u001b[0m ]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# raw_notes: can be either a pandas DataFrame or a HuggingFace Dataset\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# data_after_cleaning_df: your clinical/demographic dataset\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m run_full_workflow(raw_notes, data_after_cleaning_df, bert_models)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_notes' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "# Ensure extract_patient_embeddings() is already imported\n",
    "\n",
    "# ------------------------------\n",
    "# Function to save only new embeddings\n",
    "# ------------------------------\n",
    "def save_embeddings_only(merged_df, radiology_df, model_name, base_output_dir=\"BERT/BERT_old\"):\n",
    "    safe_model_name = model_name.replace('/', '_')\n",
    "    model_dir = os.path.join(base_output_dir, safe_model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save merged all-note-types DataFrame\n",
    "    merged_path = os.path.join(model_dir, \"merged_all_note_types.csv\")\n",
    "    merged_df.to_csv(merged_path, index=False)\n",
    "    print(f\"âœ… Merged all-note-types DataFrame saved to: {merged_path}\")\n",
    "\n",
    "    # Save radiology-only DataFrame\n",
    "    radiology_path = os.path.join(model_dir, \"merged_radiology_only.csv\")\n",
    "    radiology_df.to_csv(radiology_path, index=False)\n",
    "    print(f\"âœ… Radiology-only DataFrame saved to: {radiology_path}\")\n",
    "\n",
    "# ------------------------------\n",
    "# End-to-end workflow (updated)\n",
    "# ------------------------------\n",
    "def run_full_workflow(raw_notes, data_after_cleaning_df, bert_models):\n",
    "    \"\"\"\n",
    "    End-to-end workflow: tokenization, embedding extraction, pivoting, radiology-only extraction,\n",
    "    and saving embedding datasets for multiple BERT models.\n",
    "\n",
    "    Args:\n",
    "        raw_notes: pandas DataFrame OR HuggingFace Dataset with columns ['subject_id', 'note_type', 'text']\n",
    "        data_after_cleaning_df: pandas DataFrame with clinical/demographic features\n",
    "        bert_models: list of BERT model names\n",
    "    \"\"\"\n",
    "    # Convert pandas DataFrame to HuggingFace Dataset if needed\n",
    "    if isinstance(raw_notes, pd.DataFrame):\n",
    "        hf_dataset = Dataset.from_pandas(raw_notes.reset_index(drop=True))\n",
    "        print(f\"âœ… Converted raw_notes DataFrame to HuggingFace Dataset with {len(hf_dataset)} entries\")\n",
    "    else:\n",
    "        hf_dataset = raw_notes\n",
    "        print(f\"âœ… Using provided HuggingFace Dataset with {len(hf_dataset)} entries\")\n",
    "\n",
    "    for model_name in bert_models:\n",
    "        print(f\"\\n=== Processing {model_name} ===\")\n",
    "\n",
    "        # 1ï¸âƒ£ Tokenize\n",
    "        tokenized_dataset = tokenize_bert_dataset(hf_dataset, model_name=model_name)\n",
    "\n",
    "        # 2ï¸âƒ£ Extract embeddings, pivot all note types\n",
    "        merged_df = extract_patient_embeddings(tokenized_dataset,\n",
    "                                               model_name=model_name,\n",
    "                                               merge_note_types=True)\n",
    "\n",
    "        # 3ï¸âƒ£ Merge with cleaned clinical data\n",
    "        merged_with_cleaned = pd.merge(data_after_cleaning_df, merged_df,\n",
    "                                       on='subject_id', how='left')\n",
    "\n",
    "        # 4ï¸âƒ£ Radiology-only DataFrame\n",
    "        radiology_cols = [col for col in merged_with_cleaned.columns if col.endswith('_radiology')]\n",
    "        radiology_df = pd.concat([merged_with_cleaned['subject_id'], merged_with_cleaned[radiology_cols]], axis=1)\n",
    "\n",
    "        # 5ï¸âƒ£ Save embedding datasets\n",
    "        save_embeddings_only(merged_with_cleaned, radiology_df, model_name)\n",
    "\n",
    "        print(f\"âœ… Completed workflow for {model_name}\")\n",
    "        print(f\"   - All note types shape: {merged_with_cleaned.shape}\")\n",
    "        print(f\"   - Radiology-only shape: {radiology_df.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Example usage\n",
    "# ------------------------------\n",
    "bert_models = [\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'dmis-lab/biobert-base-cased-v1.2',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "]\n",
    "\n",
    "# raw_notes: can be either a pandas DataFrame or a HuggingFace Dataset\n",
    "# data_after_cleaning_df: your clinical/demographic dataset\n",
    "run_full_workflow(raw_notes, data_after_cleaning_df, bert_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3db2c38c-4ba0-46b2-83e9-3c70477efe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Notes\n",
    "bert_rad = pd.read_csv(\"Data/Old/data_full_notes_old.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87d04438-74a9-44af-866b-c0fa3fd27d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Notes saved to: BERT/BERT_old/rad_notes_test\n",
      "âœ… Metadata saved to: BERT/BERT_old/metadata_rad_notes_old_test.csv\n",
      "Time for 100 notes: 0.11 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test mimic_notes_parallel_for_bert\n",
    "import time\n",
    "\n",
    "sample_df = df.sample(100)\n",
    "start = time.time()\n",
    "write_mimic_notes_parallel_for_bert(df = sample_df, output_dir=\"BERT/BERT_old/rad_notes_test\", metadata_csv=\"BERT/BERT_old/metadata_rad_notes_old_test.csv\", n_jobs=1)\n",
    "print(f\"Time for 100 notes: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b65f05c2-7fc3-4091-9c9b-a22589400596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Notes saved to: BERT/BERT_old/rad_notes\n",
      "âœ… Metadata saved to: BERT/BERT_old/metadata_rad_notes_old.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract and save notes\n",
    "write_mimic_notes_parallel_for_bert(bert_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db34057c-af04-4099-a0f8-5018bccf382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset created with 100 entries\n",
      "Time for 100 notes: 0.10 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test Rebuild dataset\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "create_bert_dataset_from_notes(\"BERT/BERT_old/metadata_rad_notes_old_test.csv\", \"BERT/BERT_old/rad_notes_test\")\n",
    "print(f\"Time for 100 notes: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5df0976f-2051-4677-aa03-9a3a056b1079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset created with 303994 entries\n"
     ]
    }
   ],
   "source": [
    "# Rebuild Dataset\n",
    "rad_bert_dataset_rebuilt = create_bert_dataset_from_notes(\"BERT/BERT_old/metadata_rad_notes_old.csv\", \"BERT/BERT_old/rad_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f4b111b-58a9-419d-81bc-1b82ff2f09ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5770a4c7c3c4ec8a73cc58e110085aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/303994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize with Clinical BERT\n",
    "tokenized_dataset = tokenize_bert_dataset(rad_bert_dataset_rebuilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa815554-61eb-4575-8fd6-8172e3f11844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7a473728ec4fa39dd3f358c9448362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/303994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenized dataset saved to: BERT/BERT_old/tokenized_dataset\n"
     ]
    }
   ],
   "source": [
    "# Save for later\n",
    "save_tokenized_dataset(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00290804-dac3-4ad3-948e-a02401f26f5a",
   "metadata": {},
   "source": [
    "## Part 7 Clinical BERT ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225490e6-1f7c-4b65-a6fb-8284532bd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for further clinical bert preprocessing if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c4fb5-7b57-4b56-afbe-8049fc436500",
   "metadata": {},
   "source": [
    "Once the preprocessing for clinical BERT is complete, proceed to `sepsis_model_training.ipynb` for model training, testing, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
