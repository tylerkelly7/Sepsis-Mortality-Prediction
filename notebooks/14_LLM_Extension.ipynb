{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a272cfa0",
   "metadata": {},
   "source": [
    "# 14 LLM Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1920df-702a-4d43-b889-441a8c3b254a",
   "metadata": {},
   "source": [
    "## 0 Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49db163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import resolve_path\n",
    "\n",
    "from src.data_prep import split_data\n",
    "\n",
    "from src.features import (\n",
    "    scale_features,\n",
    "    scale_llm_features,\n",
    "    merge_embeddings_with_features,\n",
    "    save_feature_dataset\n",
    ")\n",
    "\n",
    "from src.resampling import resample_training_data, print_class_balance, save_resampled_dataset\n",
    "\n",
    "from src.models import (\n",
    "    get_classifiers,\n",
    "    get_param_distributions,\n",
    "    get_n_iter_random_per_clf,\n",
    "    repeated_cv_with_mixed_search,\n",
    "    auc_scorer\n",
    ")\n",
    "\n",
    "from src.evaluation import export_summary\n",
    "\n",
    "from src.llm_long_context_summary import LongContextSummaryConfig\n",
    "\n",
    "from src.llm_embeddings import (\n",
    "    EmbeddingConfig,\n",
    "    _call_embedding_provider,\n",
    "    prepare_embedding_inputs,\n",
    "    run_llm_embeddings_pipeline,\n",
    "    load_embeddings_artifact,\n",
    "    load_embedding_cache,\n",
    "    validate_cache_schema,\n",
    "    run_long_context_embeddings_pipeline\n",
    ")\n",
    "\n",
    "from src.llm_structured_features import (\n",
    "    smoke_test_structured_extraction,\n",
    "    ExtractionConfig,\n",
    "    extract_features_with_retries,\n",
    "    SCHEMA_VERSION,\n",
    "    SEPSIS_MORTALITY_MODEL_SCHEMA,\n",
    "    DEFAULT_FEATURES_ARTIFACT_PATH,\n",
    "    TruncationPolicy,\n",
    "    prepare_feature_inputs,\n",
    "    run_llm_structured_features_pipeline\n",
    ")\n",
    "\n",
    "import src.llm_embeddings as llmA\n",
    "\n",
    "import src.llm_structured_features as llmB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c738310-baf6-450e-8668-d9ed5b545dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL (cost tracking hygiene):\n",
    "# 1) Create a dedicated OpenAI API key for this embedding run (Dashboard ‚Üí API keys).\n",
    "# 2) Set it as OPENAI_API_KEY (env var) before launching Jupyter.\n",
    "# 3) Run the embedding build once.\n",
    "# 4) Revoke/delete the key after the run to prevent accidental future spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650958bc-182e-4e2b-a291-16002d406564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found in environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e06b29-7792-44d5-b9ac-f8e2272e7d20",
   "metadata": {},
   "source": [
    "### Smoke Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96c192-787e-4d21-a053-1671d2628a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "resp = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"test sepsis note\"],\n",
    "    dimensions=256\n",
    ")\n",
    "\n",
    "len(resp.data[0].embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c1446-8aa2-4dc5-a5ae-c1d06ff44b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_smoke = EmbeddingConfig(model=\"text-embedding-3-small\", dimensions=256, batch_size=8)\n",
    "arr = _call_embedding_provider([\"hello world\", \"sepsis mortality risk\"], cfg_smoke)\n",
    "\n",
    "print(arr.shape)          # (2, 256)\n",
    "print(np.isfinite(arr).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93657a-d785-4517-ab2a-07256a01b6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Structured feature extraction\n",
    "\n",
    "smoke_test_structured_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab13808-6792-4345-98e0-15862499bda3",
   "metadata": {},
   "source": [
    "## 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca4876a-b017-4f31-99af-1d94e9eebaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded NLP-ready dataset: (5208, 51)\n",
      "Columns: ['subject_id', 'hospital_expire_flag', 'max_age', 'los_icu', 'first_hosp_stay', 'suspected_infection', 'sofa_score', 'sepsis3', 'avg_urineoutput', 'glucose_min'] ...\n"
     ]
    }
   ],
   "source": [
    "nlp_ready_path = resolve_path(\"data/interim/data_nlp_ready.csv\")\n",
    "nlp_ready_df = pd.read_csv(nlp_ready_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded NLP-ready dataset: {nlp_ready_df.shape}\")\n",
    "print(f\"Columns: {nlp_ready_df.columns.tolist()[:10]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e2cf5e-b48b-4777-9762-e06998d2eefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Radiology_notes missing for 2 patients (kept intentionally)\n"
     ]
    }
   ],
   "source": [
    "n_missing = nlp_ready_df[\"Radiology_notes\"].isna().sum()\n",
    "print(f\"‚ÑπÔ∏è Radiology_notes missing for {n_missing} patients (kept intentionally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4493fc06-93e1-47ab-845d-1b270ef98124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aggregated Radiology notes (could replace this with discharge later)\n",
    "TEXT_COL = \"Radiology_notes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4f5ce0-3bd2-416e-9123-61413dc1b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Structured features: (5208, 44), Target: (5208,)\n",
      "Train: (4166, 44), Test: (1042, 44)\n",
      "‚úÖ Scaled original features prepared (not saved ‚Äî handled downstream)\n"
     ]
    }
   ],
   "source": [
    "# Drop note text columns\n",
    "original_df = nlp_ready_df.drop(\n",
    "    columns=[\"Radiology_notes\", \"Discharge_summary_notes\", \"combined_notes\"]\n",
    ")\n",
    "\n",
    "X_original = original_df.drop(columns=[\n",
    "    'hospital_expire_flag',\n",
    "    'first_hosp_stay',\n",
    "    'suspected_infection',\n",
    "    'sepsis3'])\n",
    "y_original = original_df[\"hospital_expire_flag\"]\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Structured features: {X_original.shape}, Target: {y_original.shape}\")\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = split_data(X_original, y_original, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {X_train_orig.shape}, Test: {X_test_orig.shape}\")\n",
    "\n",
    "X_train_orig_scaled, X_test_orig_scaled, y_train_orig, y_test_orig = scale_features(\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig, prefix=\"original\"\n",
    ")\n",
    "\n",
    "train_ids = set(X_train_orig[\"subject_id\"])\n",
    "test_ids  = set(X_test_orig[\"subject_id\"])\n",
    "\n",
    "# Align note text subsets to train/test subjects\n",
    "train_notes = nlp_ready_df.loc[nlp_ready_df[\"subject_id\"].isin(train_ids)].copy()\n",
    "test_notes  = nlp_ready_df.loc[nlp_ready_df[\"subject_id\"].isin(test_ids)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d340b28-019c-42e6-855c-762d1b5604ef",
   "metadata": {},
   "source": [
    "## 2 Generate and Cache Dense Embeddings for All Patients - LLM Extension A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b22272-4dd0-4d72-930b-ff79c352ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è long-context summary artifact missing/invalid ‚Äî generating summaries.\n",
      "üîé Long-context summary cache loaded: 0 rows (model=gpt-4.1-mini)\n",
      "üß© Summaries needed: 5208 / 5208\n",
      "‚û°Ô∏è  Summarizing: 0 / 5208\n"
     ]
    }
   ],
   "source": [
    "summary_cfg = LongContextSummaryConfig(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    max_output_tokens=3500,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "emb_cfg = EmbeddingConfig(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=256,\n",
    "    batch_size=64,\n",
    "    use_batch_api=True,\n",
    ")\n",
    "\n",
    "df_emb = run_long_context_embeddings_pipeline(\n",
    "    nlp_ready_df=nlp_ready_df,\n",
    "    raw_text_col=TEXT_COL,  # Radiology_notes aggregated text column\n",
    "    id_col=\"subject_id\",\n",
    "    summary_cfg=summary_cfg,\n",
    "    embedding_cfg=emb_cfg,\n",
    "    summary_artifact_path=\"data/processed/llm_A/long_context_summaries.parquet\",\n",
    "    embeddings_artifact_path=\"data/processed/llm_A/embeddings_all.parquet\",\n",
    "    source_path=\"data/interim/data_nlp_ready.csv\",\n",
    "    verbose=True,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7b819-3d03-437b-9946-1151a665477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "batch_id = \"batch_69580b0645bc8190a96da9ec1428fde1\"\n",
    "b = client.batches.retrieve(batch_id)\n",
    "\n",
    "print(\"status:\", b.status)\n",
    "print(\"errors:\", b.errors)\n",
    "print(\"error_file_id:\", b.error_file_id)\n",
    "print(\"input_file_id:\", b.input_file_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c79e1-dd8c-43e1-a4d9-145ef75eaa84",
   "metadata": {},
   "source": [
    "## 3 Validate Dense Embeddings and Caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23e791-0dae-4f14-9b91-dcf35c082334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cache = load_embedding_cache(cfg)\n",
    "validate_cache_schema(df_cache, cfg)\n",
    "\n",
    "df_art = load_embeddings_artifact(\"data/processed/llm_A/embeddings_all.parquet\")\n",
    "\n",
    "print(\"cache rows:\", df_cache.shape[0])\n",
    "print(\"artifact rows:\", df_art.shape[0])\n",
    "\n",
    "assert df_art.shape[0] == df_inputs.shape[0]\n",
    "assert df_art.shape[0] <= df_cache.shape[0]  # cache may contain more if you reran with different configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428e5fc-cffc-4d61-81eb-414b3f17e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm no missing Embeddings\n",
    "emb_cols = [c for c in df_art.columns if c.startswith(\"emb_\")]\n",
    "assert not df_art[emb_cols].isna().any().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a36a2d-9b17-4c36-a345-a1f64666a1c9",
   "metadata": {},
   "source": [
    "## 4 LLM Structured Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efa6fb-f0f1-414c-a30a-f654ee9616de",
   "metadata": {},
   "source": [
    "### 4.1 Mortality-Focused Schema\n",
    "\n",
    "**Schema version (model-scoped):**  \n",
    "`sepsis_mortality_gpt_4o_mini_v1`\n",
    "\n",
    "This schema is designed to extract mortality-relevant clinical signals from radiology notes for patients with sepsis.  \n",
    "\n",
    "---\n",
    "\n",
    "### Infection Context\n",
    "\n",
    "**suspected_infection_source** (categorical)  \n",
    "Allowed values:\n",
    "- respiratory  \n",
    "- urinary  \n",
    "- intra_abdominal  \n",
    "- skin_soft_tissue  \n",
    "- line_catheter  \n",
    "- cns  \n",
    "- other  \n",
    "- unknown  \n",
    "\n",
    "**suspected_pathogen_type** (categorical)  \n",
    "Allowed values:\n",
    "- bacterial  \n",
    "- viral  \n",
    "- fungal  \n",
    "- mixed  \n",
    "- unknown  \n",
    "\n",
    "---\n",
    "\n",
    "### SOFA-Aligned Organ Dysfunction Signals  \n",
    "(Indicators associated with severity and mortality)\n",
    "\n",
    "#### Respiratory (SOFA respiratory)\n",
    "- resp_failure_present (0 / 1)  \n",
    "- mechanical_ventilation_present (0 / 1)  \n",
    "- hypoxemia_present (0 / 1)  \n",
    "\n",
    "#### Cardiovascular (SOFA CV)\n",
    "- hypotension_present (0 / 1)  \n",
    "- vasopressor_use_present (0 / 1)  \n",
    "\n",
    "#### Renal (SOFA renal)\n",
    "- aki_present (0 / 1)  \n",
    "- oliguria_anuria_present (0 / 1)  \n",
    "- dialysis_present (0 / 1)  \n",
    "\n",
    "#### Coagulation (SOFA coag)\n",
    "- thrombocytopenia_present (0 / 1)  \n",
    "\n",
    "#### Liver (SOFA liver)\n",
    "- hyperbilirubinemia_present (0 / 1)  \n",
    "\n",
    "#### Central Nervous System (SOFA CNS)\n",
    "- altered_mentation_present (0 / 1)  \n",
    "\n",
    "---\n",
    "\n",
    "### Mortality Modifiers\n",
    "\n",
    "**high_risk_course_language_present** (0 / 1)  \n",
    "Language suggesting clinical deterioration or poor prognosis, e.g.:\n",
    "- ‚Äúworsening‚Äù\n",
    "- ‚Äúcritical‚Äù\n",
    "- ‚Äúguarded‚Äù\n",
    "- ‚Äúpoor prognosis‚Äù\n",
    "- ‚Äúmultiorgan failure‚Äù\n",
    "- ‚Äúrapid decline‚Äù\n",
    "\n",
    "**limitation_of_care_present** (0 / 1)  \n",
    "Language indicating limitations or withdrawal of care, e.g.:\n",
    "- DNR / DNI\n",
    "- comfort measures only\n",
    "- hospice\n",
    "- withdrawal of life-sustaining treatment\n",
    "\n",
    "---\n",
    "\n",
    "### Imaging-Level Severity Impression\n",
    "\n",
    "**imaging_severity_impression** (categorical)  \n",
    "Overall severity language in the radiology impression:\n",
    "- mild  \n",
    "- moderate  \n",
    "- severe  \n",
    "- unknown \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcefcb9-a25d-4603-bda7-2e6e8a41a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Feature Definitions\n",
    "# ==================================================\n",
    "\n",
    "# ------------------------------\n",
    "# Infection context\n",
    "# ------------------------------\n",
    "\n",
    "# suspected_infection_source: categorical\n",
    "# Allowed values:\n",
    "#   - respiratory\n",
    "#   - urinary\n",
    "#   - intra_abdominal\n",
    "#   - skin_soft_tissue\n",
    "#   - line_catheter\n",
    "#   - cns\n",
    "#   - other\n",
    "#   - unknown\n",
    "\n",
    "# suspected_pathogen_type: categorical\n",
    "# Allowed values:\n",
    "#   - bacterial\n",
    "#   - viral\n",
    "#   - fungal\n",
    "#   - mixed\n",
    "#   - unknown\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# SOFA-aligned organ dysfunction\n",
    "# (mortality-relevant signals)\n",
    "# ------------------------------\n",
    "\n",
    "# Respiratory (SOFA respiratory)\n",
    "# resp_failure_present: 0 / 1\n",
    "# mechanical_ventilation_present: 0 / 1\n",
    "# hypoxemia_present: 0 / 1\n",
    "\n",
    "# Cardiovascular (SOFA CV)\n",
    "# hypotension_present: 0 / 1\n",
    "# vasopressor_use_present: 0 / 1\n",
    "\n",
    "# Renal (SOFA renal)\n",
    "# aki_present: 0 / 1\n",
    "# oliguria_anuria_present: 0 / 1\n",
    "# dialysis_present: 0 / 1\n",
    "\n",
    "# Coagulation (SOFA coag)\n",
    "# thrombocytopenia_present: 0 / 1\n",
    "\n",
    "# Liver (SOFA liver)\n",
    "# hyperbilirubinemia_present: 0 / 1\n",
    "\n",
    "# CNS (SOFA CNS)\n",
    "# altered_mentation_present: 0 / 1\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Mortality modifiers\n",
    "# ------------------------------\n",
    "\n",
    "# high_risk_course_language_present: 0 / 1\n",
    "# Indicates language suggesting deterioration or poor prognosis, e.g.:\n",
    "#   \"worsening\", \"critical\", \"guarded\", \"poor prognosis\",\n",
    "#   \"multiorgan failure\", \"rapid decline\", etc.\n",
    "\n",
    "# limitation_of_care_present: 0 / 1\n",
    "# Indicates limitations or withdrawal of care, e.g.:\n",
    "#   DNR, DNI, comfort measures only, hospice, withdrawal of life support\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Imaging-level severity\n",
    "# ------------------------------\n",
    "\n",
    "# imaging_severity_impression: categorical\n",
    "# Allowed values:\n",
    "#   - mild\n",
    "#   - moderate\n",
    "#   - severe\n",
    "#   - unknown\n",
    "# Based on overall severity phrasing in the radiology impression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd686a-abe7-4a93-9241-1d83a8170ff6",
   "metadata": {},
   "source": [
    "### 4.2 Truncation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5361f-8dc8-43e6-93b4-c9ceccf6ca5a",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "Patient with subject id 15114531 had a token length that exceeded the max token length and had to be handled separately with token truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2e353-2f5d-4cd1-9112-1e5915481f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_text = nlp_ready_df.loc[nlp_ready_df[\"subject_id\"] == 15114531, TEXT_COL].iloc[0]\n",
    "mid = len(note_text) // 2\n",
    "q1 = mid - (mid // 2)\n",
    "q3 = mid + (mid // 2)\n",
    "note_part_1 = note_text[:q1]\n",
    "note_part_2 = note_text[q1:mid]\n",
    "note_part_3 = note_text[mid:q3]\n",
    "note_part_4 = note_text[q3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99421782-99bb-4f7f-97f5-0494d4880e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TruncationPolicy(\n",
    "    max_tokens=125_000,\n",
    "    apply_to_subject_ids={15114531},  # only this patient can be truncated\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a13638-4a3a-48c9-91cf-13101ad72447",
   "metadata": {},
   "source": [
    "### 4.3 Run Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99562f4-7d2b-4442-bcb6-0466430d661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_extract = ExtractionConfig(model=\"gpt-4o-mini\")\n",
    "\n",
    "df_inputs_B = prepare_feature_inputs(\n",
    "    nlp_ready_df,\n",
    "    text_col=TEXT_COL,\n",
    "    id_col=\"subject_id\",\n",
    ")\n",
    "\n",
    "df_llmB = run_llm_structured_features_pipeline(\n",
    "    df_inputs=df_inputs_B,\n",
    "    cfg=cfg_extract,\n",
    "    model_schema=SEPSIS_MORTALITY_MODEL_SCHEMA,\n",
    "    artifact_path=DEFAULT_FEATURES_ARTIFACT_PATH,\n",
    "    prefix=\"llmB_\",\n",
    "    source_path=\"data/interim/data_nlp_ready.csv\",\n",
    "    verbose=True,\n",
    "    batch_size=100,\n",
    "    checkpoint_every=100,\n",
    "    resume=True,\n",
    "    retry_errors=False, # <- set to True if failure occurs\n",
    "    #truncation_policy=policy # <- set := policy if failure occurs\n",
    ")\n",
    "print((resolve_path(DEFAULT_FEATURES_ARTIFACT_PATH).parent / \"features_manifest.json\").exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39006de9-a027-4c2c-b570-e182f0eb31e3",
   "metadata": {},
   "source": [
    "### 4.4 Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acbfd1-fad1-4ed4-8dbd-3d373f6d1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.llm_structured_features import ExtractionConfig, load_feature_cache\n",
    "\n",
    "cfg = ExtractionConfig(model=\"gpt-4o-mini\")\n",
    "\n",
    "df_cache = load_feature_cache(cfg)\n",
    "\n",
    "df_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63476afe-941c-4e43-a11d-553bfce1c4b4",
   "metadata": {},
   "source": [
    "## 5 Split, Standard Scale, Merge, and Save Splits for LLM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda7c77-1b2a-4f91-80c2-977f443161d5",
   "metadata": {},
   "source": [
    "### LLM A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae42f4-fa4c-41e2-8371-f9e4697b82e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dbd4d1b-212b-442d-b8eb-78334e57048f",
   "metadata": {},
   "source": [
    "### LLM B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f43f94-7c76-420a-8e93-e8945fbd5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Train/Test partitions\n",
    "\n",
    "llmB_train = df_llmB[df_llmB[\"subject_id\"].isin(train_ids)].copy()\n",
    "llmB_test  = df_llmB[df_llmB[\"subject_id\"].isin(test_ids)].copy()\n",
    "\n",
    "assert set(llmB_train[\"subject_id\"]) == train_ids\n",
    "assert set(llmB_test[\"subject_id\"])  == test_ids\n",
    "assert llmB_train.shape[0] == X_train_orig.shape[0]\n",
    "assert llmB_test.shape[0]  == X_test_orig.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f304efa-6ecf-4429-8114-704a8879a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standard Scaling on all LLM Structured Features\n",
    "\n",
    "llmB_train_scaled, llmB_test_scaled = scale_llm_features(\n",
    "    X_train=llmB_train,\n",
    "    X_test=llmB_test,\n",
    "    model=cfg_extract.model,   # e.g. \"gpt-4o-mini\"\n",
    "    prefix=\"llmB_\",\n",
    "    id_col=\"subject_id\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba051f38-dbc1-44ef-9739-85a1d69596c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_llmB_scaled, X_test_llmB_scaled = merge_embeddings_with_features(\n",
    "    X_train_features=X_train_orig_scaled,\n",
    "    X_test_features=X_test_orig_scaled,\n",
    "    X_train_embed=llmB_train_scaled,\n",
    "    X_test_embed=llmB_test_scaled,\n",
    "    id_col=\"subject_id\",\n",
    "    prefix=\"llm_B\",   # choose the variant folder/name you want\n",
    "    save_dir=\"data/processed\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad62fe-e9d2-421d-a31c-36431ff682d9",
   "metadata": {},
   "source": [
    "## 6 Apply and Save SMOTE Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56204b0f-4881-4be3-8536-06b353c6d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM variants dataset container (keep subject_id through SMOTE)\n",
    "# ============================================================\n",
    "\n",
    "datasets = {\n",
    "    \"llm_B\": {\n",
    "        \"X_train\": X_train_llmB_scaled.copy(),\n",
    "        \"X_test\":  X_test_llmB_scaled.copy(),\n",
    "        \"y_train\": y_train_orig.copy(),\n",
    "        \"y_test\":  y_test_orig.copy(),\n",
    "    },\n",
    "\n",
    "    # ----------------------------\n",
    "    # LLM A (comment out for now)\n",
    "    # ----------------------------\n",
    "    # \"llm_A\": {\n",
    "    #     \"X_train\": X_train_llmA_scaled.copy(),\n",
    "    #     \"X_test\":  X_test_llmA_scaled.copy(),\n",
    "    #     \"y_train\": y_train_orig.copy(),\n",
    "    #     \"y_test\":  y_test_orig.copy(),\n",
    "    # },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14ccaa-fad5-4918-9899-59fb7e25195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Apply SMOTE per LLM variant (train only)\n",
    "# ============================================================\n",
    "\n",
    "for variant, data in datasets.items():\n",
    "    X_train_res, y_train_res = resample_training_data(\n",
    "        data[\"X_train\"],\n",
    "        data[\"y_train\"],\n",
    "        method=\"smote\"\n",
    "    )\n",
    "\n",
    "    datasets[variant][\"X_train_res\"] = X_train_res\n",
    "    datasets[variant][\"y_train_res\"] = y_train_res\n",
    "\n",
    "    print_class_balance(y_train_res, f\"{variant} training set (after SMOTE)\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Save SMOTE-balanced training sets\n",
    "# ============================================================\n",
    "\n",
    "for variant, data in datasets.items():\n",
    "    out_dir = resolve_path(f\"data/processed/{variant}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    X_train_res = pd.DataFrame(data[\"X_train_res\"])\n",
    "    y_train_res = pd.Series(data[\"y_train_res\"], name=\"target\")\n",
    "\n",
    "    X_train_res.to_csv(\n",
    "        os.path.join(out_dir, f\"data_{variant}_xtrain_res.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "    y_train_res.to_csv(\n",
    "        os.path.join(out_dir, f\"data_{variant}_ytrain_res.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Saved SMOTE-balanced training sets for {variant} under {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34ea84-70b8-4ab9-8a52-ac5df0fdd556",
   "metadata": {},
   "source": [
    "### Remove Subject ID from All Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151926e-0981-4b6e-be08-6ae57e8df6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Remove subject_id and non-feature cols from all X_* datasets (IN PLACE)\n",
    "# ============================================================\n",
    "\n",
    "drop_cols = [\"subject_id\", \"first_hosp_stay\", \"suspected_infection\", \"sepsis3\"]\n",
    "\n",
    "for variant, data in datasets.items():\n",
    "    for key in [\"X_train\", \"X_test\", \"X_train_res\"]:\n",
    "        df = data.get(key)\n",
    "        if df is None:\n",
    "            continue\n",
    "\n",
    "        cols_to_drop = [c for c in drop_cols if c in df.columns]\n",
    "        if cols_to_drop:\n",
    "            df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Dropped id/meta cols for {variant} | \"\n",
    "        f\"X_train: {data['X_train'].shape}, \"\n",
    "        f\"X_train_res: {data['X_train_res'].shape}, \"\n",
    "        f\"X_test: {data['X_test'].shape}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535151e1-32c8-4f79-8d3f-014601440fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = [\n",
    "    \"llm_B\",\n",
    "    # \"llm_A\",\n",
    "]\n",
    "\n",
    "for variant in variants:\n",
    "    base_dir = Path(resolve_path(f\"data/processed/{variant}\"))\n",
    "\n",
    "    files_to_fix = [\n",
    "        base_dir / f\"data_{variant}_xtrain.csv\",\n",
    "        base_dir / f\"data_{variant}_xtest.csv\",\n",
    "        base_dir / f\"data_{variant}_xtrain_res.csv\",\n",
    "    ]\n",
    "\n",
    "    for fp in files_to_fix:\n",
    "        if not fp.exists():\n",
    "            print(f\"‚ö†Ô∏è Missing file (skipped): {variant}/{fp.name}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(fp)\n",
    "        cols_present = [c for c in drop_cols if c in df.columns]\n",
    "        if cols_present:\n",
    "            df.drop(columns=cols_present, inplace=True)\n",
    "            df.to_csv(fp, index=False)\n",
    "            print(f\"‚úÖ Rewrote {variant}/{fp.name} (dropped: {cols_present})\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è {variant}/{fp.name} already clean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf6f5b-2f96-4144-a2e6-712417187a34",
   "metadata": {},
   "source": [
    "## 7 Define Define Classifiers & Hyperparameter Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f01e7-e56a-4550-9845-ef83fdd2e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = get_classifiers()\n",
    "param_spaces = get_param_distributions()\n",
    "n_iter_random_per_clf = get_n_iter_random_per_clf()\n",
    "\n",
    "n_iter_random_subset = {k: n_iter_random_per_clf.get(k, 50) for k in classifiers.keys()}\n",
    "print(\"n_iter_random_per_clf:\", n_iter_random_subset)\n",
    "\n",
    "print(\"‚úÖ Classifiers and hyperparameter grids initialized.\")\n",
    "print(\"Available classifiers:\", list(classifiers.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de166fcb-adb0-496b-9062-8920cca52be2",
   "metadata": {},
   "source": [
    "## 8 Classifier Re-Training with LLM A Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bb6c7-54a5-46fc-8bdc-f7f7155032ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Run repeated CV with mixed search strategy ‚Äî LLM A (COMMENTED)\n",
    "# ============================================================\n",
    "\n",
    "# mode = \"llm_A\"\n",
    "# save_prefix = f\"results/models/{mode}/\"\n",
    "#\n",
    "# X_train = X_train_llmA_merged\n",
    "# X_test  = X_test_llmA_merged\n",
    "# y_train = y_train_orig\n",
    "# y_test  = y_test_orig\n",
    "#\n",
    "# X_train_res = X_train_llmA_res\n",
    "# y_train_res = y_train_llmA_res\n",
    "#\n",
    "# results_llmA, summary_llmA = repeated_cv_with_mixed_search(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     classifiers=classifiers,\n",
    "#     param_spaces=param_spaces,\n",
    "#     X_train_smote=X_train_res,\n",
    "#     y_train_smote=y_train_res,\n",
    "#     n_splits=5,\n",
    "#     n_repeats=10,\n",
    "#     scoring=auc_scorer,\n",
    "#     n_iter_random=None,\n",
    "#     n_iter_random_per_clf=n_iter_random_per_clf,\n",
    "#     save_prefix=save_prefix,\n",
    "#     mode=mode,\n",
    "#     log_mlflow=True,\n",
    "# )\n",
    "#\n",
    "# export_summary(summary_llmA, save_prefix=\"reports/\", mode=mode)\n",
    "# print(f\"‚úÖ Finished model training for {mode} dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5cb9f-12c0-40ea-88ff-ed16efbf89b6",
   "metadata": {},
   "source": [
    "## 9 Classifier Re-Training with LLM B Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f318bb-1d73-40e2-9bfa-502a3836efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Run repeated CV with mixed search strategy ‚Äî LLM B\n",
    "# ============================================================\n",
    "\n",
    "mode = \"llm_B\"\n",
    "save_prefix = f\"results/models/{mode}/\"\n",
    "\n",
    "# ============================================================\n",
    "# Unpack LLM-B datasets for training\n",
    "# ============================================================\n",
    "\n",
    "X_train      = datasets[\"llm_B\"][\"X_train\"]\n",
    "X_test       = datasets[\"llm_B\"][\"X_test\"]\n",
    "y_train      = datasets[\"llm_B\"][\"y_train\"]\n",
    "y_test       = datasets[\"llm_B\"][\"y_test\"]\n",
    "X_train_res  = datasets[\"llm_B\"][\"X_train_res\"]\n",
    "y_train_res  = datasets[\"llm_B\"][\"y_train_res\"]\n",
    "\n",
    "results_llmB, summary_llmB = repeated_cv_with_mixed_search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    classifiers=classifiers,\n",
    "    param_spaces=param_spaces,\n",
    "    X_train_smote=X_train_res,\n",
    "    y_train_smote=y_train_res,\n",
    "    n_splits=5,\n",
    "    n_repeats=10,\n",
    "    scoring=auc_scorer,            # same as Task 4 / 08\n",
    "    n_iter_random=None,            # keep None if using per-clf dict\n",
    "    n_iter_random_per_clf=n_iter_random_per_clf,\n",
    "    save_prefix=save_prefix,\n",
    "    mode=mode,\n",
    "    log_mlflow=True,\n",
    ")\n",
    "\n",
    "# Export summary to reports/\n",
    "export_summary(summary_llmB, save_prefix=\"reports/\", mode=mode)\n",
    "print(f\"‚úÖ Finished model training for {mode} dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c10fc3-cc1e-4314-a6e0-9e1f624e7900",
   "metadata": {},
   "source": [
    "## 10 Check Cosine Similarity Between two LLM A Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1e2d4-9fed-4269-9409-27ac0ca61ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def shuffle_lines(text: str, seed: int = 0) -> str:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lines = [ln for ln in str(text).splitlines() if ln.strip() != \"\"]\n",
    "    rng.shuffle(lines)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# pick 50 subjects\n",
    "df50 = nlp_ready_df[[\"subject_id\", TEXT_COL]].sample(50, random_state=7).copy()\n",
    "df50_shuf = df50.copy()\n",
    "df50_shuf[TEXT_COL] = df50_shuf[TEXT_COL].apply(lambda t: shuffle_lines(t, seed=7))\n",
    "\n",
    "# run summaries twice (different artifact paths so caches stay separate)\n",
    "dfA = run_long_context_embeddings_pipeline(\n",
    "    nlp_ready_df=df50,\n",
    "    raw_text_col=TEXT_COL,\n",
    "    summary_artifact_path=\"data/processed/llm_A/long_context_summaries_50.parquet\",\n",
    "    embeddings_artifact_path=\"data/processed/llm_A/embeddings_50.parquet\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "dfB = run_long_context_embeddings_pipeline(\n",
    "    nlp_ready_df=df50_shuf,\n",
    "    raw_text_col=TEXT_COL,\n",
    "    summary_artifact_path=\"data/processed/llm_A/long_context_summaries_50_shuf.parquet\",\n",
    "    embeddings_artifact_path=\"data/processed/llm_A/embeddings_50_shuf.parquet\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "emb_cols = [c for c in dfA.columns if c.startswith(\"emb_\")]\n",
    "A = dfA.sort_values(\"subject_id\")[emb_cols].to_numpy()\n",
    "B = dfB.sort_values(\"subject_id\")[emb_cols].to_numpy()\n",
    "\n",
    "sims = np.diag(cosine_similarity(A, B))\n",
    "print(\"mean cosine:\", sims.mean())\n",
    "print(\"min cosine:\", sims.min())\n",
    "print(\"p10 cosine:\", np.quantile(sims, 0.10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
