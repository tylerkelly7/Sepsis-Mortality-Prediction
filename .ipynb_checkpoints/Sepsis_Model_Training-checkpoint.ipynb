{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yuyinglu2000/Sepsis-Mortality/blob/main/Copy_of_Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Improving Prediction Accuracy of Sepsis Mortality using Natural Language Processing\n",
    "## Tyler Kelly\n",
    "# Sepsis Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this file is to perform a machine learning pipeline to train and test preprocessed sepsis data. \n",
    "\n",
    "First, I determine the best model using the 'data_after_cleaning' dataset (featuring 48 features and a sample size of 5209) and test the same models used in the author's paper:\n",
    "\n",
    "    SVC,\n",
    "    DecisionTree,\n",
    "    RandomForest,\n",
    "    GradientBoosting,\n",
    "    MLP,\n",
    "    XGB,\n",
    "    LGBM\n",
    "    \n",
    "and compare model accuracy, AUC/ROC, F1, F2, precision, precision and recall, and provide 95% CI as in the original paper.\n",
    "\n",
    "Second, I then perform side-by-side comparison of these metrics against the same dataset including NLP word embeddings. I provide side-by-side figures to highlight main differences in the models using the different feature spaces, and showcase important differences in accuracy and AUROC.\n",
    "\n",
    "Third, I display feature importances and further incorporate SHAP values into the pipeline similar to how the author's did. The SHAP values are incorporated because ____\n",
    "\n",
    "Fourth, I perform chi-square statistical tests to compare ___.\n",
    "\n",
    "Finally, I perform statistical tests to compare if there is a statistically significant difference in model performance between the best performing model for each feature space. By doing so, and determining Cohen's d for effect size, I can roughly approximate how much more one model improves. Then, I attempt to address through discussion possible entry points for noise, which could falsely inflate model accuracy and AUROC on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "This file is a continuation of 'pymain.ipynb'. In pymain step 4 (done in R), word2vec embeddings are developed and appended to the nlp_ready_df dataframe. They are saved as \n",
    "\n",
    "'data_w2v_radiology_old.csv' and \n",
    "'data_w2v_combined_old.csv' \n",
    "\n",
    "and are imported below.\n",
    "\n",
    "Currently, these dataframes represent the old dataset. After the sql is fixed, remove _old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is adapted from the github repository 'https://github.com/yuyinglu2000/Sepsis-Mortality'\n",
    "\n",
    "It is marked with #AC when primarily using the author's code, and unmarked when it is my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Original and W2V Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I begin Model Training by first ensuring that my embeddings are standardized as it was recommended by chat gpt\n",
    "\n",
    "**NOTE** I may need to scale other numeric variables too, but **AM NOT DOING THAT** right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatgpt prompt: Show how to Scale embeddings before using in ML: from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Output:\n",
    "\n",
    "\"Sure! In Python, if you're using Word2Vec embeddings (like w2v_1, w2v_2, ..., w2v_100) as features for machine learning, it's important to scale them, especially if you're using models like logistic regression, SVM, or neural networks.\n",
    "\n",
    "You can do this using StandardScaler from sklearn.preprocessing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Further, only the w2v patient vectors are scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in w2v modified dataset\n",
    "df = pd.read_csv('Data/Old/data_after_cleaning.csv')\n",
    "\n",
    "df_w2v = pd.read_csv('Data/Old/W2V_old/data_w2v_radiology_old.csv')\n",
    "\n",
    "#df = pd.read_csv('data_w2v_radiology.csv') remove _old when sql is fixed and rerun entire document, changing save file names along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>max_age</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>first_hosp_stay</th>\n",
       "      <th>suspected_infection</th>\n",
       "      <th>sofa_score</th>\n",
       "      <th>sepsis3</th>\n",
       "      <th>avg_urineoutput</th>\n",
       "      <th>glucose_min</th>\n",
       "      <th>...</th>\n",
       "      <th>race_Hispanic or Latin</th>\n",
       "      <th>race_Others race</th>\n",
       "      <th>race_White</th>\n",
       "      <th>antibiotic_Vancomycin</th>\n",
       "      <th>antibiotic_Vancomycin Antibiotic Lock</th>\n",
       "      <th>antibiotic_Vancomycin Enema</th>\n",
       "      <th>antibiotic_Vancomycin Intrathecal</th>\n",
       "      <th>antibiotic_Vancomycin Oral Liquid</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19986715</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>10.58</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>136.657143</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19973083</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2.33</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>34.263158</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19907774</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>1.83</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>105.476191</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19894745</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1.08</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19884808</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20.46</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>107.229508</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hospital_expire_flag  max_age  los_icu  first_hosp_stay  \\\n",
       "0    19986715                     0       24    10.58             True   \n",
       "1    19973083                     0       58     2.33             True   \n",
       "2    19907774                     1       65     1.83             True   \n",
       "3    19894745                     1       76     1.08             True   \n",
       "4    19884808                     1       64    20.46             True   \n",
       "\n",
       "   suspected_infection  sofa_score  sepsis3  avg_urineoutput  glucose_min  \\\n",
       "0                    1           2     True       136.657143           82   \n",
       "1                    1           8     True        34.263158           94   \n",
       "2                    1           2     True       105.476191           65   \n",
       "3                    1           4     True        34.000000          267   \n",
       "4                    1           5     True       107.229508          146   \n",
       "\n",
       "   ...  race_Hispanic or Latin  race_Others race  race_White  \\\n",
       "0  ...                       0                 0           0   \n",
       "1  ...                       0                 0           0   \n",
       "2  ...                       0                 0           0   \n",
       "3  ...                       0                 0           0   \n",
       "4  ...                       0                 0           0   \n",
       "\n",
       "   antibiotic_Vancomycin  antibiotic_Vancomycin Antibiotic Lock  \\\n",
       "0                      1                                      0   \n",
       "1                      1                                      0   \n",
       "2                      1                                      0   \n",
       "3                      1                                      0   \n",
       "4                      1                                      0   \n",
       "\n",
       "   antibiotic_Vancomycin Enema  antibiotic_Vancomycin Intrathecal  \\\n",
       "0                            0                                  0   \n",
       "1                            0                                  0   \n",
       "2                            0                                  0   \n",
       "3                            0                                  0   \n",
       "4                            0                                  0   \n",
       "\n",
       "   antibiotic_Vancomycin Oral Liquid  gender_F  gender_M  \n",
       "0                                  0         1         0  \n",
       "1                                  0         1         0  \n",
       "2                                  0         1         0  \n",
       "3                                  0         1         0  \n",
       "4                                  0         1         0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure dataset is correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>max_age</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>first_hosp_stay</th>\n",
       "      <th>suspected_infection</th>\n",
       "      <th>sofa_score</th>\n",
       "      <th>sepsis3</th>\n",
       "      <th>avg_urineoutput</th>\n",
       "      <th>glucose_min</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_rad_91</th>\n",
       "      <th>w2v_rad_92</th>\n",
       "      <th>w2v_rad_93</th>\n",
       "      <th>w2v_rad_94</th>\n",
       "      <th>w2v_rad_95</th>\n",
       "      <th>w2v_rad_96</th>\n",
       "      <th>w2v_rad_97</th>\n",
       "      <th>w2v_rad_98</th>\n",
       "      <th>w2v_rad_99</th>\n",
       "      <th>w2v_rad_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19986715</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>10.58</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>136.657143</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054860</td>\n",
       "      <td>-0.134476</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>0.138862</td>\n",
       "      <td>-0.164830</td>\n",
       "      <td>0.010343</td>\n",
       "      <td>-0.004907</td>\n",
       "      <td>-0.059874</td>\n",
       "      <td>-0.104063</td>\n",
       "      <td>0.045992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19973083</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2.33</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>34.263158</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>-0.109384</td>\n",
       "      <td>0.119165</td>\n",
       "      <td>0.184869</td>\n",
       "      <td>-0.054834</td>\n",
       "      <td>0.005417</td>\n",
       "      <td>-0.094042</td>\n",
       "      <td>-0.079441</td>\n",
       "      <td>-0.029285</td>\n",
       "      <td>0.016895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19907774</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>1.83</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>105.476191</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080038</td>\n",
       "      <td>-0.117593</td>\n",
       "      <td>-0.005822</td>\n",
       "      <td>0.107834</td>\n",
       "      <td>-0.079289</td>\n",
       "      <td>0.127771</td>\n",
       "      <td>-0.030650</td>\n",
       "      <td>-0.005073</td>\n",
       "      <td>-0.121476</td>\n",
       "      <td>0.020389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19894745</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1.08</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061370</td>\n",
       "      <td>-0.187079</td>\n",
       "      <td>-0.034887</td>\n",
       "      <td>0.117981</td>\n",
       "      <td>-0.085173</td>\n",
       "      <td>0.077792</td>\n",
       "      <td>-0.034223</td>\n",
       "      <td>-0.060790</td>\n",
       "      <td>-0.179078</td>\n",
       "      <td>-0.021921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19884808</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20.46</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>107.229508</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080242</td>\n",
       "      <td>-0.085314</td>\n",
       "      <td>-0.076269</td>\n",
       "      <td>0.158099</td>\n",
       "      <td>-0.055382</td>\n",
       "      <td>0.081583</td>\n",
       "      <td>0.066497</td>\n",
       "      <td>-0.041396</td>\n",
       "      <td>-0.131663</td>\n",
       "      <td>-0.093858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hospital_expire_flag  max_age  los_icu  first_hosp_stay  \\\n",
       "0    19986715                     0       24    10.58             True   \n",
       "1    19973083                     0       58     2.33             True   \n",
       "2    19907774                     1       65     1.83             True   \n",
       "3    19894745                     1       76     1.08             True   \n",
       "4    19884808                     1       64    20.46             True   \n",
       "\n",
       "   suspected_infection  sofa_score  sepsis3  avg_urineoutput  glucose_min  \\\n",
       "0                    1           2     True       136.657143           82   \n",
       "1                    1           8     True        34.263158           94   \n",
       "2                    1           2     True       105.476191           65   \n",
       "3                    1           4     True        34.000000          267   \n",
       "4                    1           5     True       107.229508          146   \n",
       "\n",
       "   ...  w2v_rad_91  w2v_rad_92  w2v_rad_93  w2v_rad_94  w2v_rad_95  \\\n",
       "0  ...    0.054860   -0.134476    0.010113    0.138862   -0.164830   \n",
       "1  ...    0.016587   -0.109384    0.119165    0.184869   -0.054834   \n",
       "2  ...   -0.080038   -0.117593   -0.005822    0.107834   -0.079289   \n",
       "3  ...   -0.061370   -0.187079   -0.034887    0.117981   -0.085173   \n",
       "4  ...   -0.080242   -0.085314   -0.076269    0.158099   -0.055382   \n",
       "\n",
       "   w2v_rad_96  w2v_rad_97  w2v_rad_98  w2v_rad_99  w2v_rad_100  \n",
       "0    0.010343   -0.004907   -0.059874   -0.104063     0.045992  \n",
       "1    0.005417   -0.094042   -0.079441   -0.029285     0.016895  \n",
       "2    0.127771   -0.030650   -0.005073   -0.121476     0.020389  \n",
       "3    0.077792   -0.034223   -0.060790   -0.179078    -0.021921  \n",
       "4    0.081583    0.066497   -0.041396   -0.131663    -0.093858  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling of w2v vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# df contains both features and w2v columns, and 'subject_id'\n",
    "# Step 1: Select the Word2Vec columns\n",
    "w2v_cols = [col for col in df_w2v.columns if col.startswith(\"w2v_\") or col.startswith(\"rad_w2v_\")]\n",
    "\n",
    "# Step 2: Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 3: Fit and transform the embeddings\n",
    "scaled_embeddings = scaler.fit_transform(df_w2v[w2v_cols])\n",
    "\n",
    "# Step 4: Replace or store the scaled version in a new DataFrame\n",
    "scaled_w2v = pd.DataFrame(scaled_embeddings, columns=w2v_cols)\n",
    "scaled_w2v[\"subject_id\"] = df_w2v[\"subject_id\"].values  # reattach subject_id\n",
    "\n",
    "# Step 5 (Optional): Merge scaled embeddings back into main dataset\n",
    "# Drop the original embeddings first to avoid duplication\n",
    "scaled_w2v = df_w2v.drop(columns=w2v_cols).merge(scaled_w2v, on=\"subject_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_w2v.to_csv('Data/Old/W2V_old/data_w2v_radiology_old_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject_id' 'hospital_expire_flag' 'max_age' 'los_icu' 'first_hosp_stay'\n",
      " 'suspected_infection' 'sofa_score' 'sepsis3' 'avg_urineoutput'\n",
      " 'glucose_min' 'glucose_max' 'glucose_average' 'sodium_max' 'sodium_min'\n",
      " 'sodium_average' 'diabetes_without_cc' 'diabetes_with_cc'\n",
      " 'severe_liver_disease' 'aids' 'renal_disease' 'heart_rate_min'\n",
      " 'heart_rate_max' 'heart_rate_mean' 'sbp_min' 'sbp_max' 'sbp_mean'\n",
      " 'dbp_min' 'dbp_max' 'dbp_mean' 'resp_rate_min' 'resp_rate_max'\n",
      " 'resp_rate_mean' 'spo2_min' 'spo2_max' 'spo2_mean' 'coma' 'albumin'\n",
      " 'race_Black or African American' 'race_Hispanic or Latin'\n",
      " 'race_Others race' 'race_White' 'antibiotic_Vancomycin'\n",
      " 'antibiotic_Vancomycin Antibiotic Lock' 'antibiotic_Vancomycin Enema'\n",
      " 'antibiotic_Vancomycin Intrathecal' 'antibiotic_Vancomycin Oral Liquid'\n",
      " 'gender_F' 'gender_M' 'Discharge_summary_notes' 'Radiology_notes'\n",
      " 'combined_notes' 'w2v_rad_1' 'w2v_rad_2' 'w2v_rad_3' 'w2v_rad_4'\n",
      " 'w2v_rad_5' 'w2v_rad_6' 'w2v_rad_7' 'w2v_rad_8' 'w2v_rad_9' 'w2v_rad_10'\n",
      " 'w2v_rad_11' 'w2v_rad_12' 'w2v_rad_13' 'w2v_rad_14' 'w2v_rad_15'\n",
      " 'w2v_rad_16' 'w2v_rad_17' 'w2v_rad_18' 'w2v_rad_19' 'w2v_rad_20'\n",
      " 'w2v_rad_21' 'w2v_rad_22' 'w2v_rad_23' 'w2v_rad_24' 'w2v_rad_25'\n",
      " 'w2v_rad_26' 'w2v_rad_27' 'w2v_rad_28' 'w2v_rad_29' 'w2v_rad_30'\n",
      " 'w2v_rad_31' 'w2v_rad_32' 'w2v_rad_33' 'w2v_rad_34' 'w2v_rad_35'\n",
      " 'w2v_rad_36' 'w2v_rad_37' 'w2v_rad_38' 'w2v_rad_39' 'w2v_rad_40'\n",
      " 'w2v_rad_41' 'w2v_rad_42' 'w2v_rad_43' 'w2v_rad_44' 'w2v_rad_45'\n",
      " 'w2v_rad_46' 'w2v_rad_47' 'w2v_rad_48' 'w2v_rad_49' 'w2v_rad_50'\n",
      " 'w2v_rad_51' 'w2v_rad_52' 'w2v_rad_53' 'w2v_rad_54' 'w2v_rad_55'\n",
      " 'w2v_rad_56' 'w2v_rad_57' 'w2v_rad_58' 'w2v_rad_59' 'w2v_rad_60'\n",
      " 'w2v_rad_61' 'w2v_rad_62' 'w2v_rad_63' 'w2v_rad_64' 'w2v_rad_65'\n",
      " 'w2v_rad_66' 'w2v_rad_67' 'w2v_rad_68' 'w2v_rad_69' 'w2v_rad_70'\n",
      " 'w2v_rad_71' 'w2v_rad_72' 'w2v_rad_73' 'w2v_rad_74' 'w2v_rad_75'\n",
      " 'w2v_rad_76' 'w2v_rad_77' 'w2v_rad_78' 'w2v_rad_79' 'w2v_rad_80'\n",
      " 'w2v_rad_81' 'w2v_rad_82' 'w2v_rad_83' 'w2v_rad_84' 'w2v_rad_85'\n",
      " 'w2v_rad_86' 'w2v_rad_87' 'w2v_rad_88' 'w2v_rad_89' 'w2v_rad_90'\n",
      " 'w2v_rad_91' 'w2v_rad_92' 'w2v_rad_93' 'w2v_rad_94' 'w2v_rad_95'\n",
      " 'w2v_rad_96' 'w2v_rad_97' 'w2v_rad_98' 'w2v_rad_99' 'w2v_rad_100']\n"
     ]
    }
   ],
   "source": [
    "print(scaled_w2v.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import BERT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Paths and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "cache_dir = \"BERT/BERT_cache\"\n",
    "bert_models = [\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'dmis-lab/biobert-base-cased-v1.2',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Merged Patient Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs = {}\n",
    "\n",
    "for model_name in bert_models:\n",
    "    agg_cache_path = os.path.join(cache_dir, f\"patient_agg_{model_name.replace('/', '_')}.pkl\")\n",
    "    \n",
    "    if os.path.exists(agg_cache_path):\n",
    "        print(f\"Loading patient-level embeddings for {model_name}...\")\n",
    "        merged_df = joblib.load(agg_cache_path)\n",
    "        merged_dfs[model_name] = merged_df\n",
    "    else:\n",
    "        print(f\"⚠️ Cache not found for {model_name}, run full workflow first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BERT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_bert = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioBERT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_bert = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical+Bio BERT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinbio_bert ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I generally follow the pipeline designed in the author's github. I do this to provide side by side comparison of my ML pipeline vs theirs.\n",
    "\n",
    "When necessary, I include 'w/ edits' at the top of a code block to denote minor edits I make to the author's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Space and Predictor Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Feature Space & Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code the authors used to define X and y for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AC\n",
    "X = df.drop(columns=['hospital_expire_flag','subject_id','first_hosp_stay','suspected_infection','sepsis3'])\n",
    "y = df['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V Feature Space & Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DVAHnte3G3oJ"
   },
   "outputs": [],
   "source": [
    "# AC w/ edits\n",
    "\n",
    "X_w2v = scaled_w2v.drop(columns=['hospital_expire_flag','subject_id','first_hosp_stay','suspected_infection','sepsis3', 'Discharge_summary_notes', 'Radiology_notes', 'combined_notes'])\n",
    "y_w2v = scaled_w2v['hospital_expire_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BERT Feature Space & Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clin_bert = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clin_bert = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioBERT Feature Space & Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bio_bert ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bio_bert ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio + Clinical BERT Feature Space & Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clinbio_bert ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clinbio_bert ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MAJOR NOTE**\n",
    "\n",
    "The data here does not currently have demographic data stratified. I will need to update the stratification process prior to data splitting to ensure that the data is correctly balanced, or at least make sure that is the case for w2v_improved and the BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAJOR NOTE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author's used a method with SMOTE that is not used in best practice.\n",
    "\n",
    "Chatgpt:\n",
    "\n",
    "Best practice suggestion:\n",
    "\n",
    "Instead of doing SMOTE before the train/test split, it's usually better to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ChatGpt suggestion\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# This avoids introducing synthetic samples into your test set, which could inflate performance metrics.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SMOTE for Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author's Smote Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJSuzNTfHPdq"
   },
   "outputs": [],
   "source": [
    "# AC\n",
    "'''\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "#X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using smote prior to model training causes data leakage. Information from the test set leaks into training, skewing model accuracy and inflating results. To account for this I chose to take a different approach at this step, choosing to perform train/test split before applying smote.\n",
    "\n",
    "It becomes much harder to make fair comparisons after this since the models are trained, validated, and tested differently.\n",
    "\n",
    "However, my results should be more robust as it follows best practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tts_dataset_with_smote Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prompt engineer a function to be able to accept multiple dataframes so I don't need to redefine the T/T/S each time.\n",
    "\n",
    "The test/train/smote_train arrays are returned and below defined separately for each dataset.\n",
    "\n",
    "Technically x_train_smote and y_train_smote won't be used because I'm not doing a single holdout test. Since I am performing RepeatedStratifiedKFold cv, smote is performed inside the training folds to reduce information leakage.\n",
    "\n",
    "However, in the event I do choose to use single holdout because the work gets to intense, it is an easy fall back here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chatgpt\n",
    "# Includes AC in function wrapper\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tts_dataset_with_smote(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE resampling to a single (X, y) dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        X: features (array-like or DataFrame)\n",
    "        y: labels (array-like or Series)\n",
    "        test_size: float (default=0.2)\n",
    "            Fraction of dataset to allocate to the test split.\n",
    "        random_state: int (default=42)\n",
    "            Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        dict containing:\n",
    "            - X_train: original train features\n",
    "            - X_test: test features\n",
    "            - y_train: original train labels\n",
    "            - y_test: test labels\n",
    "            - X_train_smote: SMOTE-resampled train features\n",
    "            - y_train_smote: SMOTE-resampled train labels\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Apply SMOTE only to training set\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"X_train_smote\": X_train_smote,\n",
    "        \"y_train_smote\": y_train_smote\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tts = tts_dataset_with_smote(X, y)\n",
    "\n",
    "X_train, X_test = original_tts[\"X_train\"], original_tts[\"X_test\"]\n",
    "y_train, y_test = original_tts[\"y_train\"], original_tts[\"y_test\"]\n",
    "\n",
    "X_train_smote, y_train_smote = original_tts[\"X_train_smote\"], original_tts[\"y_train_smote\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec Data TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tts = tts_dataset_with_smote(X_w2v, y_w2v)\n",
    "\n",
    "X_train_w2v, X_test_w2v = w2v_tts[\"X_train\"], w2v_tts[\"X_test\"]\n",
    "y_train_w2v, y_test_w2v = w2v_tts[\"y_train\"], w2v_tts[\"y_test\"]\n",
    "\n",
    "X_train_smote_w2v, y_train_smote_w2v = w2v_tts[\"X_train_smote\"], w2v_tts[\"y_train_smote\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec Stratified Data TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "w2v_tts_strat = tts_dataset_with_smote(X_w2v_strat, y_w2v_strat)\n",
    "\n",
    "X_train_w2v_strat, X_test_w2v_strat = w2v_tts_strat[\"X_train\"], w2v_tts_strat[\"X_test\"]\n",
    "y_train_w2v_strat, y_test_w2v_strat = w2v_tts_strat[\"y_train\"], w2v_tts_strat[\"y_test\"]\n",
    "\n",
    "X_train_smote_w2v_strat, y_train_smote_w2v_strat = w2v_tts_strat[\"X_train_smote\"], w2v_tts_strat[\"y_train_smote\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BERT TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "clin_bert_tts = tts_dataset_with_smote(X_clin_bert, y_clin_bert)\n",
    "\n",
    "X_train_clin_bert, X_test_clin_bert = clin_bert_tts[\"X_train\"], clin_bert_tts[\"X_test\"]\n",
    "y_train_clin_bert, y_test_clin_bert = clin_bert_tts[\"y_train\"], clin_bert_tts[\"y_test\"]\n",
    "\n",
    "X_train_smote_clin_bert, y_train_smote_clin_bert = clin_bert_tts[\"X_train_smote\"], clin_bert_tts[\"y_train_smote\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio BERT TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bio_bert_tts = tts_dataset_with_smote(X_bio_bert, y_bio_bert)\n",
    "\n",
    "X_train_bio_bert, X_test_bio_bert = bio_bert_tts[\"X_train\"], bio_bert_tts[\"X_test\"]\n",
    "y_train_bio_bert, y_test_bio_bert = bio_bert_tts[\"y_train\"], bio_bert_tts[\"y_test\"]\n",
    "\n",
    "X_train_smote_bio_bert, y_train_smote_bio_bert = bio_bert_tts[\"X_train_smote\"], bio_bert_tts[\"y_train_smote\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Bio BERT TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "clinbio_bert_tts = tts_dataset_with_smote(X_clinbio_bert, y_clinbio_bert)\n",
    "\n",
    "X_train_clinbio_bert, X_test_clinbio_bert = clinbio_bert_tts[\"X_train\"], clinbio_bert_tts[\"X_test\"]\n",
    "y_train_clinbio_bert, y_test_clinbio_bert = clinbio_bert_tts[\"y_train\"], clinbio_bert_tts[\"y_test\"]\n",
    "\n",
    "X_train_smote_clinbio_bert, y_train_smote_clinbio_bert = clinbio_bert_tts[\"X_train_smote\"], clinbio_bert_tts[\"y_train_smote\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SMOTE training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Count of Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pg9LJ-W1p86R",
    "outputId": "f171b425-0f2f-499e-dda9-b87534e803d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "count_of_ones = y.sum()\n",
    "\n",
    "count_of_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors chose to plot count of ones to affirm that SMOTE was appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAIhCAYAAABANwzIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCnklEQVR4nO3de1hVZf7//9eOk4dgJyCnRNQyRwNrwgZxpjzgMRHNZqxI1Mm0sjRSszGbsmZGSie1GQ+lqXgMpymryWLETMuPZ4pJzJxqtLRAzHAjRqC4vn/0Y/3a4gEJ2OD9fFzXui7Xvd77Xu+1A3q5uPfSYVmWJQAAAMAQl3m6AQAAAKAuEYABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgIFL3Mcff6zf//73at26tRo1aqTLL79cN9xwg6ZPn67vvvvO0+1JklatWqXZs2fXytyPP/64WrZsKW9vb11xxRXnrS0oKNCIESMUHBysJk2aKD4+Xu+++26t9FWbNm7cKIfDoY0bN5637vjx45o0aZJ69+6t5s2by+FwaOrUqTXaS7du3dStW7cam++TTz7R1KlTdeDAgbOeKzo6usbOdSH/+te/NGDAAIWGhsrX11eBgYFKSEjQypUrdfLkyYuer1WrVhoxYkTNNwqgEgIwcAlbuHChYmNjtXPnTj3yyCPKzMzUmjVr9Lvf/U4vvPCCRo4c6ekWJdVeAH7jjTf0l7/8RcOGDdOmTZu0fv36c9aWlpYqISFB7777rp5//nm98cYbCg0NVd++fbVp06Ya760+OHr0qBYsWKDS0lINGjSoVs4xb948zZs3r8bm++STT/TUU0+dNQDXFcuy9Pvf/15JSUk6ffq0Zs6cqfXr12vp0qW67rrrNGbMmBq9ZgA1z9vTDQCoHVu3btX999+vXr166fXXX5efn599rFevXpowYYIyMzM92GHty83NlSSNGzdOISEh561dtGiRcnNztWXLFsXHx0uSunfvruuuu06TJk3S9u3ba73fuhYVFaXCwkI5HA59++23eumll2r8HB06dKjxOT1txowZSk9P11NPPaUnnnjC7diAAQM0adIkff755x7qDkBVcAcYuERNmzZNDodDCxYscAu/FXx9fZWUlGTvnz59WtOnT9cvfvEL+fn5KSQkRMOGDdOhQ4fcXneuX9Oe+avuil/Dv/zyy5oyZYoiIiIUEBCgnj17at++fW6vW7t2rb788ks5HA57O5+q9NqqVSs9/vjjkqTQ0NAL/np/zZo1ateunR1+Jcnb21tDhw7Vjh079PXXX9vjr7zyiuLi4uR0OtWkSRO1adNGd99993l7lqS5c+fq5ptvVkhIiJo2baqYmBhNnz690q/LK36Vv3PnTt100032OZ555hmdPn3arfbTTz9V37591aRJEwUHB+u+++7T8ePHL9iLpCq91xU2bNigbt26KSgoSI0bN1bLli1122236fvvvz/v6878ujhw4IAcDof++te/aubMmWrdurUuv/xyxcfHa9u2beedKz09Xb/73e8k/fiXk4r+09PT3eqq8r4VFRVp4sSJat26tXx9fXXllVcqNTVVJ06cOG8PJ0+e1LPPPqtf/OIX+uMf/3jWmrCwMP3mN7+x97/77juNGTNGV155pXx9fdWmTRtNmTJFpaWlF7xeh8NR6W732Za4VHzNbN26VV26dFHjxo3VqlUrLVmyRJK0du1a3XDDDWrSpIliYmIq/eV36tSpcjgc2rNnj+688045nU6Fhobq7rvvlsvlOm+fQENEAAYuQeXl5dqwYYNiY2MVGRlZpdfcf//9evTRR9WrVy+9+eab+tOf/qTMzEx16dJF3377bbV7eeyxx/Tll1/qpZde0oIFC/TZZ59pwIABKi8vl/Tjr8h//etfKywsTFu3brW3n9vrmjVr7CUemZmZ2rp1q+65555zzpmbm6uOHTtWGq8Y27Nnj6Qf76zffvvtatOmjTIyMrR27Vo98cQTOnXq1AXfiy+++ELJyclavny53nrrLY0cOVIzZszQvffeW6k2Pz9fd911l4YOHao333xT/fr10+TJk7VixQq75vDhw+ratatyc3M1b948LV++XMXFxXrwwQcv2MvFOHDggPr37y9fX18tXrxYmZmZeuaZZ9S0aVOVlZVVa865c+cqKytLs2fP1sqVK3XixAndcsst5w1b/fv317Rp0+zXV3yt9O/f366pyvv2/fffq2vXrlq6dKnGjRund955R48++qjS09OVlJQky7LO2cOuXbv03XffaeDAgVX6y8MPP/yg7t27a9myZRo/frzWrl2roUOHavr06Ro8eHBV3qoqy8/P1+9//3vdc889euONNxQTE6O7775bTz/9tCZPnqxJkybp1Vdf1eWXX65Bgwbpm2++qTTHbbfdpmuuuUavvvqq/vCHP2jVqlV6+OGHa7RPoF6wAFxy8vPzLUnWHXfcUaX6vXv3WpKsMWPGuI1v377dkmQ99thj9lhUVJQ1fPjwSnN07drV6tq1q73/3nvvWZKsW265xa3uH//4hyXJ2rp1qz3Wv39/KyoqqsZ7ffLJJy1J1pEjRy44r4+Pj3XvvfdWGt+yZYslyVq1apVlWZb117/+1ZJkHTt2rEr9nkt5ebl18uRJa9myZZaXl5f13Xff2ce6du1qSbK2b9/u9poOHTpYffr0sfcfffRRy+FwWDk5OW51vXr1siRZ7733XpX7OXLkiCXJevLJJysd++c//2lJqnSeqjjz62L//v2WJCsmJsY6deqUPb5jxw5LkvXyyy+fd75XXnnlnNdW1fctLS3Nuuyyy6ydO3e61VVc59tvv33O82dkZFiSrBdeeOG8fVZ44YUXLEnWP/7xD7fxZ5991pJkrVu3zh4783tryZIlliRr//79bq+t+N766XtQce27du2yx44ePWp5eXlZjRs3tr7++mt7PCcnx5Jk/e1vf7PHKr5Xpk+f7nauMWPGWI0aNbJOnz5dpesFGgruAAPQe++9J0mVljb86le/Uvv27X/WkxB+usxC+v/vqH755ZfVmq82ez3fHb2KYzfeeKMkaciQIfrHP/7htjTiQj766CMlJSUpKChIXl5e8vHx0bBhw1ReXq7//ve/brVhYWH61a9+5TbWsWNHt/ftvffe07XXXqvrrrvOrS45ObnKPVXF9ddfL19fX40ePVpLly7V//73v589Z//+/eXl5WXv/9yviwpVed/eeustRUdH6/rrr9epU6fsrU+fPlV6esbF2LBhg5o2barf/va3buMVX781+ZSR8PBwxcbG2vuBgYEKCQnR9ddfr4iICHu8ffv2ks7+Xp/t+/WHH35QQUFBjfUJ1AcEYOASVPEYr/3791ep/ujRo5J+/B/omSIiIuzj1REUFOS2X7EeuaSkpFrz1VavQUFBZ31txaPiAgMDJUk333yzXn/9dZ06dUrDhg1TixYtFB0drZdffvm883/11Ve66aab9PXXX+v555/XBx98oJ07d2ru3LmSKr8fZ75v0o/v3U/rjh49qrCwsEp1Zxv7Oa666iqtX79eISEheuCBB3TVVVfpqquu0vPPP1/tOWv66+Jc81bM/dN5Dx8+rI8//lg+Pj5um7+/vyzLOu+Sn5YtW0rSRX1vhYWFVfrLVUhIiLy9vX/W99aZKr5Gf6ri8Wxnjkk/Ls84U239dwHqG54CAVyCvLy8lJCQoHfeeUeHDh1SixYtzltf8T+9vLy8SrXffPONgoOD7f1GjRqd9cM73377rVtdbbmYXi9GTEyMdu/eXWm8Yuynz5cdOHCgBg4cqNLSUm3btk1paWlKTk5Wq1at3D5E91Ovv/66Tpw4oddee01RUVH2eE5OTrX6lX58L/Lz8yuNn23s57rpppt00003qby8XLt27dLf//53paamKjQ0VHfccUeNn682BQcHq3Hjxlq8ePE5j59Lp06dFBgYqDfeeENpaWkXXAccFBSk7du3y7Ist9qCggKdOnXqvOdq1KiRJFX6fvs5a/IB/Ig7wMAlavLkybIsS6NGjTrrB5VOnjypf/3rX5KkHj16SJLbB4WkHz9Nv3fvXiUkJNhjrVq10scff+xW99///tftyQ4X68w7dOdzMb1ejFtvvVWffvqp2+POTp06pRUrViguLs7tV8g/7btr16569tlnJf24xOFcKsLPT5/IYVmWFi5cWK1+pR+fhLBnzx795z//cRtftWpVtee8EC8vL8XFxdl3rj/88MNaO9fZ1MQdycTERH3xxRcKCgpSp06dKm2tWrU652t9fHz06KOP6tNPP9Wf/vSns9YUFBTo//7v/yRJCQkJKi4u1uuvv+5Ws2zZMvv4uVT0ceb325tvvnmBKwRwIdwBBi5R8fHxmj9/vsaMGaPY2Fjdf//9uvbaa3Xy5El99NFHWrBggaKjozVgwAC1a9dOo0eP1t///ndddtll6tevnw4cOKA//vGPioyMdPsUeEpKioYOHaoxY8botttu05dffqnp06erefPm1e41JiZGr732mubPn6/Y2Fhddtll6tSp01lrL6bXi3H33Xdr7ty5+t3vfqdnnnlGISEhmjdvnvbt2+f2D2g88cQTOnTokBISEtSiRQsdO3ZMzz//vHx8fNS1a9dzzt+rVy/5+vrqzjvv1KRJk/TDDz9o/vz5KiwsrFa/kpSamqrFixerf//++vOf/6zQ0FCtXLlSn376aZXneOedd3TixAn70WmffPKJ/vnPf0qSbrnlFjVp0kQvvPCCNmzYoP79+6tly5b64Ycf7LunPXv2rHb/1VFxJ37BggXy9/dXo0aN1Lp167MufTiX1NRUvfrqq7r55pv18MMPq2PHjjp9+rS++uorrVu3ThMmTFBcXNw5X//II49o7969evLJJ7Vjxw4lJycrMjJSLpdL77//vhYsWKCnnnpKv/71rzVs2DDNnTtXw4cP14EDBxQTE6PNmzdr2rRpuuWWW877/t14441q166dJk6cqFOnTqlZs2Zas2aNNm/eXPU3DMDZefYzeABqW05OjjV8+HCrZcuWlq+vr9W0aVPrl7/8pfXEE09YBQUFdl15ebn17LPPWtdcc43l4+NjBQcHW0OHDrUOHjzoNt/p06et6dOnW23atLEaNWpkderUydqwYcM5nwLxyiuvuL2+4ikAS5Yssce+++4767e//a11xRVXWA6Hw7rQj6aq9noxT4GwrB+fnjFs2DArMDDQatSokdW5c2crKyvLreatt96y+vXrZ1155ZWWr6+vFRISYt1yyy3WBx98cMH5//Wvf1nXXXed1ahRI+vKK6+0HnnkEeudd9456yf6r7322kqvHz58eKWnZXzyySdWr169rEaNGlmBgYHWyJEjrTfeeKPKT4GIioqyJJ11q3j6wNatW61bb73VioqKsvz8/KygoCCra9eu1ptvvnnB+c/1FIgZM2ZUqtU5nkJxptmzZ1utW7e2vLy83L6WLuZ9Ky4uth5//HGrXbt2lq+vr+V0Oq2YmBjr4YcftvLz8y/Yg2VZ1htvvGH179/fat68ueXt7W01a9bM6t69u/XCCy9YpaWldt3Ro0et++67zwoPD7e8vb2tqKgoa/LkydYPP/zgNt/ZnrDy3//+1+rdu7cVEBBgNW/e3Bo7dqy1du3aKn/NREVFWf379680Lsl64IEH7P1zfa+c60kUQEPnsKzzPPAQAAAAuMSwBhgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMwj+EUUWnT5/WN998I39//wv+05cAAACoe5Zl6fjx44qIiNBll537Pi8BuIq++eYbRUZGeroNAAAAXMDBgwfVokWLcx4nAFeRv7+/pB/f0ICAAA93AwAAgDMVFRUpMjLSzm3nQgCuooplDwEBAQRgAACAeuxCy1X5EBwAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACj1JsAnJaWJofDodTUVHvMsixNnTpVERERaty4sbp166Y9e/a4va60tFRjx45VcHCwmjZtqqSkJB06dMitprCwUCkpKXI6nXI6nUpJSdGxY8fq4KoAAABQ39SLALxz504tWLBAHTt2dBufPn26Zs6cqTlz5mjnzp0KCwtTr169dPz4cbsmNTVVa9asUUZGhjZv3qzi4mIlJiaqvLzcrklOTlZOTo4yMzOVmZmpnJwcpaSk1Nn1AQAAoP7weAAuLi7WXXfdpYULF6pZs2b2uGVZmj17tqZMmaLBgwcrOjpaS5cu1ffff69Vq1ZJklwulxYtWqTnnntOPXv21C9/+UutWLFCu3fv1vr16yVJe/fuVWZmpl566SXFx8crPj5eCxcu1FtvvaV9+/Z55JoBAADgOR4PwA888ID69++vnj17uo3v379f+fn56t27tz3m5+enrl27asuWLZKk7OxsnTx50q0mIiJC0dHRds3WrVvldDoVFxdn13Tu3FlOp9OuOZvS0lIVFRW5bQAAAGj4vD158oyMDH344YfauXNnpWP5+fmSpNDQULfx0NBQffnll3aNr6+v253jipqK1+fn5yskJKTS/CEhIXbN2aSlpempp566uAuqRbGPLPN0CwBqSfaMYZ5uAQCM4rE7wAcPHtRDDz2kFStWqFGjRuesczgcbvuWZVUaO9OZNWerv9A8kydPlsvlsreDBw+e95wAAABoGDwWgLOzs1VQUKDY2Fh5e3vL29tbmzZt0t/+9jd5e3vbd37PvEtbUFBgHwsLC1NZWZkKCwvPW3P48OFK5z9y5Eilu8s/5efnp4CAALcNAAAADZ/HAnBCQoJ2796tnJwce+vUqZPuuusu5eTkqE2bNgoLC1NWVpb9mrKyMm3atEldunSRJMXGxsrHx8etJi8vT7m5uXZNfHy8XC6XduzYYdds375dLpfLrgEAAIA5PLYG2N/fX9HR0W5jTZs2VVBQkD2empqqadOmqW3btmrbtq2mTZumJk2aKDk5WZLkdDo1cuRITZgwQUFBQQoMDNTEiRMVExNjf6iuffv26tu3r0aNGqUXX3xRkjR69GglJiaqXbt2dXjFAAAAqA88+iG4C5k0aZJKSko0ZswYFRYWKi4uTuvWrZO/v79dM2vWLHl7e2vIkCEqKSlRQkKC0tPT5eXlZdesXLlS48aNs58WkZSUpDlz5tT59QAAAMDzHJZlWZ5uoiEoKiqS0+mUy+XyyHpgngIBXLp4CgQA1Iyq5jWPPwcYAAAAqEsEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBSPBuD58+erY8eOCggIUEBAgOLj4/XOO+/Yx0eMGCGHw+G2de7c2W2O0tJSjR07VsHBwWratKmSkpJ06NAht5rCwkKlpKTI6XTK6XQqJSVFx44dq4tLBAAAQD3j0QDcokULPfPMM9q1a5d27dqlHj16aODAgdqzZ49d07dvX+Xl5dnb22+/7TZHamqq1qxZo4yMDG3evFnFxcVKTExUeXm5XZOcnKycnBxlZmYqMzNTOTk5SklJqbPrBAAAQP3h7cmTDxgwwG3/L3/5i+bPn69t27bp2muvlST5+fkpLCzsrK93uVxatGiRli9frp49e0qSVqxYocjISK1fv159+vTR3r17lZmZqW3btikuLk6StHDhQsXHx2vfvn1q165dLV4hAAAA6pt6swa4vLxcGRkZOnHihOLj4+3xjRs3KiQkRNdcc41GjRqlgoIC+1h2drZOnjyp3r1722MRERGKjo7Wli1bJElbt26V0+m0w68kde7cWU6n0645m9LSUhUVFbltAAAAaPg8HoB3796tyy+/XH5+frrvvvu0Zs0adejQQZLUr18/rVy5Uhs2bNBzzz2nnTt3qkePHiotLZUk5efny9fXV82aNXObMzQ0VPn5+XZNSEhIpfOGhITYNWeTlpZmrxl2Op2KjIysqUsGAACAB3l0CYQktWvXTjk5OTp27JheffVVDR8+XJs2bVKHDh10++2323XR0dHq1KmToqKitHbtWg0ePPicc1qWJYfDYe//9M/nqjnT5MmTNX78eHu/qKiIEAwAAHAJ8HgA9vX11dVXXy1J6tSpk3bu3Knnn39eL774YqXa8PBwRUVF6bPPPpMkhYWFqaysTIWFhW53gQsKCtSlSxe75vDhw5XmOnLkiEJDQ8/Zl5+fn/z8/H7WtQEAAKD+8fgSiDNZlmUvcTjT0aNHdfDgQYWHh0uSYmNj5ePjo6ysLLsmLy9Pubm5dgCOj4+Xy+XSjh077Jrt27fL5XLZNQAAADCHR+8AP/bYY+rXr58iIyN1/PhxZWRkaOPGjcrMzFRxcbGmTp2q2267TeHh4Tpw4IAee+wxBQcH69Zbb5UkOZ1OjRw5UhMmTFBQUJACAwM1ceJExcTE2E+FaN++vfr27atRo0bZd5VHjx6txMREngABAABgII8G4MOHDyslJUV5eXlyOp3q2LGjMjMz1atXL5WUlGj37t1atmyZjh07pvDwcHXv3l2rV6+Wv7+/PcesWbPk7e2tIUOGqKSkRAkJCUpPT5eXl5dds3LlSo0bN85+WkRSUpLmzJlT59cLAAAAz3NYlmV5uomGoKioSE6nUy6XSwEBAXV+/thHltX5OQHUjewZwzzdAgBcEqqa1+rdGmAAAACgNhGAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUTwagOfPn6+OHTsqICBAAQEBio+P1zvvvGMftyxLU6dOVUREhBo3bqxu3bppz549bnOUlpZq7NixCg4OVtOmTZWUlKRDhw651RQWFiolJUVOp1NOp1MpKSk6duxYXVwiAAAA6hmPBuAWLVromWee0a5du7Rr1y716NFDAwcOtEPu9OnTNXPmTM2ZM0c7d+5UWFiYevXqpePHj9tzpKamas2aNcrIyNDmzZtVXFysxMRElZeX2zXJycnKyclRZmamMjMzlZOTo5SUlDq/XgAAAHiew7Isy9NN/FRgYKBmzJihu+++WxEREUpNTdWjjz4q6ce7vaGhoXr22Wd17733yuVyqXnz5lq+fLluv/12SdI333yjyMhIvf322+rTp4/27t2rDh06aNu2bYqLi5Mkbdu2TfHx8fr000/Vrl27KvVVVFQkp9Mpl8ulgICA2rn484h9ZFmdnxNA3cieMczTLQDAJaGqea3erAEuLy9XRkaGTpw4ofj4eO3fv1/5+fnq3bu3XePn56euXbtqy5YtkqTs7GydPHnSrSYiIkLR0dF2zdatW+V0Ou3wK0mdO3eW0+m0a86mtLRURUVFbhsAAAAaPo8H4N27d+vyyy+Xn5+f7rvvPq1Zs0YdOnRQfn6+JCk0NNStPjQ01D6Wn58vX19fNWvW7Lw1ISEhlc4bEhJi15xNWlqavWbY6XQqMjLyZ10nAAAA6gePB+B27dopJydH27Zt0/3336/hw4frk08+sY87HA63esuyKo2d6cyas9VfaJ7JkyfL5XLZ28GDB6t6SQAAAKjHPB6AfX19dfXVV6tTp05KS0vTddddp+eff15hYWGSVOkubUFBgX1XOCwsTGVlZSosLDxvzeHDhyud98iRI5XuLv+Un5+f/XSKig0AAAANn8cD8Jksy1Jpaalat26tsLAwZWVl2cfKysq0adMmdenSRZIUGxsrHx8ft5q8vDzl5ubaNfHx8XK5XNqxY4dds337drlcLrsGAAAA5vD25Mkfe+wx9evXT5GRkTp+/LgyMjK0ceNGZWZmyuFwKDU1VdOmTVPbtm3Vtm1bTZs2TU2aNFFycrIkyel0auTIkZowYYKCgoIUGBioiRMnKiYmRj179pQktW/fXn379tWoUaP04osvSpJGjx6txMTEKj8BAgAAAJcOjwbgw4cPKyUlRXl5eXI6nerYsaMyMzPVq1cvSdKkSZNUUlKiMWPGqLCwUHFxcVq3bp38/f3tOWbNmiVvb28NGTJEJSUlSkhIUHp6ury8vOyalStXaty4cfbTIpKSkjRnzpy6vVgAAADUC/XuOcD1Fc8BBlBbeA4wANSMBvccYAAAAKAuEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRPBqA09LSdOONN8rf318hISEaNGiQ9u3b51YzYsQIORwOt61z585uNaWlpRo7dqyCg4PVtGlTJSUl6dChQ241hYWFSklJkdPplNPpVEpKio4dO1bblwgAAIB6xqMBeNOmTXrggQe0bds2ZWVl6dSpU+rdu7dOnDjhVte3b1/l5eXZ29tvv+12PDU1VWvWrFFGRoY2b96s4uJiJSYmqry83K5JTk5WTk6OMjMzlZmZqZycHKWkpNTJdQIAAKD+8PbkyTMzM932lyxZopCQEGVnZ+vmm2+2x/38/BQWFnbWOVwulxYtWqTly5erZ8+ekqQVK1YoMjJS69evV58+fbR3715lZmZq27ZtiouLkyQtXLhQ8fHx2rdvn9q1a1dLVwgAAID6pl6tAXa5XJKkwMBAt/GNGzcqJCRE11xzjUaNGqWCggL7WHZ2tk6ePKnevXvbYxEREYqOjtaWLVskSVu3bpXT6bTDryR17txZTqfTrjlTaWmpioqK3DYAAAA0fPUmAFuWpfHjx+s3v/mNoqOj7fF+/fpp5cqV2rBhg5577jnt3LlTPXr0UGlpqSQpPz9fvr6+atasmdt8oaGhys/Pt2tCQkIqnTMkJMSuOVNaWpq9XtjpdCoyMrKmLhUAAAAe5NElED/14IMP6uOPP9bmzZvdxm+//Xb7z9HR0erUqZOioqK0du1aDR48+JzzWZYlh8Nh7//0z+eq+anJkydr/Pjx9n5RUREhGAAA4BJQL+4Ajx07Vm+++abee+89tWjR4ry14eHhioqK0meffSZJCgsLU1lZmQoLC93qCgoKFBoaatccPny40lxHjhyxa87k5+engIAAtw0AAAANn0cDsGVZevDBB/Xaa69pw4YNat269QVfc/ToUR08eFDh4eGSpNjYWPn4+CgrK8uuycvLU25urrp06SJJio+Pl8vl0o4dO+ya7du3y+Vy2TUAAAAwg0eXQDzwwANatWqV3njjDfn7+9vrcZ1Opxo3bqzi4mJNnTpVt912m8LDw3XgwAE99thjCg4O1q233mrXjhw5UhMmTFBQUJACAwM1ceJExcTE2E+FaN++vfr27atRo0bpxRdflCSNHj1aiYmJPAECAADAMB4NwPPnz5ckdevWzW18yZIlGjFihLy8vLR7924tW7ZMx44dU3h4uLp3767Vq1fL39/frp81a5a8vb01ZMgQlZSUKCEhQenp6fLy8rJrVq5cqXHjxtlPi0hKStKcOXNq/yIBAABQrzgsy7I83URDUFRUJKfTKZfL5ZH1wLGPLKvzcwKoG9kzhnm6BQC4JFQ1r9WLD8EBAAAAdYUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABglGoF4DZt2ujo0aOVxo8dO6Y2bdr87KYAAACA2lKtAHzgwAGVl5dXGi8tLdXXX3/9s5sCAAAAaov3xRS/+eab9p///e9/y+l02vvl5eV699131apVqxprDgAAAKhpFxWABw0aJElyOBwaPny42zEfHx+1atVKzz33XI01BwAAANS0iwrAp0+fliS1bt1aO3fuVHBwcK00BQAAANSWiwrAFfbv31/TfQAAAAB1oloBWJLeffddvfvuuyooKLDvDFdYvHjxz24MAAAAqA3VCsBPPfWUnn76aXXq1Enh4eFyOBw13RcAAABQK6oVgF944QWlp6crJSWlpvsBAAAAalW1ngNcVlamLl261HQvAAAAQK2rVgC+5557tGrVqpruBQAAAKh11VoC8cMPP2jBggVav369OnbsKB8fH7fjM2fOrJHmAAAAgJpWrQD88ccf6/rrr5ck5ebmuh3jA3EAAACoz6oVgN97772a7gMAAACoE9VaAwwAAAA0VNW6A9y9e/fzLnXYsGFDtRsCAAAAalO1AnDF+t8KJ0+eVE5OjnJzczV8+PCa6AsAAACoFdUKwLNmzTrr+NSpU1VcXPyzGgIAAABqU42uAR46dKgWL15ck1MCAAAANapGA/DWrVvVqFGjmpwSAAAAqFHVWgIxePBgt33LspSXl6ddu3bpj3/8Y400BgAAANSGagVgp9Pptn/ZZZepXbt2evrpp9W7d+8aaQwAAACoDdUKwEuWLKnpPgAAAIA6Ua0AXCE7O1t79+6Vw+FQhw4d9Mtf/rKm+gIAAABqRbUCcEFBge644w5t3LhRV1xxhSzLksvlUvfu3ZWRkaHmzZvXdJ8AAABAjajWUyDGjh2roqIi7dmzR999950KCwuVm5uroqIijRs3rsrzpKWl6cYbb5S/v79CQkI0aNAg7du3z63GsixNnTpVERERaty4sbp166Y9e/a41ZSWlmrs2LEKDg5W06ZNlZSUpEOHDrnVFBYWKiUlRU6nU06nUykpKTp27Fh1Lh8AAAANWLUCcGZmpubPn6/27dvbYx06dNDcuXP1zjvvVHmeTZs26YEHHtC2bduUlZWlU6dOqXfv3jpx4oRdM336dM2cOVNz5szRzp07FRYWpl69eun48eN2TWpqqtasWaOMjAxt3rxZxcXFSkxMVHl5uV2TnJysnJwcZWZmKjMzUzk5OUpJSanO5QMAAKABq9YSiNOnT8vHx6fSuI+Pj06fPl3leTIzM932lyxZopCQEGVnZ+vmm2+WZVmaPXu2pkyZYj96benSpQoNDdWqVat07733yuVyadGiRVq+fLl69uwpSVqxYoUiIyO1fv169enTR3v37lVmZqa2bdumuLg4SdLChQsVHx+vffv2qV27dtV5GwAAANAAVesOcI8ePfTQQw/pm2++sce+/vprPfzww0pISKh2My6XS5IUGBgoSdq/f7/y8/PdHq3m5+enrl27asuWLZJ+/CDeyZMn3WoiIiIUHR1t12zdulVOp9MOv5LUuXNnOZ1Ou+ZMpaWlKioqctsAAADQ8FUrAM+ZM0fHjx9Xq1atdNVVV+nqq69W69atdfz4cf3973+vViOWZWn8+PH6zW9+o+joaElSfn6+JCk0NNStNjQ01D6Wn58vX19fNWvW7Lw1ISEhlc4ZEhJi15wpLS3NXi/sdDoVGRlZresCAABA/VKtJRCRkZH68MMPlZWVpU8//VSWZalDhw72EoTqePDBB/Xxxx9r8+bNlY45HA63fcuyKo2d6cyas9Wfb57Jkydr/Pjx9n5RUREhGAAA4BJwUXeAN2zYoA4dOtjLAXr16qWxY8dq3LhxuvHGG3Xttdfqgw8+uOgmxo4dqzfffFPvvfeeWrRoYY+HhYVJUqW7tAUFBfZd4bCwMJWVlamwsPC8NYcPH6503iNHjlS6u1zBz89PAQEBbhsAAAAavosKwLNnz9aoUaPOGgadTqfuvfdezZw5s8rzWZalBx98UK+99po2bNig1q1bux1v3bq1wsLClJWVZY+VlZVp06ZN6tKliyQpNjZWPj4+bjV5eXnKzc21a+Lj4+VyubRjxw67Zvv27XK5XHYNAAAAzHBRAfg///mP+vbte87jvXv3VnZ2dpXne+CBB7RixQqtWrVK/v7+ys/PV35+vkpKSiT9uGwhNTVV06ZN05o1a5Sbm6sRI0aoSZMmSk5OlvRj8B45cqQmTJigd999Vx999JGGDh2qmJgYe0lG+/bt1bdvX40aNUrbtm3Ttm3bNGrUKCUmJvIECAAAAMNc1Brgw4cPn/XxZ/Zk3t46cuRIleebP3++JKlbt25u40uWLNGIESMkSZMmTVJJSYnGjBmjwsJCxcXFad26dfL397frZ82aJW9vbw0ZMkQlJSVKSEhQenq6vLy87JqVK1dq3Lhx9tMikpKSNGfOnCr3CgAAgEuDw7Isq6rFV111lf7617/q1ltvPevx1157TRMnTtT//ve/GmuwvigqKpLT6ZTL5fLIeuDYR5bV+TkB1I3sGcM83QIAXBKqmtcuagnELbfcoieeeEI//PBDpWMlJSV68sknlZiYePHdAgAAAHXkopZAPP7443rttdd0zTXX6MEHH1S7du3kcDi0d+9ezZ07V+Xl5ZoyZUpt9QoAAAD8bBcVgENDQ7Vlyxbdf//9mjx5sipWTzgcDvXp00fz5s0752PFAAAAgPrgov8hjKioKL399tsqLCzU559/Lsuy1LZt20r/EhsAAABQH1XrX4KTpGbNmunGG2+syV4AAACAWndRH4IDAAAAGjoCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGMWjAfj999/XgAEDFBERIYfDoddff93t+IgRI+RwONy2zp07u9WUlpZq7NixCg4OVtOmTZWUlKRDhw651RQWFiolJUVOp1NOp1MpKSk6duxYLV8dAAAA6iOPBuATJ07ouuuu05w5c85Z07dvX+Xl5dnb22+/7XY8NTVVa9asUUZGhjZv3qzi4mIlJiaqvLzcrklOTlZOTo4yMzOVmZmpnJwcpaSk1Np1AQAAoP7y9uTJ+/Xrp379+p23xs/PT2FhYWc95nK5tGjRIi1fvlw9e/aUJK1YsUKRkZFav369+vTpo7179yozM1Pbtm1TXFycJGnhwoWKj4/Xvn371K5du5q9KAAAANRr9X4N8MaNGxUSEqJrrrlGo0aNUkFBgX0sOztbJ0+eVO/eve2xiIgIRUdHa8uWLZKkrVu3yul02uFXkjp37iyn02nXnE1paamKiorcNgAAADR89ToA9+vXTytXrtSGDRv03HPPaefOnerRo4dKS0slSfn5+fL19VWzZs3cXhcaGqr8/Hy7JiQkpNLcISEhds3ZpKWl2WuGnU6nIiMja/DKAAAA4CkeXQJxIbfffrv95+joaHXq1ElRUVFau3atBg8efM7XWZYlh8Nh7//0z+eqOdPkyZM1fvx4e7+oqIgQDAAAcAmo13eAzxQeHq6oqCh99tlnkqSwsDCVlZWpsLDQra6goEChoaF2zeHDhyvNdeTIEbvmbPz8/BQQEOC2AQAAoOFrUAH46NGjOnjwoMLDwyVJsbGx8vHxUVZWll2Tl5en3NxcdenSRZIUHx8vl8ulHTt22DXbt2+Xy+WyawAAAGAOjy6BKC4u1ueff27v79+/Xzk5OQoMDFRgYKCmTp2q2267TeHh4Tpw4IAee+wxBQcH69Zbb5UkOZ1OjRw5UhMmTFBQUJACAwM1ceJExcTE2E+FaN++vfr27atRo0bpxRdflCSNHj1aiYmJPAECAADAQB4NwLt27VL37t3t/Yo1t8OHD9f8+fO1e/duLVu2TMeOHVN4eLi6d++u1atXy9/f337NrFmz5O3trSFDhqikpEQJCQlKT0+Xl5eXXbNy5UqNGzfOflpEUlLSeZ89DAAAgEuXw7Isy9NNNARFRUVyOp1yuVweWQ8c+8iyOj8ngLqRPWOYp1sAgEtCVfNag1oDDAAAAPxcBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYxdvTDQAAzBT7yDJPtwCglmTPGObpFs6LO8AAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMIpHA/D777+vAQMGKCIiQg6HQ6+//rrbccuyNHXqVEVERKhx48bq1q2b9uzZ41ZTWlqqsWPHKjg4WE2bNlVSUpIOHTrkVlNYWKiUlBQ5nU45nU6lpKTo2LFjtXx1AAAAqI88GoBPnDih6667TnPmzDnr8enTp2vmzJmaM2eOdu7cqbCwMPXq1UvHjx+3a1JTU7VmzRplZGRo8+bNKi4uVmJiosrLy+2a5ORk5eTkKDMzU5mZmcrJyVFKSkqtXx8AAADqH29Pnrxfv37q16/fWY9ZlqXZs2drypQpGjx4sCRp6dKlCg0N1apVq3TvvffK5XJp0aJFWr58uXr27ClJWrFihSIjI7V+/Xr16dNHe/fuVWZmprZt26a4uDhJ0sKFCxUfH699+/apXbt2dXOxAAAAqBfq7Rrg/fv3Kz8/X71797bH/Pz81LVrV23ZskWSlJ2drZMnT7rVREREKDo62q7ZunWrnE6nHX4lqXPnznI6nXbN2ZSWlqqoqMhtAwAAQMNXbwNwfn6+JCk0NNRtPDQ01D6Wn58vX19fNWvW7Lw1ISEhleYPCQmxa84mLS3NXjPsdDoVGRn5s64HAAAA9UO9DcAVHA6H275lWZXGznRmzdnqLzTP5MmT5XK57O3gwYMX2TkAAADqo3obgMPCwiSp0l3agoIC+65wWFiYysrKVFhYeN6aw4cPV5r/yJEjle4u/5Sfn58CAgLcNgAAADR89TYAt27dWmFhYcrKyrLHysrKtGnTJnXp0kWSFBsbKx8fH7eavLw85ebm2jXx8fFyuVzasWOHXbN9+3a5XC67BgAAAObw6FMgiouL9fnnn9v7+/fvV05OjgIDA9WyZUulpqZq2rRpatu2rdq2batp06apSZMmSk5OliQ5nU6NHDlSEyZMUFBQkAIDAzVx4kTFxMTYT4Vo3769+vbtq1GjRunFF1+UJI0ePVqJiYk8AQIAAMBAHg3Au3btUvfu3e398ePHS5KGDx+u9PR0TZo0SSUlJRozZowKCwsVFxendevWyd/f337NrFmz5O3trSFDhqikpEQJCQlKT0+Xl5eXXbNy5UqNGzfOflpEUlLSOZ89DAAAgEubw7Isy9NNNARFRUVyOp1yuVweWQ8c+8iyOj8ngLqRPWOYp1vwCH6uAZcuT/1cq2peq7drgAEAAIDaQAAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGqdcBeOrUqXI4HG5bWFiYfdyyLE2dOlURERFq3LixunXrpj179rjNUVpaqrFjxyo4OFhNmzZVUlKSDh06VNeXAgAAgHqiXgdgSbr22muVl5dnb7t377aPTZ8+XTNnztScOXO0c+dOhYWFqVevXjp+/Lhdk5qaqjVr1igjI0ObN29WcXGxEhMTVV5e7onLAQAAgId5e7qBC/H29na761vBsizNnj1bU6ZM0eDBgyVJS5cuVWhoqFatWqV7771XLpdLixYt0vLly9WzZ09J0ooVKxQZGan169erT58+dXotAAAA8Lx6fwf4s88+U0REhFq3bq077rhD//vf/yRJ+/fvV35+vnr37m3X+vn5qWvXrtqyZYskKTs7WydPnnSriYiIUHR0tF1zLqWlpSoqKnLbAAAA0PDV6wAcFxenZcuW6d///rcWLlyo/Px8denSRUePHlV+fr4kKTQ01O01oaGh9rH8/Hz5+vqqWbNm56w5l7S0NDmdTnuLjIyswSsDAACAp9TrANyvXz/ddtttiomJUc+ePbV27VpJPy51qOBwONxeY1lWpbEzVaVm8uTJcrlc9nbw4MFqXgUAAADqk3odgM/UtGlTxcTE6LPPPrPXBZ95J7egoMC+KxwWFqaysjIVFhaes+Zc/Pz8FBAQ4LYBAACg4WtQAbi0tFR79+5VeHi4WrdurbCwMGVlZdnHy8rKtGnTJnXp0kWSFBsbKx8fH7eavLw85ebm2jUAAAAwS71+CsTEiRM1YMAAtWzZUgUFBfrzn/+soqIiDR8+XA6HQ6mpqZo2bZratm2rtm3batq0aWrSpImSk5MlSU6nUyNHjtSECRMUFBSkwMBATZw40V5SAQAAAPPU6wB86NAh3Xnnnfr222/VvHlzde7cWdu2bVNUVJQkadKkSSopKdGYMWNUWFiouLg4rVu3Tv7+/vYcs2bNkre3t4YMGaKSkhIlJCQoPT1dXl5enrosAAAAeJDDsizL0000BEVFRXI6nXK5XB5ZDxz7yLI6PyeAupE9Y5inW/AIfq4Bly5P/Vyral5rUGuAAQAAgJ+LAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABgFAIwAAAAjEIABgAAgFEIwAAAADAKARgAAABGIQADAADAKARgAAAAGIUADAAAAKMQgAEAAGAUAjAAAACMQgAGAACAUQjAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxiVACeN2+eWrdurUaNGik2NlYffPCBp1sCAABAHTMmAK9evVqpqamaMmWKPvroI910003q16+fvvrqK0+3BgAAgDpkTACeOXOmRo4cqXvuuUft27fX7NmzFRkZqfnz53u6NQAAANQhb083UBfKysqUnZ2tP/zhD27jvXv31pYtW876mtLSUpWWltr7LpdLklRUVFR7jZ5HeWmJR84LoPZ56ueKp/FzDbh0eernWsV5Lcs6b50RAfjbb79VeXm5QkND3cZDQ0OVn59/1tekpaXpqaeeqjQeGRlZKz0CMJfz7/d5ugUAqFGe/rl2/PhxOZ3Ocx43IgBXcDgcbvuWZVUaqzB58mSNHz/e3j99+rS+++47BQUFnfM1QE0oKipSZGSkDh48qICAAE+3AwA/Gz/XUFcsy9Lx48cVERFx3jojAnBwcLC8vLwq3e0tKCiodFe4gp+fn/z8/NzGrrjiitpqEagkICCA/1EAuKTwcw114Xx3fisY8SE4X19fxcbGKisry208KytLXbp08VBXAAAA8AQj7gBL0vjx45WSkqJOnTopPj5eCxYs0FdffaX77mPtHQAAgEmMCcC33367jh49qqefflp5eXmKjo7W22+/raioKE+3Brjx8/PTk08+WWkJDgA0VPxcQ33jsC70nAgAAADgEmLEGmAAAACgAgEYAAAARiEAAwAAwCgEYAAAABiFAAzUI/PmzVPr1q3VqFEjxcbG6oMPPvB0SwBQbe+//74GDBigiIgIORwOvf76655uCZBEAAbqjdWrVys1NVVTpkzRRx99pJtuukn9+vXTV1995enWAKBaTpw4oeuuu05z5szxdCuAGx6DBtQTcXFxuuGGGzR//nx7rH379ho0aJDS0tI82BkA/HwOh0Nr1qzRoEGDPN0KwB1goD4oKytTdna2evfu7Tbeu3dvbdmyxUNdAQBwaSIAA/XAt99+q/LycoWGhrqNh4aGKj8/30NdAQBwaSIAA/WIw+Fw27csq9IYAAD4eQjAQD0QHBwsLy+vSnd7CwoKKt0VBgAAPw8BGKgHfH19FRsbq6ysLLfxrKwsdenSxUNdAQBwafL2dAMAfjR+/HilpKSoU6dOio+P14IFC/TVV1/pvvvu83RrAFAtxcXF+vzzz+39/fv3KycnR4GBgWrZsqUHO4PpeAwaUI/MmzdP06dPV15enqKjozVr1izdfPPNnm4LAKpl48aN6t69e6Xx4cOHKz09ve4bAv4/BGAAAAAYhTXAAAAAMAoBGAAAAEYhAAMAAMAoBGAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAYKBu3bopNTXV020AgEcQgAGggRkwYIB69ux51mNbt26Vw+HQhx9+WMddAUDDQQAGgAZm5MiR2rBhg7788stKxxYvXqzrr79eN9xwgwc6A4CGgQAMAA1MYmKiQkJClJ6e7jb+/fffa/Xq1Ro0aJDuvPNOtWjRQk2aNFFMTIxefvnl887pcDj0+uuvu41dccUVbuf4+uuvdfvtt6tZs2YKCgrSwIEDdeDAgZq5KACoQwRgAGhgvL29NWzYMKWnp8uyLHv8lVdeUVlZme655x7FxsbqrbfeUm5urkaPHq2UlBRt37692uf8/vvv1b17d11++eV6//33tXnzZl1++eXq27evysrKauKyAKDOEIABoAG6++67deDAAW3cuNEeW7x4sQYPHqwrr7xSEydO1PXXX682bdpo7Nix6tOnj1555ZVqny8jI0OXXXaZXnrpJcXExKh9+/ZasmSJvvrqK7ceAKAh8PZ0AwCAi/eLX/xCXbp00eLFi9W9e3d98cUX+uCDD7Ru3TqVl5frmWee0erVq/X111+rtLRUpaWlatq0abXPl52drc8//1z+/v5u4z/88IO++OKLn3s5AFCnCMAA0ECNHDlSDz74oObOnaslS5YoKipKCQkJmjFjhmbNmqXZs2crJiZGTZs2VWpq6nmXKjgcDrflFJJ08uRJ+8+nT59WbGysVq5cWem1zZs3r7mLAoA6QAAGgAZqyJAheuihh7Rq1SotXbpUo0aNksPh0AcffKCBAwdq6NChkn4Mr5999pnat29/zrmaN2+uvLw8e/+zzz7T999/b+/fcMMNWr16tUJCQhQQEFB7FwUAdYA1wADQQF1++eW6/fbb9dhjj+mbb77RiBEjJElXX321srKytGXLFu3du1f33nuv8vPzzztXjx49NGfOHH344YfatWuX7rvvPvn4+NjH77rrLgUHB2vgwIH64IMPtH//fm3atEkPPfSQDh06VJuXCQA1jgAMAA3YyJEjVVhYqJ49e6ply5aSpD/+8Y+64YYb1KdPH3Xr1k1hYWEaNGjQeed57rnnFBkZqZtvvlnJycmaOHGimjRpYh9v0qSJ3n//fbVs2VKDBw9W+/btdffdd6ukpIQ7wgAaHId15qIvAAAA4BLGHWAAAAAYhQAMAAAAoxCAAQAAYBQCMAAAAIxCAAYAAIBRCMAAAAAwCgEYAAAARiEAAwAAwCgEYAAAABiFAAwAAACjEIABAABglP8HdEahuo+fwT0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AC w/ edits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'y' is your column and 'df' is your DataFrame\n",
    "total_rows = len(df)\n",
    "count_of_ones = y.sum()\n",
    "count_of_zeros = total_rows - count_of_ones\n",
    "\n",
    "# Preparing data for the bar chart\n",
    "data = {'Count': [count_of_zeros, count_of_ones], 'Value': ['0', '1']}\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Value', y='Count', data=data)\n",
    "plt.title('Count of 0s and 1s in the Column')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Metrics and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Instantiate Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary of the classifiers described in the introduction section of this file is created, instantiating instances of each of the classifier classes, utilizing the same hyperparameters as the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0BtxnikmJMqE"
   },
   "outputs": [],
   "source": [
    "# AC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "classifiers = {\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'MLP': MLPClassifier(random_state=42),\n",
    "    'XGB': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that the authors chose to use logloss as the evaluation metric for extreme gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Model Metrics - Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTE**\n",
    "\n",
    "The below blocks perform training/testing accuracy on non-cross validated data. It might not be worth running this / including it in results, its more for getting initial insights to accuracy before validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors used the below code to compare model metrics across the different machine learning classifiers for printing test only metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBsMa-7uinUG",
    "outputId": "395dd9f0-b34d-45ac-b325-c7caa9f07a56"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# AC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def f2_score(precision, recall):\n",
    "    return 5 * (precision * recall) / ((4 * precision) + recall)\n",
    "\n",
    "# Iterating through each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities and labels\n",
    "    y_scores_test = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred_binary = (y_scores_test > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    auc_test = roc_auc_score(y_test, y_scores_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_binary)\n",
    "    f1_test = f1_score(y_test, y_pred_binary)\n",
    "    precision_test = precision_score(y_test, y_pred_binary)\n",
    "    recall_test = recall_score(y_test, y_pred_binary)\n",
    "    f2_test = f2_score(precision_test, recall_test)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{name}: Test AUC = {auc_test:.4f}, Test Accuracy = {accuracy_test:.4f}, \"\n",
    "          f\"F1 Score = {f1_test:.4f}, Precision = {precision_test:.4f}, \"\n",
    "          f\"Recall = {recall_test:.4f}, F2 Score = {f2_test:.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function wrapper for Initial Test AUC\n",
    "\n",
    "I prompt engineer the following wrapper function. This allows not having to concisely pass different train/test data from the different models to get differing AUC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatgpt output using AC inside\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def f2_score(precision, recall):\n",
    "    return 5 * (precision * recall) / ((4 * precision) + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "def evaluate_classifiers_test(classifiers, X_train, y_train, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classifiers on given data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    classifiers : dict\n",
    "        Dictionary of {name: classifier} to evaluate.\n",
    "    X_train, y_train : array-like\n",
    "        Training features and labels.\n",
    "    X_test, y_test : array-like\n",
    "        Test features and labels.\n",
    "    threshold : float, optional (default=0.5)\n",
    "        Probability threshold to convert scores to binary predictions.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of metrics per classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        # Fit model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities and labels\n",
    "        y_scores_test = clf.predict_proba(X_test)[:, 1]\n",
    "        y_pred_binary = (y_scores_test > threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        auc_test = roc_auc_score(y_test, y_scores_test)\n",
    "        accuracy_test = accuracy_score(y_test, y_pred_binary)\n",
    "        f1_test = f1_score(y_test, y_pred_binary)\n",
    "        precision_test = precision_score(y_test, y_pred_binary)\n",
    "        recall_test = recall_score(y_test, y_pred_binary)\n",
    "        f2_test = f2_score(precision_test, recall_test)\n",
    "\n",
    "        # Save metrics\n",
    "        results[name] = {\n",
    "            \"AUC\": auc_test,\n",
    "            \"Accuracy\": accuracy_test,\n",
    "            \"F1\": f1_test,\n",
    "            \"Precision\": precision_test,\n",
    "            \"Recall\": recall_test,\n",
    "            \"F2\": f2_test\n",
    "        }\n",
    "\n",
    "        # Print results\n",
    "        print(f\"{name}: Test AUC = {auc_test:.4f}, Test Accuracy = {accuracy_test:.4f}, \"\n",
    "              f\"F1 = {f1_test:.4f}, Precision = {precision_test:.4f}, \"\n",
    "              f\"Recall = {recall_test:.4f}, F2 = {f2_test:.4f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Dataset Initial Test AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifiers_test(classifiers, X_train, y_train, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V Dataset Initial Test AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifiers_test(classifiers, X_train_w2v, y_train_w2v, X_test_w2v, y_test_w2v);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F2 score and Defining Function to accept training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author's then use a similar code chunk (below) to predict probabilities and labels on both train and test datasets. Since this one is more informative, I also create a wrapper function to accept the different models train/test data to be passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybRHw5DO8LBF",
    "outputId": "9e1ef9c6-61d0-4cd6-eb2c-0cd6a5e95d48"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# AC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def f2_score(precision, recall):\n",
    "    return 5 * (precision * recall) / ((4 * precision) + recall)\n",
    "\n",
    "# Iterating through each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit model on training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities and labels for both train and test datasets\n",
    "    y_scores_train = clf.predict_proba(X_train)[:, 1]\n",
    "    y_pred_binary_train = (y_scores_train > 0.5).astype(int)\n",
    "\n",
    "    y_scores_test = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred_binary_test = (y_scores_test > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics for train dataset\n",
    "    auc_train = roc_auc_score(y_train, y_scores_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_binary_train)\n",
    "    f1_train = f1_score(y_train, y_pred_binary_train)\n",
    "    precision_train = precision_score(y_train, y_pred_binary_train)\n",
    "    recall_train = recall_score(y_train, y_pred_binary_train)\n",
    "    sensitivity_train = recall_train  # Sensitivity is the same as Recall\n",
    "    f2_train = f2_score(precision_train, recall_train)\n",
    "\n",
    "    # Calculate metrics for test dataset\n",
    "    auc_test = roc_auc_score(y_test, y_scores_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_binary_test)\n",
    "    f1_test = f1_score(y_test, y_pred_binary_test)\n",
    "    precision_test = precision_score(y_test, y_pred_binary_test)\n",
    "    recall_test = recall_score(y_test, y_pred_binary_test)\n",
    "    sensitivity_test = recall_test  # Sensitivity is the same as Recall\n",
    "    f2_test = f2_score(precision_test, recall_test)\n",
    "\n",
    "    # Print results for both train and test datasets\n",
    "    print(f\"{name}: Train AUC = {auc_train:.4f}, Train Accuracy = {accuracy_train:.4f}, \"\n",
    "          f\"Train F1 Score = {f1_train:.4f}, Train Precision = {precision_train:.4f}, \"\n",
    "          f\"Train Recall = {recall_train:.4f}, Train Sensitivity = {sensitivity_train:.4f}, Train F2 Score = {f2_train:.4f}\")\n",
    "\n",
    "    print(f\"{name}: Test AUC = {auc_test:.4f}, Test Accuracy = {accuracy_test:.4f}, \"\n",
    "          f\"Test F1 Score = {f1_test:.4f}, Test Precision = {precision_test:.4f}, \"\n",
    "          f\"Test Recall = {recall_test:.4f}, Test Sensitivity = {sensitivity_test:.4f}, Test F2 Score = {f2_test:.4f}\")\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I wrap the author's  code in a wrapper function to accept multiple model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From chatgpt\n",
    "# Includes AC in function wrapper\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def f2_score(precision, recall):\n",
    "    \"\"\"Custom F2 score definition.\"\"\"\n",
    "    return 5 * (precision * recall) / ((4 * precision) + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "def evaluate_classifiers(classifiers, X_train, y_train, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fit and evaluate multiple classifiers with given train/test data.\n",
    "    \n",
    "    Parameters:\n",
    "        classifiers (dict): Dictionary of {name: model} pairs.\n",
    "        X_train, y_train: Training features and labels.\n",
    "        X_test, y_test: Test features and labels.\n",
    "        threshold (float): Decision threshold for binary classification (default=0.5).\n",
    "    \n",
    "    Returns:\n",
    "        results (dict): Dictionary with metrics per classifier.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        # Fit model on training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities and labels for both train and test datasets\n",
    "        y_scores_train = clf.predict_proba(X_train)[:, 1]\n",
    "        y_pred_binary_train = (y_scores_train > threshold).astype(int)\n",
    "\n",
    "        y_scores_test = clf.predict_proba(X_test)[:, 1]\n",
    "        y_pred_binary_test = (y_scores_test > threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics for train dataset\n",
    "        auc_train = roc_auc_score(y_train, y_scores_train)\n",
    "        accuracy_train = accuracy_score(y_train, y_pred_binary_train)\n",
    "        f1_train = f1_score(y_train, y_pred_binary_train)\n",
    "        precision_train = precision_score(y_train, y_pred_binary_train)\n",
    "        recall_train = recall_score(y_train, y_pred_binary_train)\n",
    "        sensitivity_train = recall_train  # Sensitivity is the same as Recall\n",
    "        f2_train = f2_score(precision_train, recall_train)\n",
    "\n",
    "        # Calculate metrics for test dataset\n",
    "        auc_test = roc_auc_score(y_test, y_scores_test)\n",
    "        accuracy_test = accuracy_score(y_test, y_pred_binary_test)\n",
    "        f1_test = f1_score(y_test, y_pred_binary_test)\n",
    "        precision_test = precision_score(y_test, y_pred_binary_test)\n",
    "        recall_test = recall_score(y_test, y_pred_binary_test)\n",
    "        sensitivity_test = recall_test  # Sensitivity is the same as Recall\n",
    "        f2_test = f2_score(precision_test, recall_test)\n",
    "\n",
    "        # Save results\n",
    "        results[name] = {\n",
    "            \"AUC_train\": auc_train,\n",
    "            \"AUC_test\": auc_test,\n",
    "            \"Accuracy_train\": accuracy_train,\n",
    "            \"Accuracy_test\": accuracy_test,\n",
    "            \"F1_train\": f1_train,\n",
    "            \"F1_test\": f1_test,\n",
    "            \"Precision_train\": precision_train,\n",
    "            \"Precision_test\": precision_test,\n",
    "            \"Recall_train\": sensitivity_train,\n",
    "            \"Recall_test\": sensitivity_test,\n",
    "            \"F2_train\": f2_train,\n",
    "            \"F2_test\": f2_test\n",
    "        }\n",
    "\n",
    "        # Print results for both train and test datasets\n",
    "        print(f\"{name}: Train AUC = {auc_train:.4f}, Train Accuracy = {accuracy_train:.4f}, \"\n",
    "              f\"Train F1 Score = {f1_train:.4f}, Train Precision = {precision_train:.4f}, \"\n",
    "              f\"Train Recall = {recall_train:.4f}, Train Sensitivity = {sensitivity_train:.4f}, Train F2 Score = {f2_train:.4f}/n/n\")\n",
    "    \n",
    "        print(f\"{name}: Test AUC = {auc_test:.4f}, Test Accuracy = {accuracy_test:.4f}, \"\n",
    "              f\"Test F1 Score = {f1_test:.4f}, Test Precision = {precision_test:.4f}, \"\n",
    "              f\"Test Recall = {recall_test:.4f}, Test Sensitivity = {sensitivity_test:.4f}, Test F2 Score = {f2_test:.4f}/n/n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Feature Space Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply the function `evaluate_classifiers()` to the original clean dataset trained on **non-smote** `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_metrics = evaluate_classifiers(classifiers=classifiers, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, threshold =0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Feature Space Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply the function `evaluate_classifiers()` to the word2vec dataset trained on **non-smote** `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_metrics = evaluate_classifiers(classifiers=classifiers, X_train=X_train_w2v, y_train=y_train_w2v, X_test=X_test_w2v, y_test=y_test_w2v, threshold =0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aNDRoqdhTdz"
   },
   "source": [
    "## Determining Model Metrics - SMOTE Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Feature Space SMOTE Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply the function `evaluate_classifiers()` to the original clean dataset trained on **SMOTE** `X_train_smote` and `y_train_smote`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_smote_metrics = evaluate_classifiers(classifiers=classifiers, X_train=X_train_smote, y_train=y_train_smote, X_test=X_test, y_test=y_test, threshold =0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Feature Space SMOTE Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply the function `evaluate_classifiers()` to the word2vec dataset trained on **SMOTE** `X_train_smote` and `y_train_smote`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_metrics = evaluate_classifiers(classifiers=classifiers, X_train=X_train_w2v, y_train=y_train_w2v, X_test=X_test_w2v, y_test=y_test_w2v, threshold =0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC and ROC Determination using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors chose to use AUC to test their smote dataset (which included resampled test data) they performed cross-validation incorrectly and it led to inflated results. I instead applied smote only to the training folds, and the code below is derived from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzaoYBlgI5fN",
    "outputId": "10b2a75e-9ae1-4797-8b55-367081ca0731"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# AC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "num_folds = 5  # Reduced number of folds for faster execution\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr')\n",
    "\n",
    "# Select five specific models\n",
    "selected_classifiers = {\n",
    "    'SVC': classifiers['SVC'],\n",
    "    'DecisionTree' : classifiers['DecisionTree'],\n",
    "    'RandomForest': classifiers['RandomForest'],\n",
    "    'GradientBoosting': classifiers['GradientBoosting'],\n",
    "    'XGB': classifiers['XGB'],\n",
    "    'MLP': classifiers['MLP'],\n",
    "    'LGBM': classifiers['LGBM']\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in selected_classifiers.items():\n",
    "    scores = cross_validate(clf, X_smote, y_smote, cv=kf, scoring={'auc': auc_scorer}, n_jobs=-1)\n",
    "    results[name] = {'AUC': np.mean(scores['test_auc'])}\n",
    "\n",
    "# Print results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: AUC = {metrics['AUC']:.4f}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeated_cv_with_mixed_search Function Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I was worried about reproducability in smote resampling across folds for cross validation, I chose to use RepeatedStratifiedKFold cross validation to ensure that the final assembled model was as robust as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, make_scorer\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    Run repeated stratified K-fold CV with SMOTE, using:\n",
    "      - GridSearchCV for lighter classifiers (SVC, DecisionTree, MLP)\n",
    "      - RandomizedSearchCV for heavier ensembles (RF, XGB, LGBM, GradientBoosting)\n",
    "\n",
    "    Return results + summary DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        X_train, y_train : Training data\n",
    "        X_test, y_test   : Holdout test set\n",
    "        classifiers      : dict {name: estimator}\n",
    "        param_spaces     : dict {name: grid or distribution}\n",
    "        n_splits         : folds per repeat\n",
    "        n_repeats        : number of repeats\n",
    "        scoring          : metric for optimization\n",
    "        n_jobs           : parallel jobs\n",
    "        random_state     : seed\n",
    "        verbose          : verbosity level\n",
    "        n_iter_random : default iterations if per-classifier dict not provided\n",
    "        n_iter_random_per_clf : dict {classifier_name: n_iter}, overrides default\n",
    "        save_prefix : str\n",
    "            Prefix for saved files: {save_prefix}_YYYYMMDD_HHMM_summary.csv and {save_prefix}_YYYYMMDD_HHMM_full.pkl\n",
    "\n",
    "    Returns:\n",
    "        results : dict {classifier_name: {\"best_estimator\": ..., \n",
    "                                          \"cv_results\": ..., \n",
    "                                          \"test_metrics\": ...}}\n",
    "\n",
    "        summary_df : pd.DataFrame\n",
    "            Tidy table of classifier performance with columns:\n",
    "                - Classifier\n",
    "                - Best Params\n",
    "                - CV Mean Score\n",
    "                - CV Std Score\n",
    "                - Holdout ROC-AUC\n",
    "                - Holdout Precision\n",
    "                - Holdout Recall\n",
    "                - Holdout F1\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def repeated_cv_with_mixed_search(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    classifiers, param_spaces,\n",
    "    X_train_smote=None, y_train_smote=None,  # SMOTE retraining\n",
    "    n_splits=5, n_repeats=10, scoring=\"roc_auc\",\n",
    "    n_jobs=-1, random_state=42, verbose=1,\n",
    "    n_iter_random=50, n_iter_random_per_clf=None,\n",
    "    save_prefix=\"Data/\",\n",
    "    descriptive_cv=True  # flag for optional descriptive CV on both original and SMOTE\n",
    "):\n",
    "    os.makedirs(save_prefix, exist_ok=True)\n",
    "    results = {}\n",
    "    summary_rows = []\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # classifiers that should use RandomizedSearchCV\n",
    "    randomized_models = {\"RandomForest\", \"XGB\", \"LGBM\", \"GradientBoosting\"}\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n🔹 Running {name}...\")\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),     # safe default\n",
    "            (\"smote\", SMOTE(random_state=random_state)),\n",
    "            (\"clf\", clf)\n",
    "        ])\n",
    "\n",
    "        param_space = param_spaces.get(name, {})\n",
    "        \n",
    "        # Choose search strategy\n",
    "        if name in randomized_models:\n",
    "            # Use per-classifier iterations if provided, else default\n",
    "            n_iter = (n_iter_random_per_clf.get(name) \n",
    "                      if n_iter_random_per_clf and name in n_iter_random_per_clf \n",
    "                      else n_iter_random)\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_distributions=param_space,\n",
    "                n_iter=n_iter,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                n_jobs=n_jobs,\n",
    "                refit=True,\n",
    "                verbose=verbose,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            search = GridSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_grid=param_space,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                n_jobs=n_jobs,\n",
    "                refit=True,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "        # Fit search\n",
    "        search.fit(X_train, y_train)\n",
    "\n",
    "        # Holdout evaluation\n",
    "        y_pred = search.predict(X_test)\n",
    "        y_proba = search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        test_auc = roc_auc_score(y_test, y_proba)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        # Initialize summary row\n",
    "        summary_row = {\n",
    "            \"Classifier\": name,\n",
    "            \"Best Params\": search.best_params_,\n",
    "            \"CV Mean Score\": search.best_score_,\n",
    "            \"CV Std Score\": search.cv_results_[\"std_test_score\"][search.best_index_],\n",
    "            \"Holdout ROC-AUC\": test_auc,\n",
    "            \"Holdout Precision\": report[\"1\"][\"precision\"],\n",
    "            \"Holdout Recall\": report[\"1\"][\"recall\"],\n",
    "            \"Holdout F1\": report[\"1\"][\"f1-score\"],\n",
    "            \"Descriptive CV Mean AUC\": None,\n",
    "            \"Descriptive CV Std AUC\": None,\n",
    "            \"Holdout ROC-AUC (SMOTE)\": None,\n",
    "            \"Descriptive CV Mean AUC (SMOTE)\": None,\n",
    "            \"Descriptive CV Std AUC (SMOTE)\": None,\n",
    "            \"Final Holdout ROC-AUC (SMOTE)\": None,\n",
    "        }\n",
    "\n",
    "        # --- Optional descriptive CV on original training set ---\n",
    "        if descriptive_cv:\n",
    "            print(f\"   Performing descriptive KFold CV on original training set for {name}...\")\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "            auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "            cv_scores = cross_val_score(\n",
    "                search.best_estimator_, X_train, y_train,\n",
    "                cv=kf, scoring=auc_scorer, n_jobs=n_jobs\n",
    "            )\n",
    "            summary_row[\"Descriptive CV Mean AUC\"] = cv_scores.mean()\n",
    "            summary_row[\"Descriptive CV Std AUC\"] = cv_scores.std()\n",
    "            print(f\"   Descriptive CV AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "        # Save detailed results\n",
    "        results[name] = {\n",
    "            \"best_estimator\": search.best_estimator_,\n",
    "            \"best_params\": search.best_params_,\n",
    "            \"cv_results\": search.cv_results_,\n",
    "            \"test_metrics\": {\n",
    "                \"roc_auc\": test_auc,\n",
    "                \"classification_report\": report\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add to summary\n",
    "        summary_rows.append(summary_row)\n",
    "        print(f\"✅ {name} done. Best params: {search.best_params_}\")\n",
    "        print(f\"   CV ROC-AUC: {search.best_score_:.3f} ± {search.cv_results_['std_test_score'][search.best_index_]:.3f}\")\n",
    "        print(f\"   Holdout ROC-AUC: {test_auc:.3f}\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(by=\"Holdout ROC-AUC\", ascending=False)\n",
    "    \n",
    "   # --- Step 6: Use already-fitted best_estimator_ from CV search to evaluate on holdout ---\n",
    "    print(\"\\n📌 Evaluating all classifiers on holdout test set using CV-trained pipelines:\")\n",
    "    for name in classifiers.keys():\n",
    "        best_pipe = results[name][\"best_estimator\"]  # already fitted during CV\n",
    "        y_pred_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"{name}: Holdout ROC-AUC = {auc:.4f}\")\n",
    "\n",
    "    # --- Step 7: Save best classifier based on holdout ROC-AUC ---\n",
    "    best_idx = summary_df['Holdout ROC-AUC'].idxmax()\n",
    "    best_model_name = summary_df.loc[best_idx, 'Classifier']\n",
    "    best_params = results[best_model_name]['best_params']\n",
    "    best_pipeline = clone(results[best_model_name]['best_estimator'])\n",
    "    best_pipeline.set_params(**best_params)\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"\\n🏆 Best classifier (no SMOTE) = {best_model_name}, Holdout ROC-AUC = {test_auc:.4f}\")\n",
    "\n",
    "    # Auto-save best model with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    best_model_path = f\"{save_prefix}_{timestamp}_best_model.pkl\"\n",
    "    with open(best_model_path, \"wb\") as f:\n",
    "        pickle.dump(best_pipeline, f)\n",
    "    print(f\"💾 Saved best model (no SMOTE) to {best_model_path}\")\n",
    "\n",
    "    # --- Step 8 & 9: Evaluate CV-trained classifiers on SMOTE data ---\n",
    "    if X_train_smote is not None and y_train_smote is not None:\n",
    "        print(\"\\n📌 Evaluating CV-trained classifiers on SMOTE-balanced training data:\")\n",
    "        smote_summary = []\n",
    "        for name in classifiers.keys():\n",
    "            # Use the pipeline already fitted during CV\n",
    "            best_pipe = results[name][\"best_estimator\"]\n",
    "            \n",
    "            # Optionally, retrain on full SMOTE set\n",
    "            best_pipe.fit(X_train_smote, y_train_smote)\n",
    "            \n",
    "            # Predict on holdout\n",
    "            y_pred_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            smote_summary.append((name, auc))\n",
    "            print(f\"{name}: Holdout ROC-AUC on SMOTE training = {auc:.4f}\")\n",
    "    \n",
    "            # Optional descriptive CV on SMOTE\n",
    "            if descriptive_cv:\n",
    "                print(f\"   Performing descriptive KFold CV on SMOTE data for {name}...\")\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "                auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "                cv_scores = cross_val_score(\n",
    "                    best_pipe, X_train_smote, y_train_smote,\n",
    "                    cv=kf, scoring=auc_scorer, n_jobs=n_jobs\n",
    "                )\n",
    "                print(f\"   Descriptive CV AUC (SMOTE): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "                # Update summary_df with SMOTE metrics\n",
    "                summary_df.loc[summary_df['Classifier'] == name, 'Holdout ROC-AUC (SMOTE)'] = auc\n",
    "                summary_df.loc[summary_df['Classifier'] == name, 'Descriptive CV Mean AUC (SMOTE)'] = cv_scores.mean()\n",
    "                summary_df.loc[summary_df['Classifier'] == name, 'Descriptive CV Std AUC (SMOTE)'] = cv_scores.std()\n",
    "\n",
    "        # Pick best SMOTE classifier based on holdout AUC\n",
    "        best_smote_name, best_smote_auc = max(smote_summary, key=lambda x: x[1])\n",
    "        print(f\"🏆 Best SMOTE-trained classifier = {best_smote_name}, Holdout ROC-AUC = {best_smote_auc:.4f}\")\n",
    "        \n",
    "        # Final evaluation of best SMOTE-trained model\n",
    "        final_smote_model = results[best_smote_name][\"best_estimator\"]\n",
    "        final_smote_model.fit(X_train_smote, y_train_smote)\n",
    "        y_test_proba = final_smote_model.predict_proba(X_test)[:, 1]\n",
    "        final_smote_auc = roc_auc_score(y_test, y_test_proba)\n",
    "        print(f\"🎯 Final evaluation of best SMOTE-trained classifier = {best_smote_name}, ROC-AUC = {final_smote_auc:.4f}\")\n",
    "    \n",
    "        # Update summary_df\n",
    "        summary_df.loc[summary_df['Classifier'] == best_smote_name, 'Final Holdout ROC-AUC (SMOTE)'] = final_smote_auc\n",
    "    \n",
    "        # Save final best SMOTE model\n",
    "        smote_model_path = f\"{save_prefix}_{timestamp}_best_smote_model.pkl\"\n",
    "        with open(smote_model_path, \"wb\") as f:\n",
    "            pickle.dump(final_smote_model, f)\n",
    "        print(f\"💾 Saved best SMOTE-trained model to {smote_model_path}\")\n",
    "\n",
    "        # --- Save full summary_df with both original and SMOTE descriptive CV metrics ---\n",
    "        full_summary_csv_path = f\"{save_prefix}_{timestamp}_full_summary_with_smote.csv\"\n",
    "        summary_df.to_csv(full_summary_csv_path, index=False)\n",
    "        print(f\"💾 Saved full summary including original and SMOTE metrics to {full_summary_csv_path}\")\n",
    "\n",
    "        # Save full results dict (all classifiers, CV results, params, metrics)\n",
    "        full_results_path = f\"{save_prefix}_{timestamp}_full.pkl\"\n",
    "        with open(full_results_path, \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"💾 Saved full results dict to {full_results_path}\")\n",
    "\n",
    "    return results, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test repeated_cv_with_mixed_search\n",
    "Using simple parameter spaces for the 6 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Running SVC...\n",
      "Fitting 50 folds for each of 6 candidates, totalling 300 fits\n",
      "   Performing descriptive KFold CV on original training set for SVC...\n",
      "   Descriptive CV AUC: 0.6944 ± 0.0169\n",
      "✅ SVC done. Best params: {'clf__C': 0.1, 'clf__kernel': 'linear'}\n",
      "   CV ROC-AUC: 0.696 ± 0.018\n",
      "   Holdout ROC-AUC: 0.728\n",
      "\n",
      "🔹 Running DecisionTree...\n",
      "Fitting 50 folds for each of 3 candidates, totalling 150 fits\n",
      "   Performing descriptive KFold CV on original training set for DecisionTree...\n",
      "   Descriptive CV AUC: 0.6025 ± 0.0273\n",
      "✅ DecisionTree done. Best params: {'clf__max_depth': 5}\n",
      "   CV ROC-AUC: 0.602 ± 0.027\n",
      "   Holdout ROC-AUC: 0.555\n",
      "\n",
      "🔹 Running RandomForest...\n",
      "Fitting 50 folds for each of 6 candidates, totalling 300 fits\n",
      "   Performing descriptive KFold CV on original training set for RandomForest...\n",
      "   Descriptive CV AUC: 0.7050 ± 0.0126\n",
      "✅ RandomForest done. Best params: {'clf__n_estimators': 200, 'clf__max_depth': None}\n",
      "   CV ROC-AUC: 0.704 ± 0.015\n",
      "   Holdout ROC-AUC: 0.704\n",
      "\n",
      "🔹 Running GradientBoosting...\n",
      "Fitting 50 folds for each of 4 candidates, totalling 200 fits\n",
      "   Performing descriptive KFold CV on original training set for GradientBoosting...\n",
      "   Descriptive CV AUC: 0.7005 ± 0.0164\n",
      "✅ GradientBoosting done. Best params: {'clf__n_estimators': 100, 'clf__learning_rate': 0.1}\n",
      "   CV ROC-AUC: 0.701 ± 0.019\n",
      "   Holdout ROC-AUC: 0.727\n",
      "\n",
      "🔹 Running MLP...\n",
      "Fitting 50 folds for each of 4 candidates, totalling 200 fits\n",
      "   Performing descriptive KFold CV on original training set for MLP...\n",
      "   Descriptive CV AUC: 0.6393 ± 0.0083\n",
      "✅ MLP done. Best params: {'clf__alpha': 0.001, 'clf__hidden_layer_sizes': (50,)}\n",
      "   CV ROC-AUC: 0.634 ± 0.020\n",
      "   Holdout ROC-AUC: 0.613\n",
      "\n",
      "🔹 Running XGB...\n",
      "Fitting 50 folds for each of 4 candidates, totalling 200 fits\n",
      "   Performing descriptive KFold CV on original training set for XGB...\n",
      "   Descriptive CV AUC: 0.7035 ± 0.0153\n",
      "✅ XGB done. Best params: {'clf__max_depth': 3, 'clf__learning_rate': 0.1}\n",
      "   CV ROC-AUC: 0.705 ± 0.018\n",
      "   Holdout ROC-AUC: 0.717\n",
      "\n",
      "🔹 Running LGBM...\n",
      "Fitting 50 folds for each of 4 candidates, totalling 200 fits\n",
      "   Performing descriptive KFold CV on original training set for LGBM...\n",
      "   Descriptive CV AUC: 0.6960 ± 0.0153\n",
      "✅ LGBM done. Best params: {'clf__max_depth': 10, 'clf__learning_rate': 0.1}\n",
      "   CV ROC-AUC: 0.695 ± 0.015\n",
      "   Holdout ROC-AUC: 0.685\n",
      "\n",
      "📌 Evaluating all classifiers on holdout test set using CV-trained pipelines:\n",
      "SVC: Holdout ROC-AUC = 0.7277\n",
      "DecisionTree: Holdout ROC-AUC = 0.5551\n",
      "RandomForest: Holdout ROC-AUC = 0.7039\n",
      "GradientBoosting: Holdout ROC-AUC = 0.7269\n",
      "MLP: Holdout ROC-AUC = 0.6131\n",
      "XGB: Holdout ROC-AUC = 0.7173\n",
      "LGBM: Holdout ROC-AUC = 0.6849\n",
      "\n",
      "🏆 Best classifier (no SMOTE) = SVC, Holdout ROC-AUC = 0.7277\n",
      "💾 Saved best model (no SMOTE) to Results/Old/Models/_20250917_0028_best_model.pkl\n",
      "\n",
      "📌 Evaluating CV-trained classifiers on SMOTE-balanced training data:\n",
      "SVC: Holdout ROC-AUC on SMOTE training = 0.6681\n",
      "   Performing descriptive KFold CV on SMOTE data for SVC...\n",
      "   Descriptive CV AUC (SMOTE): 0.8584 ± 0.0079\n",
      "DecisionTree: Holdout ROC-AUC on SMOTE training = 0.5665\n",
      "   Performing descriptive KFold CV on SMOTE data for DecisionTree...\n",
      "   Descriptive CV AUC (SMOTE): 0.7642 ± 0.0158\n",
      "RandomForest: Holdout ROC-AUC on SMOTE training = 0.6908\n",
      "   Performing descriptive KFold CV on SMOTE data for RandomForest...\n",
      "   Descriptive CV AUC (SMOTE): 0.9413 ± 0.0027\n",
      "GradientBoosting: Holdout ROC-AUC on SMOTE training = 0.6901\n",
      "   Performing descriptive KFold CV on SMOTE data for GradientBoosting...\n",
      "   Descriptive CV AUC (SMOTE): 0.8910 ± 0.0057\n",
      "MLP: Holdout ROC-AUC on SMOTE training = 0.6361\n",
      "   Performing descriptive KFold CV on SMOTE data for MLP...\n",
      "   Descriptive CV AUC (SMOTE): 0.8756 ± 0.0046\n",
      "XGB: Holdout ROC-AUC on SMOTE training = 0.6915\n",
      "   Performing descriptive KFold CV on SMOTE data for XGB...\n",
      "   Descriptive CV AUC (SMOTE): 0.8895 ± 0.0066\n",
      "LGBM: Holdout ROC-AUC on SMOTE training = 0.6845\n",
      "   Performing descriptive KFold CV on SMOTE data for LGBM...\n",
      "   Descriptive CV AUC (SMOTE): 0.9139 ± 0.0049\n",
      "🏆 Best SMOTE-trained classifier = XGB, Holdout ROC-AUC = 0.6915\n",
      "🎯 Final evaluation of best SMOTE-trained classifier = XGB, ROC-AUC = 0.6915\n",
      "💾 Saved best SMOTE-trained model to Results/Old/Models/_20250917_0028_best_smote_model.pkl\n",
      "💾 Saved full summary including original and SMOTE metrics to Results/Old/Models/_20250917_0028_full_summary_with_smote.csv\n",
      "💾 Saved full results dict to Results/Old/Models/_20250917_0028_full.pkl\n",
      "\n",
      "🔎 Summary of model performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>CV Mean Score</th>\n",
       "      <th>CV Std Score</th>\n",
       "      <th>Holdout ROC-AUC</th>\n",
       "      <th>Holdout Precision</th>\n",
       "      <th>Holdout Recall</th>\n",
       "      <th>Holdout F1</th>\n",
       "      <th>Descriptive CV Mean AUC</th>\n",
       "      <th>Descriptive CV Std AUC</th>\n",
       "      <th>Holdout ROC-AUC (SMOTE)</th>\n",
       "      <th>Descriptive CV Mean AUC (SMOTE)</th>\n",
       "      <th>Descriptive CV Std AUC (SMOTE)</th>\n",
       "      <th>Final Holdout ROC-AUC (SMOTE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__kernel': 'linear'}</td>\n",
       "      <td>0.696191</td>\n",
       "      <td>0.017987</td>\n",
       "      <td>0.727715</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.709251</td>\n",
       "      <td>0.472141</td>\n",
       "      <td>0.694439</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.668144</td>\n",
       "      <td>0.858366</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'clf__n_estimators': 100, 'clf__learning_rate...</td>\n",
       "      <td>0.701349</td>\n",
       "      <td>0.018787</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.425150</td>\n",
       "      <td>0.312775</td>\n",
       "      <td>0.360406</td>\n",
       "      <td>0.700461</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.690057</td>\n",
       "      <td>0.891013</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGB</td>\n",
       "      <td>{'clf__max_depth': 3, 'clf__learning_rate': 0.1}</td>\n",
       "      <td>0.705070</td>\n",
       "      <td>0.018189</td>\n",
       "      <td>0.717283</td>\n",
       "      <td>0.394872</td>\n",
       "      <td>0.339207</td>\n",
       "      <td>0.364929</td>\n",
       "      <td>0.703483</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>0.691479</td>\n",
       "      <td>0.889506</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.691479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'clf__n_estimators': 200, 'clf__max_depth': N...</td>\n",
       "      <td>0.703861</td>\n",
       "      <td>0.015386</td>\n",
       "      <td>0.703865</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.299559</td>\n",
       "      <td>0.340852</td>\n",
       "      <td>0.704972</td>\n",
       "      <td>0.012639</td>\n",
       "      <td>0.690754</td>\n",
       "      <td>0.941327</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>{'clf__max_depth': 10, 'clf__learning_rate': 0.1}</td>\n",
       "      <td>0.695423</td>\n",
       "      <td>0.015303</td>\n",
       "      <td>0.684917</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.246696</td>\n",
       "      <td>0.308540</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.684463</td>\n",
       "      <td>0.913946</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{'clf__alpha': 0.001, 'clf__hidden_layer_sizes...</td>\n",
       "      <td>0.634133</td>\n",
       "      <td>0.019660</td>\n",
       "      <td>0.613108</td>\n",
       "      <td>0.324042</td>\n",
       "      <td>0.409692</td>\n",
       "      <td>0.361868</td>\n",
       "      <td>0.639290</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.636064</td>\n",
       "      <td>0.875584</td>\n",
       "      <td>0.00461</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>{'clf__max_depth': 5}</td>\n",
       "      <td>0.602295</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.555053</td>\n",
       "      <td>0.248210</td>\n",
       "      <td>0.458150</td>\n",
       "      <td>0.321981</td>\n",
       "      <td>0.602529</td>\n",
       "      <td>0.027271</td>\n",
       "      <td>0.566501</td>\n",
       "      <td>0.764179</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier                                        Best Params  \\\n",
       "0               SVC           {'clf__C': 0.1, 'clf__kernel': 'linear'}   \n",
       "3  GradientBoosting  {'clf__n_estimators': 100, 'clf__learning_rate...   \n",
       "5               XGB   {'clf__max_depth': 3, 'clf__learning_rate': 0.1}   \n",
       "2      RandomForest  {'clf__n_estimators': 200, 'clf__max_depth': N...   \n",
       "6              LGBM  {'clf__max_depth': 10, 'clf__learning_rate': 0.1}   \n",
       "4               MLP  {'clf__alpha': 0.001, 'clf__hidden_layer_sizes...   \n",
       "1      DecisionTree                              {'clf__max_depth': 5}   \n",
       "\n",
       "   CV Mean Score  CV Std Score  Holdout ROC-AUC  Holdout Precision  \\\n",
       "0       0.696191      0.017987         0.727715           0.353846   \n",
       "3       0.701349      0.018787         0.726948           0.425150   \n",
       "5       0.705070      0.018189         0.717283           0.394872   \n",
       "2       0.703861      0.015386         0.703865           0.395349   \n",
       "6       0.695423      0.015303         0.684917           0.411765   \n",
       "4       0.634133      0.019660         0.613108           0.324042   \n",
       "1       0.602295      0.026949         0.555053           0.248210   \n",
       "\n",
       "   Holdout Recall  Holdout F1  Descriptive CV Mean AUC  \\\n",
       "0        0.709251    0.472141                 0.694439   \n",
       "3        0.312775    0.360406                 0.700461   \n",
       "5        0.339207    0.364929                 0.703483   \n",
       "2        0.299559    0.340852                 0.704972   \n",
       "6        0.246696    0.308540                 0.696025   \n",
       "4        0.409692    0.361868                 0.639290   \n",
       "1        0.458150    0.321981                 0.602529   \n",
       "\n",
       "   Descriptive CV Std AUC Holdout ROC-AUC (SMOTE)  \\\n",
       "0                0.016931                0.668144   \n",
       "3                0.016418                0.690057   \n",
       "5                0.015263                0.691479   \n",
       "2                0.012639                0.690754   \n",
       "6                0.015298                0.684463   \n",
       "4                0.008252                0.636064   \n",
       "1                0.027271                0.566501   \n",
       "\n",
       "  Descriptive CV Mean AUC (SMOTE) Descriptive CV Std AUC (SMOTE)  \\\n",
       "0                        0.858366                       0.007948   \n",
       "3                        0.891013                       0.005656   \n",
       "5                        0.889506                       0.006574   \n",
       "2                        0.941327                       0.002734   \n",
       "6                        0.913946                       0.004856   \n",
       "4                        0.875584                        0.00461   \n",
       "1                        0.764179                       0.015779   \n",
       "\n",
       "  Final Holdout ROC-AUC (SMOTE)  \n",
       "0                          None  \n",
       "3                          None  \n",
       "5                      0.691479  \n",
       "2                          None  \n",
       "6                          None  \n",
       "4                          None  \n",
       "1                          None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Param spaces per classifier\n",
    "param_spaces = {\n",
    "    \"SVC\": {\"clf__C\": [0.1, 1, 10], \"clf__kernel\": [\"linear\", \"rbf\"]},\n",
    "    \"DecisionTree\": {\"clf__max_depth\": [3, 5, 10]},\n",
    "    \"RandomForest\": {\"clf__max_depth\": [5, 10, None], \"clf__n_estimators\": [100, 200]},\n",
    "    \"GradientBoosting\": {\"clf__learning_rate\": [0.01, 0.1], \"clf__n_estimators\": [100, 200]},\n",
    "    \"MLP\": {\"clf__hidden_layer_sizes\": [(50,), (100,)], \"clf__alpha\": [0.0001, 0.001]},\n",
    "    \"XGB\": {\"clf__max_depth\": [3, 5], \"clf__learning_rate\": [0.01, 0.1]},\n",
    "    \"LGBM\": {\"clf__max_depth\": [-1, 10], \"clf__learning_rate\": [0.01, 0.1]}\n",
    "}\n",
    "\n",
    "n_iter_random_per_clf = {\n",
    "    \"RandomForest\": 50,\n",
    "    \"GradientBoosting\": 30,\n",
    "    \"XGB\": 50,\n",
    "    \"LGBM\": 50\n",
    "}\n",
    "\n",
    "\n",
    "# Runs for each classifier\n",
    "results_orig, summary_orig = repeated_cv_with_mixed_search(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    classifiers=classifiers,\n",
    "    param_spaces=param_spaces, # pass grids for small models, distributions for big models\n",
    "    X_train_smote=X_train_smote, y_train_smote=y_train_smote,\n",
    "    n_splits=5,\n",
    "    n_repeats=10,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_iter_random=20,  # default for others\n",
    "    n_iter_random_per_clf=n_iter_random_per_clf,\n",
    "    descriptive_cv=True,\n",
    "    save_prefix=\"Results/Old/Models/\"\n",
    ")\n",
    "\n",
    "print(\"\\n🔎 Summary of model performance:\")\n",
    "display(summary_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Author's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From chatgpt\n",
    "# Includes AC in function wrapper\n",
    "\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_classifiers_cv(classifiers, X, y, num_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate multiple classifiers using cross-validation and AUC score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifiers : dict\n",
    "        Dictionary of sklearn classifiers { 'name': model_object }.\n",
    "    X : array-like\n",
    "        Feature matrix (e.g., X_smote).\n",
    "    y : array-like\n",
    "        Target labels (e.g., y_smote).\n",
    "    num_folds : int, default=5\n",
    "        Number of folds for K-Fold CV.\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Dictionary with mean AUC scores for each classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define CV splitter\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Define scorer\n",
    "    auc_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr')\n",
    "\n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        scores = cross_validate(\n",
    "            clf, X, y,\n",
    "            cv=kf,\n",
    "            scoring={'auc': auc_scorer},\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        results[name] = {'AUC': np.mean(scores['test_auc'])}\n",
    "\n",
    "    # Print results\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"{name}: AUC = {metrics['AUC']:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: AUC = 0.6691\n",
      "DecisionTree: AUC = 0.5582\n",
      "RandomForest: AUC = 0.7074\n",
      "GradientBoosting: AUC = 0.7076\n",
      "MLP: AUC = 0.6789\n",
      "XGB: AUC = 0.6771\n",
      "LGBM: AUC = 0.6927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SVC': {'AUC': 0.6690687748912584},\n",
       " 'DecisionTree': {'AUC': 0.5582295773389933},\n",
       " 'RandomForest': {'AUC': 0.7074267346966213},\n",
       " 'GradientBoosting': {'AUC': 0.7076009787679167},\n",
       " 'MLP': {'AUC': 0.6789245772442337},\n",
       " 'XGB': {'AUC': 0.6771418733720084},\n",
       " 'LGBM': {'AUC': 0.6927323177203915}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifiers_cv(classifiers, X_train, y_train, num_folds=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: AUC = 0.7022\n",
      "DecisionTree: AUC = 0.7478\n",
      "RandomForest: AUC = 0.9381\n",
      "GradientBoosting: AUC = 0.8918\n",
      "MLP: AUC = 0.8551\n",
      "XGB: AUC = 0.9184\n",
      "LGBM: AUC = 0.9128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SVC': {'AUC': 0.7021562202841526},\n",
       " 'DecisionTree': {'AUC': 0.7477705458889629},\n",
       " 'RandomForest': {'AUC': 0.9380976270669448},\n",
       " 'GradientBoosting': {'AUC': 0.8918349896356487},\n",
       " 'MLP': {'AUC': 0.8551307406467836},\n",
       " 'XGB': {'AUC': 0.9184332090527226},\n",
       " 'LGBM': {'AUC': 0.9127650711364745}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifiers_cv(classifiers, X_train_smote, y_train_smote, num_folds=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Knobs on Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the RF lost 20% compared to the author's original reported AUCROC score, I wanted to improve the parameter search for it, and for all other classifiers, and the gridsearch spaces are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_spaces = {\n",
    "    # Random Forest (big search space → RandomizedSearchCV)\n",
    "    \"RandomForest\": {\n",
    "        \"clf__n_estimators\": randint(200, 1001),       # sample 200–1000 trees\n",
    "        \"clf__max_depth\": [None, 10, 20, 50],\n",
    "        \"clf__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        \"clf__min_samples_split\": randint(2, 11),      # 2–10\n",
    "        \"clf__min_samples_leaf\": randint(1, 5),        # 1–4\n",
    "        \"clf__bootstrap\": [True, False],\n",
    "        \"clf__class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "\n",
    "    # Gradient Boosting (sklearn) (→ RandomizedSearchCV)\n",
    "    \"GradientBoosting\": {\n",
    "        \"clf__n_estimators\": randint(100, 501),        # 100–500\n",
    "        \"clf__learning_rate\": uniform(0.01, 0.3),      # 0.01–0.31\n",
    "        \"clf__max_depth\": [3, 5, 10],\n",
    "        \"clf__min_samples_split\": randint(2, 11),\n",
    "        \"clf__min_samples_leaf\": randint(1, 5),\n",
    "        \"clf__subsample\": uniform(0.7, 0.3),           # 0.7–1.0\n",
    "        \"clf__max_features\": [\"sqrt\", \"log2\", None]\n",
    "    },\n",
    "\n",
    "    # XGBoost (→ RandomizedSearchCV)\n",
    "    \"XGB\": {\n",
    "        \"clf__n_estimators\": randint(200, 1001),\n",
    "        \"clf__learning_rate\": uniform(0.01, 0.3),\n",
    "        \"clf__max_depth\": randint(3, 11),\n",
    "        \"clf__subsample\": uniform(0.7, 0.3),           # 0.7–1.0\n",
    "        \"clf__colsample_bytree\": uniform(0.7, 0.3),    # 0.7–1.0\n",
    "        \"clf__gamma\": uniform(0, 0.5),\n",
    "        \"clf__min_child_weight\": randint(1, 7)\n",
    "    },\n",
    "\n",
    "    # LightGBM (→ RandomizedSearchCV)\n",
    "    \"LGBM\": {\n",
    "        \"clf__n_estimators\": randint(200, 1001),\n",
    "        \"clf__learning_rate\": uniform(0.01, 0.3),\n",
    "        \"clf__max_depth\": [-1, 10, 20, 50],\n",
    "        \"clf__num_leaves\": randint(31, 256),\n",
    "        \"clf__subsample\": uniform(0.7, 0.3),\n",
    "        \"clf__colsample_bytree\": uniform(0.7, 0.3),\n",
    "        \"clf__min_child_samples\": randint(10, 101)\n",
    "    },\n",
    "\n",
    "    # SVC (→ GridSearchCV, grid is fine since small)\n",
    "    \"SVC\": {\n",
    "        \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "        \"clf__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        \"clf__gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "\n",
    "    # MLP (→ GridSearchCV, modest grid)\n",
    "    \"MLP\": {\n",
    "        \"clf__hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "        \"clf__alpha\": [0.0001, 0.001, 0.01],\n",
    "        \"clf__learning_rate_init\": [0.001, 0.01],\n",
    "        \"clf__solver\": [\"adam\", \"lbfgs\"]\n",
    "    },\n",
    "\n",
    "    # Decision Tree (→ GridSearchCV)\n",
    "    \"DecisionTree\": {\n",
    "        \"clf__max_depth\": [None, 5, 10, 20, 50],\n",
    "        \"clf__min_samples_split\": [2, 5, 10, 20],\n",
    "        \"clf__min_samples_leaf\": [1, 2, 4, 10],\n",
    "        \"clf__criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "n_iter_random_per_clf = {\n",
    "    \"RandomForest\": 50,\n",
    "    \"GradientBoosting\": 30,\n",
    "    \"XGB\": 50,\n",
    "    \"LGBM\": 50\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Repeated_CV on Original Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Running SVC...\n",
      "Fitting 50 folds for each of 24 candidates, totalling 1200 fits\n",
      "   Performing descriptive KFold CV on original training set for SVC...\n",
      "   Descriptive CV AUC: 0.6960 ± 0.0183\n",
      "✅ SVC done. Best params: {'clf__C': 0.01, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
      "   CV ROC-AUC: 0.698 ± 0.018\n",
      "   Holdout ROC-AUC: 0.726\n",
      "\n",
      "🔹 Running DecisionTree...\n",
      "Fitting 50 folds for each of 240 candidates, totalling 12000 fits\n",
      "   Performing descriptive KFold CV on original training set for DecisionTree...\n",
      "   Descriptive CV AUC: 0.6187 ± 0.0156\n",
      "✅ DecisionTree done. Best params: {'clf__criterion': 'entropy', 'clf__max_depth': 10, 'clf__min_samples_leaf': 10, 'clf__min_samples_split': 2}\n",
      "   CV ROC-AUC: 0.607 ± 0.021\n",
      "   Holdout ROC-AUC: 0.615\n",
      "\n",
      "🔹 Running RandomForest...\n",
      "Fitting 50 folds for each of 50 candidates, totalling 2500 fits\n",
      "   Performing descriptive KFold CV on original training set for RandomForest...\n",
      "   Descriptive CV AUC: 0.7093 ± 0.0137\n",
      "✅ RandomForest done. Best params: {'clf__bootstrap': True, 'clf__class_weight': None, 'clf__max_depth': 20, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 8, 'clf__n_estimators': 898}\n",
      "   CV ROC-AUC: 0.710 ± 0.017\n",
      "   Holdout ROC-AUC: 0.714\n",
      "\n",
      "🔹 Running GradientBoosting...\n",
      "Fitting 50 folds for each of 30 candidates, totalling 1500 fits\n",
      "   Performing descriptive KFold CV on original training set for GradientBoosting...\n",
      "   Descriptive CV AUC: 0.6988 ± 0.0157\n",
      "✅ GradientBoosting done. Best params: {'clf__learning_rate': 0.019393987736667576, 'clf__max_depth': 5, 'clf__max_features': 'log2', 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 3, 'clf__n_estimators': 301, 'clf__subsample': 0.9684482051282945}\n",
      "   CV ROC-AUC: 0.702 ± 0.017\n",
      "   Holdout ROC-AUC: 0.716\n",
      "\n",
      "🔹 Running MLP...\n",
      "Fitting 50 folds for each of 36 candidates, totalling 1800 fits\n",
      "   Performing descriptive KFold CV on original training set for MLP...\n",
      "   Descriptive CV AUC: 0.6381 ± 0.0081\n",
      "✅ MLP done. Best params: {'clf__alpha': 0.01, 'clf__hidden_layer_sizes': (50,), 'clf__learning_rate_init': 0.001, 'clf__solver': 'adam'}\n",
      "   CV ROC-AUC: 0.635 ± 0.020\n",
      "   Holdout ROC-AUC: 0.620\n",
      "\n",
      "🔹 Running XGB...\n",
      "Fitting 50 folds for each of 50 candidates, totalling 2500 fits\n",
      "   Performing descriptive KFold CV on original training set for XGB...\n",
      "   Descriptive CV AUC: 0.7094 ± 0.0151\n",
      "✅ XGB done. Best params: {'clf__colsample_bytree': 0.7076257380232285, 'clf__gamma': 0.053945713496652226, 'clf__learning_rate': 0.019428755706020276, 'clf__max_depth': 9, 'clf__min_child_weight': 1, 'clf__n_estimators': 763, 'clf__subsample': 0.8689826715929151}\n",
      "   CV ROC-AUC: 0.708 ± 0.016\n",
      "   Holdout ROC-AUC: 0.707\n",
      "\n",
      "🔹 Running LGBM...\n",
      "Fitting 50 folds for each of 50 candidates, totalling 2500 fits\n",
      "   Performing descriptive KFold CV on original training set for LGBM...\n",
      "   Descriptive CV AUC: 0.7047 ± 0.0109\n",
      "✅ LGBM done. Best params: {'clf__colsample_bytree': 0.7880464524154114, 'clf__learning_rate': 0.014223946814525337, 'clf__max_depth': 20, 'clf__min_child_samples': 90, 'clf__n_estimators': 591, 'clf__num_leaves': 193, 'clf__subsample': 0.9370526621593617}\n",
      "   CV ROC-AUC: 0.705 ± 0.016\n",
      "   Holdout ROC-AUC: 0.714\n",
      "\n",
      "📌 Evaluating all classifiers on holdout test set using CV-trained pipelines:\n",
      "SVC: Holdout ROC-AUC = 0.7257\n",
      "DecisionTree: Holdout ROC-AUC = 0.6153\n",
      "RandomForest: Holdout ROC-AUC = 0.7142\n",
      "GradientBoosting: Holdout ROC-AUC = 0.7158\n",
      "MLP: Holdout ROC-AUC = 0.6200\n",
      "XGB: Holdout ROC-AUC = 0.7069\n",
      "LGBM: Holdout ROC-AUC = 0.7140\n",
      "\n",
      "🏆 Best classifier (no SMOTE) = SVC, Holdout ROC-AUC = 0.7257\n",
      "💾 Saved best model (no SMOTE) to Results/Old/Models/original/_20250917_0236_best_model.pkl\n",
      "\n",
      "📌 Evaluating CV-trained classifiers on SMOTE-balanced training data:\n",
      "SVC: Holdout ROC-AUC on SMOTE training = 0.6728\n",
      "   Performing descriptive KFold CV on SMOTE data for SVC...\n",
      "   Descriptive CV AUC (SMOTE): 0.8585 ± 0.0087\n",
      "DecisionTree: Holdout ROC-AUC on SMOTE training = 0.5745\n",
      "   Performing descriptive KFold CV on SMOTE data for DecisionTree...\n",
      "   Descriptive CV AUC (SMOTE): 0.8006 ± 0.0101\n",
      "RandomForest: Holdout ROC-AUC on SMOTE training = 0.6993\n",
      "   Performing descriptive KFold CV on SMOTE data for RandomForest...\n",
      "   Descriptive CV AUC (SMOTE): 0.9263 ± 0.0037\n",
      "GradientBoosting: Holdout ROC-AUC on SMOTE training = 0.7127\n",
      "   Performing descriptive KFold CV on SMOTE data for GradientBoosting...\n",
      "   Descriptive CV AUC (SMOTE): 0.8993 ± 0.0047\n",
      "MLP: Holdout ROC-AUC on SMOTE training = 0.6258\n",
      "   Performing descriptive KFold CV on SMOTE data for MLP...\n",
      "   Descriptive CV AUC (SMOTE): 0.8756 ± 0.0042\n",
      "XGB: Holdout ROC-AUC on SMOTE training = 0.6933\n",
      "   Performing descriptive KFold CV on SMOTE data for XGB...\n",
      "   Descriptive CV AUC (SMOTE): 0.9348 ± 0.0042\n",
      "LGBM: Holdout ROC-AUC on SMOTE training = 0.6932\n",
      "   Performing descriptive KFold CV on SMOTE data for LGBM...\n",
      "   Descriptive CV AUC (SMOTE): 0.9185 ± 0.0049\n",
      "🏆 Best SMOTE-trained classifier = GradientBoosting, Holdout ROC-AUC = 0.7127\n",
      "🎯 Final evaluation of best SMOTE-trained classifier = GradientBoosting, ROC-AUC = 0.7127\n",
      "💾 Saved best SMOTE-trained model to Results/Old/Models/original/_20250917_0236_best_smote_model.pkl\n",
      "💾 Saved full summary including original and SMOTE metrics to Results/Old/Models/original/_20250917_0236_full_summary_with_smote.csv\n",
      "💾 Saved full results dict to Results/Old/Models/original/_20250917_0236_full.pkl\n",
      "\n",
      "🔎 Summary of model performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>CV Mean Score</th>\n",
       "      <th>CV Std Score</th>\n",
       "      <th>Holdout ROC-AUC</th>\n",
       "      <th>Holdout Precision</th>\n",
       "      <th>Holdout Recall</th>\n",
       "      <th>Holdout F1</th>\n",
       "      <th>Descriptive CV Mean AUC</th>\n",
       "      <th>Descriptive CV Std AUC</th>\n",
       "      <th>Holdout ROC-AUC (SMOTE)</th>\n",
       "      <th>Descriptive CV Mean AUC (SMOTE)</th>\n",
       "      <th>Descriptive CV Std AUC (SMOTE)</th>\n",
       "      <th>Final Holdout ROC-AUC (SMOTE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'clf__C': 0.01, 'clf__gamma': 'scale', 'clf__...</td>\n",
       "      <td>0.698479</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.725721</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.713656</td>\n",
       "      <td>0.472303</td>\n",
       "      <td>0.696040</td>\n",
       "      <td>0.018313</td>\n",
       "      <td>0.672755</td>\n",
       "      <td>0.858546</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'clf__learning_rate': 0.019393987736667576, '...</td>\n",
       "      <td>0.701588</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.715824</td>\n",
       "      <td>0.404858</td>\n",
       "      <td>0.440529</td>\n",
       "      <td>0.421941</td>\n",
       "      <td>0.698772</td>\n",
       "      <td>0.015660</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.899262</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.7127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'clf__bootstrap': True, 'clf__class_weight': ...</td>\n",
       "      <td>0.710130</td>\n",
       "      <td>0.016776</td>\n",
       "      <td>0.714159</td>\n",
       "      <td>0.413146</td>\n",
       "      <td>0.387665</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.709296</td>\n",
       "      <td>0.013673</td>\n",
       "      <td>0.699322</td>\n",
       "      <td>0.926342</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>{'clf__colsample_bytree': 0.7880464524154114, ...</td>\n",
       "      <td>0.705320</td>\n",
       "      <td>0.015867</td>\n",
       "      <td>0.714002</td>\n",
       "      <td>0.443709</td>\n",
       "      <td>0.295154</td>\n",
       "      <td>0.354497</td>\n",
       "      <td>0.704672</td>\n",
       "      <td>0.010936</td>\n",
       "      <td>0.693197</td>\n",
       "      <td>0.918516</td>\n",
       "      <td>0.004851</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGB</td>\n",
       "      <td>{'clf__colsample_bytree': 0.7076257380232285, ...</td>\n",
       "      <td>0.708449</td>\n",
       "      <td>0.015968</td>\n",
       "      <td>0.706948</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.207048</td>\n",
       "      <td>0.274854</td>\n",
       "      <td>0.709430</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.934812</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{'clf__alpha': 0.01, 'clf__hidden_layer_sizes'...</td>\n",
       "      <td>0.634653</td>\n",
       "      <td>0.019821</td>\n",
       "      <td>0.620032</td>\n",
       "      <td>0.329825</td>\n",
       "      <td>0.414097</td>\n",
       "      <td>0.367188</td>\n",
       "      <td>0.638145</td>\n",
       "      <td>0.008077</td>\n",
       "      <td>0.625778</td>\n",
       "      <td>0.875595</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_depth'...</td>\n",
       "      <td>0.607292</td>\n",
       "      <td>0.020591</td>\n",
       "      <td>0.615286</td>\n",
       "      <td>0.304878</td>\n",
       "      <td>0.440529</td>\n",
       "      <td>0.360360</td>\n",
       "      <td>0.618683</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>0.574501</td>\n",
       "      <td>0.800583</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier                                        Best Params  \\\n",
       "0               SVC  {'clf__C': 0.01, 'clf__gamma': 'scale', 'clf__...   \n",
       "3  GradientBoosting  {'clf__learning_rate': 0.019393987736667576, '...   \n",
       "2      RandomForest  {'clf__bootstrap': True, 'clf__class_weight': ...   \n",
       "6              LGBM  {'clf__colsample_bytree': 0.7880464524154114, ...   \n",
       "5               XGB  {'clf__colsample_bytree': 0.7076257380232285, ...   \n",
       "4               MLP  {'clf__alpha': 0.01, 'clf__hidden_layer_sizes'...   \n",
       "1      DecisionTree  {'clf__criterion': 'entropy', 'clf__max_depth'...   \n",
       "\n",
       "   CV Mean Score  CV Std Score  Holdout ROC-AUC  Holdout Precision  \\\n",
       "0       0.698479      0.018458         0.725721           0.352941   \n",
       "3       0.701588      0.017152         0.715824           0.404858   \n",
       "2       0.710130      0.016776         0.714159           0.413146   \n",
       "6       0.705320      0.015867         0.714002           0.443709   \n",
       "5       0.708449      0.015968         0.706948           0.408696   \n",
       "4       0.634653      0.019821         0.620032           0.329825   \n",
       "1       0.607292      0.020591         0.615286           0.304878   \n",
       "\n",
       "   Holdout Recall  Holdout F1  Descriptive CV Mean AUC  \\\n",
       "0        0.713656    0.472303                 0.696040   \n",
       "3        0.440529    0.421941                 0.698772   \n",
       "2        0.387665    0.400000                 0.709296   \n",
       "6        0.295154    0.354497                 0.704672   \n",
       "5        0.207048    0.274854                 0.709430   \n",
       "4        0.414097    0.367188                 0.638145   \n",
       "1        0.440529    0.360360                 0.618683   \n",
       "\n",
       "   Descriptive CV Std AUC Holdout ROC-AUC (SMOTE)  \\\n",
       "0                0.018313                0.672755   \n",
       "3                0.015660                  0.7127   \n",
       "2                0.013673                0.699322   \n",
       "6                0.010936                0.693197   \n",
       "5                0.015078                0.693333   \n",
       "4                0.008077                0.625778   \n",
       "1                0.015638                0.574501   \n",
       "\n",
       "  Descriptive CV Mean AUC (SMOTE) Descriptive CV Std AUC (SMOTE)  \\\n",
       "0                        0.858546                       0.008683   \n",
       "3                        0.899262                       0.004728   \n",
       "2                        0.926342                       0.003672   \n",
       "6                        0.918516                       0.004851   \n",
       "5                        0.934812                       0.004214   \n",
       "4                        0.875595                       0.004172   \n",
       "1                        0.800583                       0.010074   \n",
       "\n",
       "  Final Holdout ROC-AUC (SMOTE)  \n",
       "0                          None  \n",
       "3                        0.7127  \n",
       "2                          None  \n",
       "6                          None  \n",
       "5                          None  \n",
       "4                          None  \n",
       "1                          None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_orig, summary_orig = repeated_cv_with_mixed_search(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    classifiers=classifiers,\n",
    "    param_spaces=param_spaces, # pass grids for small models, distributions for big models\n",
    "    X_train_smote=X_train_smote, y_train_smote=y_train_smote,\n",
    "    n_splits=5,\n",
    "    n_repeats=10,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_iter_random=20,  # default for others\n",
    "    n_iter_random_per_clf=n_iter_random_per_clf,\n",
    "    descriptive_cv=True,\n",
    "    save_prefix=\"Results/Old/Models/original/original\"\n",
    ")\n",
    "\n",
    "print(\"\\n🔎 Summary of model performance:\")\n",
    "display(summary_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Original Training Models from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Results/Old/Models/original/original_20250917_0236_best_model.pkl\", \"rb\") as f:  # <-- put your filename here\n",
    "    results_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Repeated_CV on W2V Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Running SVC...\n",
      "Fitting 50 folds for each of 24 candidates, totalling 1200 fits\n",
      "   Performing descriptive KFold CV on original training set for SVC...\n",
      "   Descriptive CV AUC: 0.7252 ± 0.0182\n",
      "✅ SVC done. Best params: {'clf__C': 0.01, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
      "   CV ROC-AUC: 0.724 ± 0.018\n",
      "   Holdout ROC-AUC: 0.728\n",
      "\n",
      "🔹 Running DecisionTree...\n",
      "Fitting 50 folds for each of 240 candidates, totalling 12000 fits\n",
      "   Performing descriptive KFold CV on original training set for DecisionTree...\n",
      "   Descriptive CV AUC: 0.6041 ± 0.0222\n",
      "✅ DecisionTree done. Best params: {'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__min_samples_leaf': 10, 'clf__min_samples_split': 2}\n",
      "   CV ROC-AUC: 0.621 ± 0.022\n",
      "   Holdout ROC-AUC: 0.622\n",
      "\n",
      "🔹 Running RandomForest...\n",
      "Fitting 50 folds for each of 50 candidates, totalling 2500 fits\n",
      "   Performing descriptive KFold CV on original training set for RandomForest...\n",
      "   Descriptive CV AUC: 0.7246 ± 0.0154\n",
      "✅ RandomForest done. Best params: {'clf__bootstrap': False, 'clf__class_weight': None, 'clf__max_depth': 50, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 6, 'clf__n_estimators': 991}\n",
      "   CV ROC-AUC: 0.727 ± 0.016\n",
      "   Holdout ROC-AUC: 0.732\n",
      "\n",
      "🔹 Running GradientBoosting...\n",
      "Fitting 50 folds for each of 30 candidates, totalling 1500 fits\n",
      "   Performing descriptive KFold CV on original training set for GradientBoosting...\n",
      "   Descriptive CV AUC: 0.7379 ± 0.0218\n",
      "✅ GradientBoosting done. Best params: {'clf__learning_rate': 0.11128455142108838, 'clf__max_depth': 10, 'clf__max_features': None, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 9, 'clf__n_estimators': 332, 'clf__subsample': 0.8211508513174122}\n",
      "   CV ROC-AUC: 0.734 ± 0.016\n",
      "   Holdout ROC-AUC: 0.735\n",
      "\n",
      "🔹 Running MLP...\n",
      "Fitting 50 folds for each of 36 candidates, totalling 1800 fits\n",
      "   Performing descriptive KFold CV on original training set for MLP...\n",
      "   Descriptive CV AUC: 0.6994 ± 0.0279\n",
      "✅ MLP done. Best params: {'clf__alpha': 0.01, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate_init': 0.01, 'clf__solver': 'adam'}\n",
      "   CV ROC-AUC: 0.701 ± 0.017\n",
      "   Holdout ROC-AUC: 0.701\n",
      "\n",
      "🔹 Running XGB...\n",
      "Fitting 50 folds for each of 50 candidates, totalling 2500 fits\n",
      "   Performing descriptive KFold CV on original training set for XGB...\n",
      "   Descriptive CV AUC: 0.7423 ± 0.0207\n",
      "✅ XGB done. Best params: {'clf__colsample_bytree': 0.9454044297767479, 'clf__gamma': 0.4303652916281717, 'clf__learning_rate': 0.01208563915935721, 'clf__max_depth': 10, 'clf__min_child_weight': 3, 'clf__n_estimators': 848, 'clf__subsample': 0.8454489914076949}\n",
      "   CV ROC-AUC: 0.741 ± 0.015\n",
      "   Holdout ROC-AUC: 0.742\n",
      "\n",
      "🔹 Running LGBM...\n",
      "Fitting 50 folds for each of 50 candidates, totalling 2500 fits\n",
      "   Performing descriptive KFold CV on original training set for LGBM...\n",
      "   Descriptive CV AUC: 0.7418 ± 0.0207\n",
      "✅ LGBM done. Best params: {'clf__colsample_bytree': 0.8252233009446337, 'clf__learning_rate': 0.07663234314121907, 'clf__max_depth': 20, 'clf__min_child_samples': 53, 'clf__n_estimators': 863, 'clf__num_leaves': 237, 'clf__subsample': 0.7504873126518792}\n",
      "   CV ROC-AUC: 0.741 ± 0.016\n",
      "   Holdout ROC-AUC: 0.729\n",
      "\n",
      "📌 Evaluating all classifiers on holdout test set using CV-trained pipelines:\n",
      "SVC: Holdout ROC-AUC = 0.7282\n",
      "DecisionTree: Holdout ROC-AUC = 0.6217\n",
      "RandomForest: Holdout ROC-AUC = 0.7320\n",
      "GradientBoosting: Holdout ROC-AUC = 0.7349\n",
      "MLP: Holdout ROC-AUC = 0.7011\n",
      "XGB: Holdout ROC-AUC = 0.7420\n",
      "LGBM: Holdout ROC-AUC = 0.7292\n",
      "\n",
      "🏆 Best classifier (no SMOTE) = XGB, Holdout ROC-AUC = 0.7420\n",
      "💾 Saved best model (no SMOTE) to Results/Old/Models/w2v/w2v_20250917_1815_best_model.pkl\n",
      "\n",
      "📌 Evaluating CV-trained classifiers on SMOTE-balanced training data:\n",
      "SVC: Holdout ROC-AUC on SMOTE training = 0.6883\n",
      "   Performing descriptive KFold CV on SMOTE data for SVC...\n",
      "   Descriptive CV AUC (SMOTE): 0.8744 ± 0.0065\n",
      "DecisionTree: Holdout ROC-AUC on SMOTE training = 0.6087\n",
      "   Performing descriptive KFold CV on SMOTE data for DecisionTree...\n",
      "   Descriptive CV AUC (SMOTE): 0.7553 ± 0.0184\n",
      "RandomForest: Holdout ROC-AUC on SMOTE training = 0.7223\n",
      "   Performing descriptive KFold CV on SMOTE data for RandomForest...\n",
      "   Descriptive CV AUC (SMOTE): 0.9565 ± 0.0042\n",
      "GradientBoosting: Holdout ROC-AUC on SMOTE training = 0.7167\n",
      "   Performing descriptive KFold CV on SMOTE data for GradientBoosting...\n",
      "   Descriptive CV AUC (SMOTE): 0.9607 ± 0.0027\n",
      "MLP: Holdout ROC-AUC on SMOTE training = 0.6913\n",
      "   Performing descriptive KFold CV on SMOTE data for MLP...\n",
      "   Descriptive CV AUC (SMOTE): 0.9190 ± 0.0077\n",
      "XGB: Holdout ROC-AUC on SMOTE training = 0.7317\n",
      "   Performing descriptive KFold CV on SMOTE data for XGB...\n",
      "   Descriptive CV AUC (SMOTE): 0.9489 ± 0.0025\n",
      "LGBM: Holdout ROC-AUC on SMOTE training = 0.7187\n",
      "   Performing descriptive KFold CV on SMOTE data for LGBM...\n",
      "   Descriptive CV AUC (SMOTE): 0.9551 ± 0.0028\n",
      "🏆 Best SMOTE-trained classifier = XGB, Holdout ROC-AUC = 0.7317\n",
      "🎯 Final evaluation of best SMOTE-trained classifier = XGB, ROC-AUC = 0.7317\n",
      "💾 Saved best SMOTE-trained model to Results/Old/Models/w2v/w2v_20250917_1815_best_smote_model.pkl\n",
      "💾 Saved full summary including original and SMOTE metrics to Results/Old/Models/w2v/w2v_20250917_1815_full_summary_with_smote.csv\n",
      "💾 Saved full results dict to Results/Old/Models/w2v/w2v_20250917_1815_full.pkl\n",
      "\n",
      "🔎 Summary of model performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>CV Mean Score</th>\n",
       "      <th>CV Std Score</th>\n",
       "      <th>Holdout ROC-AUC</th>\n",
       "      <th>Holdout Precision</th>\n",
       "      <th>Holdout Recall</th>\n",
       "      <th>Holdout F1</th>\n",
       "      <th>Descriptive CV Mean AUC</th>\n",
       "      <th>Descriptive CV Std AUC</th>\n",
       "      <th>Holdout ROC-AUC (SMOTE)</th>\n",
       "      <th>Descriptive CV Mean AUC (SMOTE)</th>\n",
       "      <th>Descriptive CV Std AUC (SMOTE)</th>\n",
       "      <th>Final Holdout ROC-AUC (SMOTE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGB</td>\n",
       "      <td>{'clf__colsample_bytree': 0.9454044297767479, ...</td>\n",
       "      <td>0.740814</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>0.742023</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.742252</td>\n",
       "      <td>0.020739</td>\n",
       "      <td>0.731705</td>\n",
       "      <td>0.948922</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.731705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'clf__learning_rate': 0.11128455142108838, 'c...</td>\n",
       "      <td>0.734148</td>\n",
       "      <td>0.016075</td>\n",
       "      <td>0.734877</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.171806</td>\n",
       "      <td>0.254072</td>\n",
       "      <td>0.737901</td>\n",
       "      <td>0.021806</td>\n",
       "      <td>0.716694</td>\n",
       "      <td>0.96067</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'clf__bootstrap': False, 'clf__class_weight':...</td>\n",
       "      <td>0.726965</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>0.731991</td>\n",
       "      <td>0.494382</td>\n",
       "      <td>0.193833</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.724602</td>\n",
       "      <td>0.015406</td>\n",
       "      <td>0.722343</td>\n",
       "      <td>0.956503</td>\n",
       "      <td>0.004243</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>{'clf__colsample_bytree': 0.8252233009446337, ...</td>\n",
       "      <td>0.740644</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>0.729202</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.233480</td>\n",
       "      <td>0.318318</td>\n",
       "      <td>0.741798</td>\n",
       "      <td>0.020743</td>\n",
       "      <td>0.718672</td>\n",
       "      <td>0.955105</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'clf__C': 0.01, 'clf__gamma': 'scale', 'clf__...</td>\n",
       "      <td>0.724436</td>\n",
       "      <td>0.017858</td>\n",
       "      <td>0.728175</td>\n",
       "      <td>0.349882</td>\n",
       "      <td>0.651982</td>\n",
       "      <td>0.455385</td>\n",
       "      <td>0.725165</td>\n",
       "      <td>0.018246</td>\n",
       "      <td>0.688287</td>\n",
       "      <td>0.874378</td>\n",
       "      <td>0.00651</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{'clf__alpha': 0.01, 'clf__hidden_layer_sizes'...</td>\n",
       "      <td>0.700999</td>\n",
       "      <td>0.017309</td>\n",
       "      <td>0.701095</td>\n",
       "      <td>0.416290</td>\n",
       "      <td>0.405286</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.699430</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.691338</td>\n",
       "      <td>0.918997</td>\n",
       "      <td>0.007673</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_depth': 5...</td>\n",
       "      <td>0.620877</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>0.621716</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.440529</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.604083</td>\n",
       "      <td>0.022249</td>\n",
       "      <td>0.608654</td>\n",
       "      <td>0.755338</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier                                        Best Params  \\\n",
       "5               XGB  {'clf__colsample_bytree': 0.9454044297767479, ...   \n",
       "3  GradientBoosting  {'clf__learning_rate': 0.11128455142108838, 'c...   \n",
       "2      RandomForest  {'clf__bootstrap': False, 'clf__class_weight':...   \n",
       "6              LGBM  {'clf__colsample_bytree': 0.8252233009446337, ...   \n",
       "0               SVC  {'clf__C': 0.01, 'clf__gamma': 'scale', 'clf__...   \n",
       "4               MLP  {'clf__alpha': 0.01, 'clf__hidden_layer_sizes'...   \n",
       "1      DecisionTree  {'clf__criterion': 'gini', 'clf__max_depth': 5...   \n",
       "\n",
       "   CV Mean Score  CV Std Score  Holdout ROC-AUC  Holdout Precision  \\\n",
       "5       0.740814      0.014607         0.742023           0.530612   \n",
       "3       0.734148      0.016075         0.734877           0.487500   \n",
       "2       0.726965      0.016388         0.731991           0.494382   \n",
       "6       0.740644      0.015842         0.729202           0.500000   \n",
       "0       0.724436      0.017858         0.728175           0.349882   \n",
       "4       0.700999      0.017309         0.701095           0.416290   \n",
       "1       0.620877      0.021805         0.621716           0.300300   \n",
       "\n",
       "   Holdout Recall  Holdout F1  Descriptive CV Mean AUC  \\\n",
       "5        0.229075    0.320000                 0.742252   \n",
       "3        0.171806    0.254072                 0.737901   \n",
       "2        0.193833    0.278481                 0.724602   \n",
       "6        0.233480    0.318318                 0.741798   \n",
       "0        0.651982    0.455385                 0.725165   \n",
       "4        0.405286    0.410714                 0.699430   \n",
       "1        0.440529    0.357143                 0.604083   \n",
       "\n",
       "   Descriptive CV Std AUC Holdout ROC-AUC (SMOTE)  \\\n",
       "5                0.020739                0.731705   \n",
       "3                0.021806                0.716694   \n",
       "2                0.015406                0.722343   \n",
       "6                0.020743                0.718672   \n",
       "0                0.018246                0.688287   \n",
       "4                0.027886                0.691338   \n",
       "1                0.022249                0.608654   \n",
       "\n",
       "  Descriptive CV Mean AUC (SMOTE) Descriptive CV Std AUC (SMOTE)  \\\n",
       "5                        0.948922                       0.002544   \n",
       "3                         0.96067                       0.002745   \n",
       "2                        0.956503                       0.004243   \n",
       "6                        0.955105                       0.002808   \n",
       "0                        0.874378                        0.00651   \n",
       "4                        0.918997                       0.007673   \n",
       "1                        0.755338                       0.018434   \n",
       "\n",
       "  Final Holdout ROC-AUC (SMOTE)  \n",
       "5                      0.731705  \n",
       "3                          None  \n",
       "2                          None  \n",
       "6                          None  \n",
       "0                          None  \n",
       "4                          None  \n",
       "1                          None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_w2v, summary_w2v = repeated_cv_with_mixed_search(\n",
    "    X_train_w2v, y_train_w2v, X_test_w2v, y_test_w2v,\n",
    "    classifiers=classifiers,\n",
    "    param_spaces=param_spaces, # pass grids for small models, distributions for big models\n",
    "    X_train_smote=X_train_smote_w2v, y_train_smote=y_train_smote_w2v,\n",
    "    n_splits=5,\n",
    "    n_repeats=10,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_iter_random=20,  # default for others\n",
    "    n_iter_random_per_clf=n_iter_random_per_clf,\n",
    "    descriptive_cv=True,\n",
    "    save_prefix=\"Results/Old/Models/w2v/w2v\"\n",
    ")\n",
    "\n",
    "print(\"\\n🔎 Summary of model performance:\")\n",
    "display(summary_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload W2V Training Models from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Results/Old/Models/w2v/w2v_20250917_1815_full.pkl\", \"rb\") as f:  # <-- put your filename here\n",
    "    results_w2v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Function for All Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def save_all_classifiers(results, save_prefix=\"Results/Models/\"):\n",
    "    \"\"\"\n",
    "    Save all trained classifiers from results dict returned by repeated_cv_with_mixed_search().\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Output from repeated_cv_with_mixed_search, with structure:\n",
    "        results[classifier_name][\"best_estimator\"] = fitted pipeline\n",
    "    save_prefix : str\n",
    "        Directory prefix where models will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_prefix, exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "    for name, res in results.items():\n",
    "        if \"best_estimator\" in res and res[\"best_estimator\"] is not None:\n",
    "            model_path = f\"{save_prefix}{timestamp}_{name}_model.pkl\"\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(res[\"best_estimator\"], f)\n",
    "            print(f\"💾 Saved {name} model to {model_path}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Skipping {name}, no best_estimator found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Original Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "save_all_classifiers(results_orig, save_prefix=\"Results/Old/Models/original_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save W2V Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "save_all_classifiers(results_w2v, save_prefix=\"Results/Old/Models/w2v_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_all_classifiers(save_prefix=\"Results/Models/\", timestamp=None):\n",
    "    \"\"\"\n",
    "    Load all saved classifiers from pickle files into a dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    save_prefix : str\n",
    "        Directory prefix where models were saved.\n",
    "    timestamp : str or None\n",
    "        If provided, only load models with this timestamp.\n",
    "        Otherwise, load all models found in the folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loaded_models : dict\n",
    "        {classifier_name: fitted_pipeline}\n",
    "    \"\"\"\n",
    "    loaded_models = {}\n",
    "\n",
    "    # Pattern to match model files\n",
    "    if timestamp is not None:\n",
    "        pattern = os.path.join(save_prefix, f\"{timestamp}_*_model.pkl\")\n",
    "    else:\n",
    "        pattern = os.path.join(save_prefix, \"*_model.pkl\")\n",
    "\n",
    "    for model_file in glob.glob(pattern):\n",
    "        clf_name = os.path.basename(model_file).split(\"_\")[-2]  # extract classifier name\n",
    "        with open(model_file, \"rb\") as f:\n",
    "            loaded_models[clf_name] = pickle.load(f)\n",
    "        print(f\"✅ Loaded {clf_name} from {model_file}\")\n",
    "\n",
    "    return loaded_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Original Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load everything back\n",
    "models_orig = load_all_classifiers(\"Results/Old/Models/original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload W2V Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load everything back\n",
    "models_w2v = load_all_classifiers(\"Results/Old/Models/w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvY0Q54KhhP2"
   },
   "source": [
    "## Confidence Intervals for AUC using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnwSpOJ-xMI5",
    "outputId": "e1f2939a-e512-41df-ccc2-d99017139943"
   },
   "outputs": [],
   "source": [
    "# AC w/ edits\n",
    "\n",
    "from scipy.stats import sem, t\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = 5 - 1  # degrees of freedom for t-distribution\n",
    "t_critical = t.ppf((1 + confidence_level) / 2, degrees_freedom)  # t-critical value for 95% CI\n",
    "\n",
    "results_with_confidence_interval = {}\n",
    "\n",
    "for name, clf in selected_classifiers.items():\n",
    "    # Get cross-validated scores for each metric\n",
    "    auc_scores = cross_val_score(clf, X_smote, y_smote, cv=kf, scoring=auc_scorer, n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Calculate means\n",
    "    auc_mean = np.mean(auc_scores)\n",
    "\n",
    "\n",
    "    # Calculate standard errors\n",
    "    auc_sem = sem(auc_scores)\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    auc_conf_interval = (auc_mean - t_critical * auc_sem, auc_mean + t_critical * auc_sem)\n",
    "    # acc_conf_interval = (acc_mean - t_critical * acc_sem, acc_mean + t_critical * acc_sem)\n",
    "\n",
    "    # Store the results\n",
    "    results_with_confidence_interval[name] = {\n",
    "        'AUC Mean': auc_mean,\n",
    "        'AUC 95% CI': auc_conf_interval\n",
    "        # 'Accuracy Mean': acc_mean,\n",
    "        # 'Accuracy 95% CI': acc_conf_interval\n",
    "    }\n",
    "\n",
    "# Print the results with confidence intervals\n",
    "for name, metrics in results_with_confidence_interval.items():\n",
    "    print(f\"{name}: AUC = {metrics['AUC Mean']:.4f} (95% CI: {metrics['AUC 95% CI']})\")\n",
    "    # print(f\"{name}: Accuracy = {metrics['Accuracy Mean']:.4f} (95% CI: {metrics['Accuracy 95% CI']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDifgC3FiAMU"
   },
   "source": [
    "## Visualization for AUC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author's used the below code chunk to get AUCROC curves. I pulled  from it and created a function that accepts the summary_df from repeated_cv_with_mixed_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "whkt8NcUapIM",
    "outputId": "4ad8cf71-2c65-4056-a225-a22cde5ae7d0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# AC w/ edits\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_classifiers = {\n",
    "    'SVC': classifiers['SVC'],\n",
    "    'DecisionTree' : classifiers['DecisionTree'],\n",
    "    'RandomForest': classifiers['RandomForest'],\n",
    "    'GradientBoosting': classifiers['GradientBoosting'],\n",
    "    'XGB': classifiers['XGB'],\n",
    "    'MLP': classifiers['MLP'],\n",
    "    'LGBM': classifiers['LGBM']\n",
    "}\n",
    "\n",
    "# Define a dictionary to store ROC curve data for each classifier\n",
    "roc_curves = {}\n",
    "\n",
    "# Loop through selected classifiers\n",
    "for name, clf in selected_classifiers.items():\n",
    "    # Perform cross-validation and get predicted probabilities\n",
    "    y_scores = cross_val_predict(clf, X_smote, y_smote, cv=kf, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "\n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_smote, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Store ROC curve data in the dictionary\n",
    "    roc_curves[name] = {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, roc_curve_data in roc_curves.items():\n",
    "    fpr, tpr, roc_auc = roc_curve_data['fpr'], roc_curve_data['tpr'], roc_curve_data['roc_auc']\n",
    "    mean_auc = results_with_confidence_interval[name]['AUC Mean']\n",
    "    lower, upper = results_with_confidence_interval[name]['AUC 95% CI']\n",
    "\n",
    "    # Modify the label string to include AUC and confidence interval\n",
    "    label = f'{name} (AUC = {mean_auc:.2f} ± {upper - lower:.2f})'\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.plot(fpr, tpr, label=label)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the random chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='gray')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves with AUC Confidence Intervals')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3845gLv4LXB4",
    "outputId": "1a4786cc-65e8-4852-a9cd-35287ca64de0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# AC w/ edits\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_classifiers = {\n",
    "    'SVC': classifiers['SVC'],\n",
    "    'DecisionTree' : classifiers['DecisionTree'],\n",
    "    'RandomForest': classifiers['RandomForest'],\n",
    "    'GradientBoosting': classifiers['GradientBoosting'],\n",
    "    'XGB': classifiers['XGB'],\n",
    "    'MLP': classifiers['MLP'],\n",
    "    'LGBM': classifiers['LGBM']\n",
    "}\n",
    "\n",
    "# Define a dictionary to store ROC curve data and other metrics for each classifier\n",
    "roc_curves_and_metrics = {}\n",
    "\n",
    "# Loop through selected classifiers\n",
    "for name, clf in selected_classifiers.items():\n",
    "    # Perform cross-validation and get predicted probabilities\n",
    "    y_scores_cv = cross_val_predict(clf, X_train, y_train, cv=kf, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "\n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_train, y_scores_cv)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Calculate other metrics\n",
    "    y_pred_cv = (y_scores_cv > 0.5).astype(int)\n",
    "    accuracy_mean_cv = accuracy_score(y_train, y_pred_cv)\n",
    "    precision_mean_cv = precision_score(y_train, y_pred_cv)\n",
    "    recall_mean_cv = recall_score(y_train, y_pred_cv)\n",
    "    sensitivity_mean_cv = recall_mean_cv  # Sensitivity is the same as Recall\n",
    "    f1_mean_cv = f1_score(y_train, y_pred_cv)\n",
    "\n",
    "    # Store ROC curve data and metrics in the dictionary\n",
    "    roc_curves_and_metrics[name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'roc_auc': roc_auc,\n",
    "        'accuracy_mean_cv': accuracy_mean_cv,\n",
    "        'precision_mean_cv': precision_mean_cv,\n",
    "        'recall_mean_cv': recall_mean_cv,\n",
    "        'sensitivity_mean_cv': sensitivity_mean_cv,\n",
    "        'f1_mean_cv': f1_mean_cv\n",
    "    }\n",
    "\n",
    "# Print cross-validation results\n",
    "for name, data in roc_curves_and_metrics.items():\n",
    "    fpr, tpr, roc_auc = data['fpr'], data['tpr'], data['roc_auc']\n",
    "    accuracy_mean_cv, precision_mean_cv, recall_mean_cv, sensitivity_mean_cv, f1_mean_cv = data['accuracy_mean_cv'], data['precision_mean_cv'], data['recall_mean_cv'], data['sensitivity_mean_cv'], data['f1_mean_cv']\n",
    "\n",
    "    # Modify the label string to include AUC and other metrics\n",
    "    label = f'{name} (AUC = {roc_auc:.2f}, Acc = {accuracy_mean_cv:.2f}, Precision = {precision_mean_cv:.2f}, Recall = {recall_mean_cv:.2f}, Sensitivity = {sensitivity_mean_cv:.2f}, F1 = {f1_mean_cv:.2f})'\n",
    "\n",
    "    # Plot the ROC curve (optional, can be removed if not needed)\n",
    "    plt.plot(fpr, tpr, label=label)\n",
    "\n",
    "    # Print metrics for cross-validation\n",
    "    print(f\"{name}: CV AUC Mean = {roc_auc:.4f}, \"\n",
    "          f\"CV Accuracy Mean = {accuracy_mean_cv:.4f}, \"\n",
    "          f\"CV F1 Score Mean = {f1_mean_cv:.4f}, \"\n",
    "          f\"CV Precision Mean = {precision_mean_cv:.4f}, \"\n",
    "          f\"CV Recall Mean = {recall_mean_cv:.4f}, \"\n",
    "          f\"CV Sensitivity Mean = {sensitivity_mean_cv:.4f}\")\n",
    "\n",
    "# Plot the random chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='gray')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves with AUC and Metrics')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curves and Comparison Bar Charts Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_comparison_roc(loaded_models, X_test, y_test, dataset_label=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for all loaded classifiers on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loaded_models : dict\n",
    "        {classifier_name: fitted_pipeline}, e.g. output of load_all_classifiers()\n",
    "    X_test : array-like\n",
    "        Test features\n",
    "    y_test : array-like\n",
    "        True labels\n",
    "    dataset_label : str\n",
    "        Label for the dataset (e.g., \"Original\", \"SMOTE\", \"BERT\")\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    for clf_name, model in loaded_models.items():\n",
    "        if not hasattr(model, \"predict_proba\"):\n",
    "            print(f\"⚠️ {clf_name} does not support predict_proba, skipping.\")\n",
    "            continue\n",
    "\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, lw=2,\n",
    "                 label=f\"{clf_name} ({dataset_label}) AUC={roc_auc:.2f}\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", lw=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curves ({dataset_label})\")\n",
    "    plt.legend(loc=\"lower right\", fontsize=9)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_comparison_roc_across_datasets(models_dicts, dataset_labels, X_tests, y_tests):\n",
    "    \"\"\"\n",
    "    Plot ROC curves across multiple datasets with legend ordered by AUC.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models_dicts : list of dict\n",
    "        Each dict maps {classifier_name: fitted_pipeline}, usually from load_all_classifiers().\n",
    "    dataset_labels : list of str\n",
    "        Labels for each dataset, e.g., [\"Original\", \"W2V\", \"BERT\"].\n",
    "    X_tests : list\n",
    "        Test sets corresponding to each dataset.\n",
    "    y_tests : list\n",
    "        True labels for each dataset.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    legend_entries = []\n",
    "\n",
    "    for models, label, X_test, y_test in zip(models_dicts, dataset_labels, X_tests, y_tests):\n",
    "        for clf_name, model in models.items():\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=2)\n",
    "            legend_entries.append((roc_auc, f\"{clf_name} ({label}) AUC={roc_auc:.2f}\"))\n",
    "\n",
    "    # Sort legend by AUC (descending)\n",
    "    legend_entries.sort(key=lambda x: x[0], reverse=True)\n",
    "    handles, labels = [], []\n",
    "    for _, lbl in legend_entries:\n",
    "        handles.append(plt.Line2D([0], [0]))  # dummy handle\n",
    "        labels.append(lbl)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", lw=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves Comparison Across Datasets\")\n",
    "    plt.legend(handles, labels, loc=\"lower right\", fontsize=9)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_holdout_comparison_from_models(all_loaded_models, dataset_labels, X_tests, y_tests, metric=\"roc_auc\"):\n",
    "    \"\"\"\n",
    "    Bar plot comparing holdout metrics across datasets using loaded classifiers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_loaded_models : list of dict\n",
    "        Each dict is {classifier_name: fitted_pipeline}, e.g. output of load_all_classifiers()\n",
    "    dataset_labels : list of str\n",
    "        Labels for each dataset, e.g. [\"Original\", \"W2V\", \"BERT\"]\n",
    "    X_tests : list of array-like\n",
    "        Test sets corresponding to each dataset\n",
    "    y_tests : list of array-like\n",
    "        True labels for each test set\n",
    "    metric : str\n",
    "        Which metric to compute (\"roc_auc\" supported; extendable later)\n",
    "    \"\"\"\n",
    "    classifiers = list(all_loaded_models[0].keys())\n",
    "    n_clfs = len(classifiers)\n",
    "    n_datasets = len(all_loaded_models)\n",
    "    bar_width = 0.8 / n_datasets\n",
    "    x = np.arange(n_clfs)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i, (models, label, X_test, y_test) in enumerate(zip(all_loaded_models, dataset_labels, X_tests, y_tests)):\n",
    "        values = []\n",
    "        for clf_name in classifiers:\n",
    "            model = models[clf_name]\n",
    "            if metric == \"roc_auc\":\n",
    "                if not hasattr(model, \"predict_proba\"):\n",
    "                    print(f\"⚠️ {clf_name} does not support predict_proba, skipping.\")\n",
    "                    values.append(np.nan)\n",
    "                    continue\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                score = roc_auc_score(y_test, y_pred_proba)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "            values.append(score)\n",
    "\n",
    "        plt.bar(x + i * bar_width, values, width=bar_width, label=label)\n",
    "\n",
    "    plt.xticks(x + bar_width * (n_datasets - 1) / 2, classifiers, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(metric.upper())\n",
    "    plt.title(f\"{metric.upper()} Comparison Across Datasets\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for the holdout test set\n",
    "plot_comparison_roc(models_orig, X_test, y_test, dataset_label=\"Original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for the holdout test set\n",
    "plot_comparison_roc(models_w2v, X_test_w2v, y_test_w2v, dataset_label=\"W2V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves across datasets\n",
    "plot_comparison_roc_across_datasets(\n",
    "    models_dicts=[models_orig, models_w2v], #[, models_bert],\n",
    "    dataset_labels=[\"Original\", \"W2V\"], # [, \"BERT\"]\n",
    "    X_tests=[X_test, X_test_w2v], #[, X_test_bert]\n",
    "    y_tests=[y_test, y_test_w2v] #[, y_test_bert]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare holdout ROC-AUCs across datasets\n",
    "plot_holdout_comparison_from_models(\n",
    "    all_loaded_models=[models_orig, models_w2v], #[ models_bert],\n",
    "    dataset_labels=[\"Original\", \"W2V\"], #[\"BERT\"],\n",
    "    X_tests=[X_test, X_test_w2v], #[, X_test_bert],\n",
    "    y_tests=[y_test, y_test_w2v], #[, y_test_bert],\n",
    "    metric=\"roc_auc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec2E--QLiHg9"
   },
   "source": [
    "# **Feature Importance & SHAP Value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Smote Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Retrain Final Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def retrain_on_smote(results_dict, model_name, X_train_smote, y_train_smote):\n",
    "    \"\"\"\n",
    "    Retrain a CV-selected classifier on full SMOTE training data.\n",
    "    Works for all classifiers, tree-based or not.\n",
    "    \"\"\"\n",
    "    if model_name not in results_dict:\n",
    "        raise ValueError(f\"Model '{model_name}' not in results_dict.\")\n",
    "    \n",
    "    # Clone the pipeline from repeated CV\n",
    "    pipe = clone(results_dict[model_name]['best_estimator'])\n",
    "    \n",
    "    # Remove SMOTE step if present (already done in X_train_smote)\n",
    "    if 'smote' in pipe.named_steps:\n",
    "        pipe.steps = [step for step in pipe.steps if step[0] != 'smote']\n",
    "    \n",
    "    # Fit on full SMOTE dataset\n",
    "    pipe.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain Original Models on SMOTE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store retrained models\n",
    "retrained_models_smote = {}\n",
    "\n",
    "for name in classifiers.keys():\n",
    "    try:\n",
    "        print(f\"\\n🔹 Retraining {name} on full SMOTE data...\")\n",
    "        retrained_models_smote[name] = retrain_on_smote(results_orig, name, X_train_smote, y_train_smote)\n",
    "        print(f\"✅ {name} retrained successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error retraining {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain W2V Models on SMOTE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store retrained models\n",
    "retrained_models_w2v_smote = {}\n",
    "\n",
    "for name in classifiers.keys():\n",
    "    try:\n",
    "        print(f\"\\n🔹 Retraining {name} on full SMOTE data...\")\n",
    "        retrained_models_w2v_smote[name] = retrain_on_smote(results_w2v, name, X_train_smote_w2v, y_train_smote_w2v)\n",
    "        print(f\"✅ {name} retrained successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error retraining {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "uKTbW5glrnez",
    "outputId": "fed77d14-287e-4b6f-d234-53488b387eb5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# AC w/ edits\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to your data (replace X_smote and y_smote with your data)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Get the names of the features (replace feature_names with your actual feature names)\n",
    "feature_names = X_train.columns  # Replace with your feature names\n",
    "\n",
    "# Sort feature importances and feature names in descending order\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "sorted_feature_importances = feature_importances[sorted_idx][:15]  # Select the top 15 feature importances\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_idx][:15]  # Select the corresponding feature names\n",
    "\n",
    "# Create a vertical bar plot with different colors for each feature\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_feature_names)), sorted_feature_importances, color=plt.cm.Paired(np.arange(len(sorted_feature_names))))\n",
    "plt.yticks(range(len(sorted_feature_names)), sorted_feature_names)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to show the most important features at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_top_features(tree_model_pipe, top_n=15):\n",
    "    \"\"\"\n",
    "    Plot top N feature importances for a retrained tree-based pipeline.\n",
    "    Skips non-tree models.\n",
    "    \"\"\"\n",
    "    clf = tree_model_pipe.named_steps['clf']\n",
    "\n",
    "    if not hasattr(clf, 'feature_importances_'):\n",
    "        raise TypeError(f\"{type(clf).__name__} does not support feature_importances_\")\n",
    "    \n",
    "    # Feature names\n",
    "    if hasattr(tree_model_pipe.named_steps['scaler'], 'feature_names_in_'):\n",
    "        feature_names = tree_model_pipe.named_steps['scaler'].feature_names_in_\n",
    "    elif hasattr(clf, 'feature_names_in_'):\n",
    "        feature_names = clf.feature_names_in_\n",
    "    else:\n",
    "        feature_names = [f'feature_{i}' for i in range(clf.n_features_in_)]\n",
    "    \n",
    "    importances = clf.feature_importances_\n",
    "    fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    fi_df = fi_df.sort_values(by='importance', ascending=False).head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(fi_df['feature'][::-1], fi_df['importance'][::-1], color='skyblue')\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(f\"Top {top_n} Feature Importances: {type(clf).__name__}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Feature Importances for Original vs W2V for XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Top-15 feature importance for XGB ---\n",
    "plot_top_features(retrained_models_smote['XGB'], top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_features(retrained_models_w2v_smote['XGB'], top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Feature Importances for Original vs W2V for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Top-15 feature importance for Random Forest ---\n",
    "plot_top_features(retrained_models_smote['RandomForest'], top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_features(retrained_models_w2v_smote['RandomForest'], top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hBOZBXHifCw"
   },
   "source": [
    "### SHAP Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def plot_shap_features(tree_model_pipe, X_sample, top_n=15):\n",
    "    \"\"\"\n",
    "    Plot SHAP summary for a retrained tree-based pipeline.\n",
    "    Skips non-tree models.\n",
    "    \"\"\"\n",
    "    clf = tree_model_pipe.named_steps['clf']\n",
    "\n",
    "    if not hasattr(clf, 'feature_importances_'):\n",
    "        raise TypeError(f\"{type(clf).__name__} does not support SHAP feature importance\")\n",
    "    \n",
    "    explainer = shap.Explainer(clf, X_sample)\n",
    "    shap_values = explainer(X_sample)\n",
    "    \n",
    "    shap.summary_plot(shap_values, X_sample, max_display=top_n, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP summary plot - Original XGB ---\n",
    "plot_shap_features(retrained_models_smote['XGB'], X_sample=X_train_smote, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP summary plot - Original RandomForest ---\n",
    "plot_shap_features(retrained_models_smote['RandomForest'], X_sample=X_train_smote, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP summary plot - W2V XGB ---\n",
    "plot_shap_features(retrained_models_w2v_smote['XGB'], X_sample=X_train_smote_w2v, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP summary plot - W2V RandomForest ---\n",
    "plot_shap_features(retrained_w2v_models_smote['RandomForest'], X_sample=X_train_smote_w2v, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFIuwO6eJ5Q7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# AC w/ edits\n",
    "\n",
    "model = classifiers['RandomForest'].fit(X_smote, y_smote)\n",
    "\n",
    "# pip install shap\n",
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model.predict, X_test)\n",
    "# Calculates the SHAP values - It takes some time\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "X_test\n",
    "\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delong's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combine_effects\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# You need: y_test (true labels), and predicted probabilities for both models\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Example placeholders:\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_test  \u001b[38;5;66;03m# your true labels\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_pred_gb \u001b[38;5;241m=\u001b[39m best_gb_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# GradientBoosting (orig+SMOTE)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m best_xgb_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_w2v)[:, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# XGB (W2V+SMOTE)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from statsmodels.stats.meta_analysis import combine_effects\n",
    "\n",
    "# You need: y_test (true labels), and predicted probabilities for both models\n",
    "# Example placeholders:\n",
    "y_true = y_test  # your true labels\n",
    "y_pred_gb = best_gb_model.predict_proba(X_test)[:, 1]   # GradientBoosting (orig+SMOTE)\n",
    "y_pred_xgb = best_xgb_model.predict_proba(X_test_w2v)[:, 1]  # XGB (W2V+SMOTE)\n",
    "\n",
    "# --------------------------\n",
    "# DeLong’s test function\n",
    "# --------------------------\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def delong_roc_test(y_true, y_pred1, y_pred2):\n",
    "    from statsmodels.stats.proportion import proportions_chisquare\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # Compute AUROCs\n",
    "    auc1 = roc_auc_score(y_true, y_pred1)\n",
    "    auc2 = roc_auc_score(y_true, y_pred2)\n",
    "    diff = auc1 - auc2\n",
    "    \n",
    "    # Bootstrap variance for test\n",
    "    n_bootstraps = 1000\n",
    "    rng = np.random.RandomState(42)\n",
    "    diffs = []\n",
    "    for i in range(n_bootstraps):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        auc1_bs = roc_auc_score(y_true[idx], y_pred1[idx])\n",
    "        auc2_bs = roc_auc_score(y_true[idx], y_pred2[idx])\n",
    "        diffs.append(auc1_bs - auc2_bs)\n",
    "    \n",
    "    se = np.std(diffs)\n",
    "    z_score = diff / se\n",
    "    p_value = 2 * norm.sf(abs(z_score))\n",
    "    return diff, p_value, (np.percentile(diffs, 2.5), np.percentile(diffs, 97.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "diff, p_val, ci = delong_roc_test(y_true, y_pred_xgb, y_pred_gb)\n",
    "print(f\"Difference in AUROC: {diff:.3f}\")\n",
    "print(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n",
    "print(f\"P-value: {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapped CI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         diffs\u001b[38;5;241m.\u001b[39mappend(auc1 \u001b[38;5;241m-\u001b[39m auc2)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(diffs), np\u001b[38;5;241m.\u001b[39mpercentile(diffs, \u001b[38;5;241m2.5\u001b[39m), np\u001b[38;5;241m.\u001b[39mpercentile(diffs, \u001b[38;5;241m97.5\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m mean_diff, ci_low, ci_high \u001b[38;5;241m=\u001b[39m bootstrap_auc_diff(y_true, y_pred_xgb, y_pred_gb)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean AUROC difference = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_diff\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m95% CI = [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mci_low\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mci_high\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_auc_diff(y_true, pred1, pred2, n_bootstraps=2000):\n",
    "    rng = np.random.RandomState(42)\n",
    "    diffs = []\n",
    "    for i in range(n_bootstraps):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        auc1 = roc_auc_score(y_true[idx], pred1[idx])\n",
    "        auc2 = roc_auc_score(y_true[idx], pred2[idx])\n",
    "        diffs.append(auc1 - auc2)\n",
    "    return np.mean(diffs), np.percentile(diffs, 2.5), np.percentile(diffs, 97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff, ci_low, ci_high = bootstrap_auc_diff(y_true, y_pred_xgb, y_pred_gb)\n",
    "print(f\"Mean AUROC difference = {mean_diff:.3f}\")\n",
    "print(f\"95% CI = [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRz2h3h2zTW4",
    "outputId": "e2411bc1-b640-4329-b434-82388ec03de4"
   },
   "outputs": [],
   "source": [
    "# AC w/ edits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming scaled_w2v is already loaded into the workspace\n",
    "\n",
    "# Split your dataset into a train and test set\n",
    "train, test = train_test_split(scaled_w2v, test_size=0.2)  # For example, 80% training, 20% test\n",
    "\n",
    "# Select the numeric columns you want to compare\n",
    "numeric_cols = [\n",
    "    'los_icu', 'max_age', 'sofa_score', 'avg_urineoutput',  'glucose_min', 'glucose_max',\n",
    "    'glucose_average', 'sodium_min', 'sodium_max', 'sodium_average',\n",
    "    'heart_rate_min', 'heart_rate_max', 'heart_rate_mean', 'sbp_min',\n",
    "    'sbp_max', 'sbp_mean', 'dbp_min', 'dbp_max', 'dbp_mean', 'resp_rate_min',\n",
    "    'resp_rate_max', 'resp_rate_mean', 'spo2_min', 'spo2_max', 'spo2_mean'\n",
    "]\n",
    "\n",
    "# NOTE PUT 'temperature_min', 'temperature_max', 'temperature_avg', back into the above numeric columns after fixing sql query!!!\n",
    "\n",
    "# Initialize a DataFrame to store p-values and means\n",
    "comparison = pd.DataFrame(index=numeric_cols, columns=['Train Mean', 'Test Mean', 'P-Value'])\n",
    "\n",
    "# Calculate p-values and means for each variable between train and test sets\n",
    "for col in numeric_cols:\n",
    "    train_mean = train[col].dropna().mean()\n",
    "    test_mean = test[col].dropna().mean()\n",
    "    p_value = stats.ttest_ind(train[col].dropna(), test[col].dropna(), equal_var=False).pvalue\n",
    "\n",
    "    comparison.loc[col, 'Train Mean'] = train_mean\n",
    "    comparison.loc[col, 'Test Mean'] = test_mean\n",
    "    comparison.loc[col, 'P-Value'] = p_value\n",
    "\n",
    "# Print the resulting DataFrame with means and p-values\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X60vryevW3WL",
    "outputId": "d9b5bb50-05e8-4d43-b505-f1d2e7844f51"
   },
   "outputs": [],
   "source": [
    "# AC w/ edits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming scaled_w2v is already loaded into the workspace\n",
    "\n",
    "# Split your dataset into a train and test set\n",
    "train, test = train_test_split(scaled_w2v, test_size=0.2)  # For example, 80% training, 20% test\n",
    "\n",
    "# Select the categorical columns you want to compare\n",
    "categorical_cols = [\n",
    "    'hospital_expire_flag','diabetes_without_cc', 'diabetes_with_cc', 'severe_liver_disease','aids','renal_disease',\n",
    "    'antibiotic_Vancomycin','antibiotic_Vancomycin Antibiotic Lock', \n",
    "    'antibiotic_Vancomycin Enema','antibiotic_Vancomycin Intrathecal',\n",
    "    'antibiotic_Vancomycin Oral Liquid',\n",
    "    'gender_F'\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "NOTE Use the below when the sql query is fixed!!!\n",
    "categorical_cols = [\n",
    "    'hospital_expire_flag','diabetes_without_cc', 'diabetes_with_cc', 'severe_liver_disease','aids',\n",
    "    'renal_disease','antibiotic_Carbapenem','antibiotic_Aminoglycoside',\n",
    "    'antibiotic_Glycopeptide','antibiotic_Oxazolidinone','antibiotic_Penicillin',\n",
    "    'antibiotic_Sulfonamide','antibiotic_Tetracycline','gender_F'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a DataFrame to store Chi-Square p-values and percentage of ones\n",
    "comparison = pd.DataFrame(index=categorical_cols, columns=['Chi-Square P-Value', 'Percentage of Ones'])\n",
    "\n",
    "# Calculate Chi-Square p-values and percentage of ones for each categorical variable between train and test sets\n",
    "for col in categorical_cols:\n",
    "    train_values = train[col].value_counts()\n",
    "    test_values = test[col].value_counts()\n",
    "\n",
    "    # Create a set of all unique categories across both train and test sets\n",
    "    all_categories = set(train_values.index) | set(test_values.index)\n",
    "\n",
    "    # Ensure that both train and test sets have counts for all categories\n",
    "    train_values = train_values.reindex(all_categories, fill_value=0)\n",
    "    test_values = test_values.reindex(all_categories, fill_value=0)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.DataFrame({'Train': train_values, 'Test': test_values})\n",
    "\n",
    "    # Calculate Chi-Square p-value\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Calculate the percentage of ones in the test set\n",
    "    total_ones_test = test_values.get(1, 0)\n",
    "    total_values_test = test_values.sum()\n",
    "    percentage_ones = (total_ones_test / total_values_test) * 100\n",
    "\n",
    "    comparison.loc[col, 'Chi-Square P-Value'] = p\n",
    "    comparison.loc[col, 'Percentage of Ones'] = percentage_ones\n",
    "\n",
    "# Print the resulting DataFrame with Chi-Square p-values and percentage of ones\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzyAbj2BeL55",
    "outputId": "bb5f55a9-2dde-4825-a69f-f08ac27473b5"
   },
   "outputs": [],
   "source": [
    "# AC w/ edits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming scaled_w2v is already loaded into the workspace\n",
    "\n",
    "# Split your dataset into a train and test set\n",
    "train, test = train_test_split(scaled_w2v, test_size=0.2)  # For example, 80% training, 20% test\n",
    "\n",
    "# Select the categorical columns you want to compare\n",
    "\n",
    "categorical_cols = [\n",
    "    'hospital_expire_flag','diabetes_without_cc', 'diabetes_with_cc', 'severe_liver_disease','aids','renal_disease',\n",
    "    'antibiotic_Vancomycin','antibiotic_Vancomycin Antibiotic Lock', \n",
    "    'antibiotic_Vancomycin Enema','antibiotic_Vancomycin Intrathecal',\n",
    "    'antibiotic_Vancomycin Oral Liquid',\n",
    "    'gender_F'\n",
    "]\n",
    "\"\"\"\n",
    "NOTE Use the below when the sql query is fixed!!!\n",
    "categorical_cols = [\n",
    "    'hospital_expire_flag','diabetes_without_cc', 'diabetes_with_cc', 'severe_liver_disease','aids',\n",
    "    'renal_disease','antibiotic_Carbapenem','antibiotic_Aminoglycoside',\n",
    "    'antibiotic_Glycopeptide','antibiotic_Oxazolidinone','antibiotic_Penicillin',\n",
    "    'antibiotic_Sulfonamide','antibiotic_Tetracycline','gender_F'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a DataFrame to store Chi-Square p-values and percentage of ones\n",
    "comparison = pd.DataFrame(index=categorical_cols, columns=['Chi-Square P-Value', 'Percentage of Ones (Train)', 'Percentage of Ones (Test)'])\n",
    "\n",
    "# Calculate Chi-Square p-values, train percentages of ones, and test percentages of ones for each categorical variable\n",
    "for col in categorical_cols:\n",
    "    train_values = train[col].value_counts()\n",
    "    test_values = test[col].value_counts()\n",
    "\n",
    "    # Create a set of all unique categories across both train and test sets\n",
    "    all_categories = set(train_values.index) | set(test_values.index)\n",
    "\n",
    "    # Ensure that both train and test sets have counts for all categories\n",
    "    train_values = train_values.reindex(all_categories, fill_value=0)\n",
    "    test_values = test_values.reindex(all_categories, fill_value=0)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.DataFrame({'Train': train_values, 'Test': test_values})\n",
    "\n",
    "    # Calculate Chi-Square p-value\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Calculate the percentage of ones in train and test sets\n",
    "    train_percentage_ones = (train[col] == 1).sum() / len(train) * 100\n",
    "    test_percentage_ones = (test[col] == 1).sum() / len(test) * 100\n",
    "\n",
    "    comparison.loc[col, 'Chi-Square P-Value'] = p\n",
    "    comparison.loc[col, 'Percentage of Ones (Train)'] = train_percentage_ones\n",
    "    comparison.loc[col, 'Percentage of Ones (Test)'] = test_percentage_ones\n",
    "\n",
    "# Print the resulting DataFrame with Chi-Square p-values and percentages of ones in train and test sets\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTDMnuSPw3vF",
    "outputId": "b16ddb4b-5009-4bef-efd5-3647762fcf63"
   },
   "outputs": [],
   "source": [
    "# AC w/ edits\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming scaled_w2v is already loaded into the workspace\n",
    "\n",
    "# Generate a statistical summary for both numeric and categorical columns\n",
    "summary = df.describe(include='all')\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
