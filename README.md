# Masters-Thesis: Sepsis Mortality Prediction

![Python](https://img.shields.io/badge/python-3.11-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Build](https://img.shields.io/badge/build-scaffolded-yellow)

ğŸ“˜ **[Read the full thesis (PDF)](docs/Tyler%20Kelly%20Thesis.pdf)**

---

## ğŸ“Œ Overview

This project was developed as a Masterâ€™s Thesis in Biostatistics at the University of Pittsburgh and focuses on ICU sepsis mortality prediction using EHR data and natural language processing of clinical text.


### Motivation
Sepsis remains a leading cause of mortality in intensive care units, motivating the need for reproducible and interpretable machine learning approaches to mortality prediction using routinely collected EHR data.

### Objective
The objective is to develop, evaluate, and statistically compare supervised machine learning models for predicting in-hospital mortality among ICU patients with sepsis using multi-modal EHR data from MIMIC-IV.

### Project Summary

This work frames sepsis mortality prediction as a binary classification task with class imbalance.
Models are tuned on the original training data and subsequently retrained on SMOTE-balanced data for final evaluation on both resampling schemes.

The pipeline integrates:
- Data ingestion and preprocessing using the cleaned MIMIC-IVâ€“derived dataset released by [Gao et al.](https://github.com/yuyinglu2000/Sepsis-Mortality)
- Unstructured clinical feature engineering
- Clinical text representation using NLP embeddings (Word2Vec; experimental LLM extensions explored separately)
- Supervised model training with and without SMOTE to address class imbalance
- Rigorous evaluation using AUROC, calibration, and statistical testing
- Reproducible experiment tracking and artifact logging (MLflow)
- A lightweight API demonstration for inference (in progress)

---
<!--
- **Goal:** [Short description of the prediction or classification task]
- **Dataset:** [Name + link if public; instructions if restricted]
- **Tech stack:** Python, scikit-learn, XGBoost, PyTorch, SHAP, Streamlit
- **Key results:** 
  - Best model: [Model + metric]
  - Interpretability: [e.g., SHAP feature importances]
  - Deployment: [Optional: Streamlit demo / API]



---

-->

## ğŸ› ï¸ Tech Stack

- **Languages**: Python (3.11), R, SQL
- **Libraries**: scikit-learn, XGBoost, LightGBM, pandas, numpy,  matplotlib, seaborn, plotly
- **NLP**: Word2Vec (gensim; main thesis NLP technique); experimental LLM-based feature extraction extensions using gpt-4.1-mini and gpt-4o-mini
- **Experiment Tracking**: MLflow
- **API**: Flask (FastAPI planned)
- **Containerization**: Docker, Docker Compose (planned)
- **CI/CD**: GitHub Actions
- **Testing**: PyTest

---

## Methodology

1. Outcome labeling and cohort definition using the cleaned MIMIC-IVâ€“derived dataset released by Gao et al.
2. Structured data preprocessing and normalization
3. Clinical text embedding using Word2Vec as the primary thesis representation
4. Model selection and hyperparameter tuning using repeated stratified cross-validation on non-resampled training data
5. Retraining of selected models on SMOTE-balanced training data
6. Performance evaluation using AUROC, calibration curves, and SHAP
7. Artifact persistence and experiment tracking with MLflow


---
<!--
## ğŸ—ï¸ Project Architecture

![Architecture](docs/architecture.png)

---

## ğŸ“Š Results

Internal AUROC: [to be added]

External validation AUROC: [to be added]

Feature importance and SHAP visualizations: see results/plots/

---

## Results Snapshot
| Model            | AUC  | Accuracy | Notes |
|------------------|------|----------|-------|
| Logistic Reg.    | 0.72 | 0.68     | Baseline |
| XGBoost          | 0.85 | 0.79     | Best performer |

---

## ğŸ” Interpretability Example
![SHAP summary plot](reports/figures/shap_summary.png)

---
-->

## ğŸ“ Academic Context

### ğŸ“„ Full Thesis Document

The complete, submitted Masterâ€™s thesis is available here:

ğŸ“˜ **[Tyler Kelly â€” Masterâ€™s Thesis (PDF)](docs/Tyler%20Kelly%20Thesis.pdf)**

This repository supports my Masterâ€™s Thesis defense by ensuring reproducibility and transparency:
- Committee members can follow the modular notebooks.
- Recruiters can inspect the scaffold and MLOps integrations.

This repository is also intended for academic and educational purposes.
Results should not be interpreted as clinical decision support without external validation.

### ğŸ“Š Dataset Source

The primary dataset used in this project is the cleaned MIMIC-IVâ€“derived cohort released by Gao et al.:

ğŸ”— **Gao et al. â€” Sepsis Mortality Prediction Repository**  
https://github.com/yuyinglu2000/Sepsis-Mortality

Specifically, this work uses the cleaned dataset provided in `Data_after_Cleaning.csv`, with all downstream preprocessing, feature engineering, modeling, and evaluation performed in this repository.

---

## ğŸ“‚ Repo Structure
```bash
Masters-Thesis/
â”œâ”€â”€ data/               # raw, processed, external datasets (gitignored)
â”œâ”€â”€ notebooks/          # modular workflow notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 04_model_training.ipynb
â”‚   â”œâ”€â”€ 05_baseline_model_evaluation.ipynb
â”‚   â”œâ”€â”€ 06_visualization.ipynb
â”‚   â”œâ”€â”€ 07_w2v_hyperparam_search.ipynb
â”‚   â”œâ”€â”€ 08_w2v_optimized_training.ipynb
â”‚   â”œâ”€â”€ 09_final_model_evaluation.ipynb
â”‚   â”œâ”€â”€ 10_statistical_testing.ipynb
â”‚   â”œâ”€â”€ 11_external_validation.ipynb  # currently unused
â”‚   â”œâ”€â”€ 12_reporting.ipynb
â”‚   â”œâ”€â”€ 13_Cosine_Similarity.ipynb
â”‚   â”œâ”€â”€ 14_LLM_Extension.ipynb         # exploratory work, not part of core thesis results
â”‚   â”œâ”€â”€ 15_api_demo.ipynb             # in development
â”‚   â””â”€â”€ archive/        # legacy notebooks + Rmd
â”œâ”€â”€ src/                # reusable, testable pipeline modules (data, features, models, evaluation)
â”œâ”€â”€ scripts/            # CLI and automation scripts
â”œâ”€â”€ sql/                # SQL utilities for extracting unstructured notes from MIMIC-IV
â”œâ”€â”€ results/            # trained models and plots (gitignored)
â”œâ”€â”€ mlflow_tracking/    # MLflow experiment logs (gitignored)
â”œâ”€â”€ configs/            # experiment configs
â”œâ”€â”€ tests/              # unit tests
â”œâ”€â”€ Makefile            # workflow automation
â”œâ”€â”€ Dockerfile          # containerization
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ environment.yml     # Conda environment
â””â”€â”€ README.md           # this file
```

---

## âš™ï¸ Setup Instructions

> Note: Raw MIMIC-IV data are not included due to licensing restrictions. SQL scripts are provided for reproducible extraction.

### Setup
```bash
git clone https://github.com/tylerkelly7/Masters-Thesis.git
cd Masters-Thesis
conda env create -f environment.yml
conda activate Masters-Thesis
```

---
<!--
## ğŸ”„ ML Pipeline

### Run the entire ML workflow:

```bash
make preprocess
make features
make train
make eval
```
-->

### Pipeline steps:

1. Extract raw notes data from MIMIC-IV (via SQL) and merge to the cleaned MIMIC-IVâ€“derived dataset released by Gao et al.

2. Clean & preprocess structured and text data

3. Engineer features (structured + embeddings)

4. Train models (Logistic Regression, Random Forest, Gradient Boosting, XGBoost, etc.)

5. Evaluate models (AUROC, calibration, SHAP/feature importance)

6. Save model artifacts with MLflow

---
<!--
## ğŸ”¬ API for Predictions

```bash
make run-api
```

---

## ğŸ§ª End-to-End Test

### Run pipeline + API test with one command:

```bash
make pipeline_test
```

---
-->

## ğŸ““ Notebooks
Modular Jupyter notebooks (01â€“15) implement the full pipeline, with legacy work preserved in `archive/`.

---

## âœ… Unit Tests
Core pipeline logic has been refactored into `src/` to support testability and reuse.
PyTest-based unit tests validate data preprocessing, feature engineering, and model training logic.
Coverage is being expanded as part of ongoing pipeline hardening.

---
<!--
## ğŸ³ Containerization

### Build Docker image:

```bash
docker build -t masters_thesis .
```

### Run with Docker:

```bash
docker run -p 5000:5000 masters_thesis
```

### Or with Docker Compose:

```bash
docker-compose up --build
```

---
-->

<!--
## ğŸ“š Documentation

Full project documentation is in [Full Documentation](docs/index.md)  
[GitHub Pages](https://tylerkelly7.github.io/Masters-Thesis/)

-->
## ğŸ“ˆ Future Work

- Extend resampling support beyond SMOTE (e.g., SMOTEENN, ADASYN) via the modular resampling interface
- Expand NLP representations using contextual embeddings (BERT variants)
- Investigate deep learning approaches for longitudinal risk modeling using time-aware representations


---

## ğŸ“„ License

This project is licensed under the MIT License.

---

ğŸ™‹ About Me

Built by Tyler Kelly
ğŸ“§ Email: tylerjkelly77@gmail.com

ğŸ”— [LinkedIn](https://www.linkedin.com/in/tylerkelly7)  

ğŸ”— [Portfolio](https://github.com/tylerkelly7)